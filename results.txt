hvflip+zoom+brightness on 128x128 images

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 182s 1s/step - auc: 0.6308 - loss: 0.6506 - val_auc: 0.8321 - val_loss: 0.4352
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 61s 514ms/step - auc: 0.7792 - loss: 0.5632 - val_auc: 0.8408 - val_loss: 0.5263
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 58s 485ms/step - auc: 0.8114 - loss: 0.5239 - val_auc: 0.8489 - val_loss: 0.5414
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 58s 489ms/step - auc: 0.8040 - loss: 0.5293 - val_auc: 0.8514 - val_loss: 0.4967
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 487ms/step - auc: 0.8045 - loss: 0.5296 - val_auc: 0.8514 - val_loss: 0.4961
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 486ms/step - auc: 0.8056 - loss: 0.5316 - val_auc: 0.8568 - val_loss: 0.4827
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 484ms/step - auc: 0.8201 - loss: 0.5061 - val_auc: 0.8562 - val_loss: 0.4919
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 59s 495ms/step - auc: 0.8328 - loss: 0.4992 - val_auc: 0.8599 - val_loss: 0.4471
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 485ms/step - auc: 0.8331 - loss: 0.5027 - val_auc: 0.8600 - val_loss: 0.4057
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 59s 501ms/step - auc: 0.8343 - loss: 0.4942 - val_auc: 0.8598 - val_loss: 0.4155
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 482ms/step - auc: 0.8376 - loss: 0.4872 - val_auc: 0.8608 - val_loss: 0.4481
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 482ms/step - auc: 0.8306 - loss: 0.4995 - val_auc: 0.8631 - val_loss: 0.4469
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 481ms/step - auc: 0.8398 - loss: 0.4846 - val_auc: 0.8636 - val_loss: 0.4352
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 489ms/step - auc: 0.8174 - loss: 0.5189 - val_auc: 0.8637 - val_loss: 0.4461
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 58s 497ms/step - auc: 0.8387 - loss: 0.4848 - val_auc: 0.8638 - val_loss: 0.4457
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 481ms/step - auc: 0.8489 - loss: 0.4775 - val_auc: 0.8660 - val_loss: 0.3871
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 481ms/step - auc: 0.8304 - loss: 0.4949 - val_auc: 0.8664 - val_loss: 0.4597
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 484ms/step - auc: 0.8333 - loss: 0.5116 - val_auc: 0.8653 - val_loss: 0.3330
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 58s 492ms/step - auc: 0.8374 - loss: 0.4982 - val_auc: 0.8643 - val_loss: 0.3587
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 482ms/step - auc: 0.8314 - loss: 0.4817 - val_auc: 0.8684 - val_loss: 0.4580


hvflip+zoom on 128x128 images

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 168s 1s/step - auc: 0.6503 - loss: 0.6602 - val_auc: 0.8145 - val_loss: 0.5016
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 58s 490ms/step - auc: 0.7716 - loss: 0.5570 - val_auc: 0.8367 - val_loss: 0.5140
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 490ms/step - auc: 0.7914 - loss: 0.5482 - val_auc: 0.8453 - val_loss: 0.5162
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 56s 479ms/step - auc: 0.8069 - loss: 0.5323 - val_auc: 0.8468 - val_loss: 0.4821
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 480ms/step - auc: 0.8122 - loss: 0.5142 - val_auc: 0.8470 - val_loss: 0.4817
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 83s 486ms/step - auc: 0.8159 - loss: 0.5184 - val_auc: 0.8486 - val_loss: 0.3904
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 483ms/step - auc: 0.8183 - loss: 0.5098 - val_auc: 0.8552 - val_loss: 0.4280
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 482ms/step - auc: 0.8079 - loss: 0.5190 - val_auc: 0.8553 - val_loss: 0.4170
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 479ms/step - auc: 0.8314 - loss: 0.5007 - val_auc: 0.8602 - val_loss: 0.4947
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 478ms/step - auc: 0.8344 - loss: 0.4937 - val_auc: 0.8606 - val_loss: 0.4874
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 56s 475ms/step - auc: 0.8419 - loss: 0.4882 - val_auc: 0.8618 - val_loss: 0.4822
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 483ms/step - auc: 0.8380 - loss: 0.4947 - val_auc: 0.8604 - val_loss: 0.4620
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 56s 475ms/step - auc: 0.8277 - loss: 0.4969 - val_auc: 0.8615 - val_loss: 0.4672
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 58s 494ms/step - auc: 0.8429 - loss: 0.4865 - val_auc: 0.8615 - val_loss: 0.4455
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 56s 480ms/step - auc: 0.8276 - loss: 0.4976 - val_auc: 0.8615 - val_loss: 0.4454
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 487ms/step - auc: 0.8370 - loss: 0.4904 - val_auc: 0.8635 - val_loss: 0.4244
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 55s 470ms/step - auc: 0.8314 - loss: 0.4952 - val_auc: 0.8623 - val_loss: 0.4271
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 481ms/step - auc: 0.8378 - loss: 0.4774 - val_auc: 0.8649 - val_loss: 0.4562
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 489ms/step - auc: 0.8641 - loss: 0.4571 - val_auc: 0.8646 - val_loss: 0.4762
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 487ms/step - auc: 0.8469 - loss: 0.4800 - val_auc: 0.8656 - val_loss: 0.3690



hvflip+brightness on 128x128 images

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 153s 967ms/step - auc: 0.6657 - loss: 0.6349 - val_auc: 0.8322 - val_loss: 0.4638
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 417ms/step - auc: 0.7713 - loss: 0.5749 - val_auc: 0.8467 - val_loss: 0.5453
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 411ms/step - auc: 0.7895 - loss: 0.5543 - val_auc: 0.8522 - val_loss: 0.5487
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 412ms/step - auc: 0.8166 - loss: 0.5167 - val_auc: 0.8544 - val_loss: 0.5156
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 410ms/step - auc: 0.8216 - loss: 0.5265 - val_auc: 0.8547 - val_loss: 0.5130
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 410ms/step - auc: 0.8230 - loss: 0.5105 - val_auc: 0.8607 - val_loss: 0.5157
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 425ms/step - auc: 0.8193 - loss: 0.5079 - val_auc: 0.8635 - val_loss: 0.4831
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 408ms/step - auc: 0.7996 - loss: 0.5333 - val_auc: 0.8658 - val_loss: 0.4564
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 424ms/step - auc: 0.8303 - loss: 0.5047 - val_auc: 0.8643 - val_loss: 0.5601
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 402ms/step - auc: 0.8369 - loss: 0.5010 - val_auc: 0.8666 - val_loss: 0.4529
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 417ms/step - auc: 0.8511 - loss: 0.4758 - val_auc: 0.8676 - val_loss: 0.4727
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 423ms/step - auc: 0.8482 - loss: 0.4742 - val_auc: 0.8675 - val_loss: 0.4809
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 418ms/step - auc: 0.8478 - loss: 0.4795 - val_auc: 0.8679 - val_loss: 0.4864
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 410ms/step - auc: 0.8553 - loss: 0.4680 - val_auc: 0.8680 - val_loss: 0.4709
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 408ms/step - auc: 0.8508 - loss: 0.4788 - val_auc: 0.8679 - val_loss: 0.4698
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 405ms/step - auc: 0.8358 - loss: 0.4986 - val_auc: 0.8642 - val_loss: 0.4745
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 420ms/step - auc: 0.8555 - loss: 0.4627 - val_auc: 0.8670 - val_loss: 0.4637
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 406ms/step - auc: 0.8407 - loss: 0.4982 - val_auc: 0.8688 - val_loss: 0.5120
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 426ms/step - auc: 0.8523 - loss: 0.4716 - val_auc: 0.8707 - val_loss: 0.4264
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 417ms/step - auc: 0.8568 - loss: 0.4727 - val_auc: 0.8715 - val_loss: 0.4863


hvflip+brightness on 128x128 images with 2e-3 learning rate

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 165s 1s/step - auc: 0.6185 - loss: 0.6587 - val_auc: 0.8419 - val_loss: 0.5078
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 418ms/step - auc: 0.7965 - loss: 0.5372 - val_auc: 0.8579 - val_loss: 0.4606
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 406ms/step - auc: 0.8204 - loss: 0.5053 - val_auc: 0.8614 - val_loss: 0.4190
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 405ms/step - auc: 0.8200 - loss: 0.5128 - val_auc: 0.8638 - val_loss: 0.5001
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 403ms/step - auc: 0.8295 - loss: 0.4976 - val_auc: 0.8635 - val_loss: 0.4854
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 404ms/step - auc: 0.8247 - loss: 0.5058 - val_auc: 0.8615 - val_loss: 0.5285
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 411ms/step - auc: 0.8374 - loss: 0.4881 - val_auc: 0.8586 - val_loss: 0.4960
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 405ms/step - auc: 0.8378 - loss: 0.5007 - val_auc: 0.8625 - val_loss: 0.5594
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 401ms/step - auc: 0.8481 - loss: 0.4828 - val_auc: 0.8667 - val_loss: 0.4722
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 412ms/step - auc: 0.8378 - loss: 0.4764 - val_auc: 0.8663 - val_loss: 0.5165
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 407ms/step - auc: 0.8472 - loss: 0.4704 - val_auc: 0.8671 - val_loss: 0.4387
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 402ms/step - auc: 0.8459 - loss: 0.4838 - val_auc: 0.8676 - val_loss: 0.4316
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 403ms/step - auc: 0.8550 - loss: 0.4680 - val_auc: 0.8670 - val_loss: 0.4679
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 403ms/step - auc: 0.8496 - loss: 0.4779 - val_auc: 0.8681 - val_loss: 0.4684
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 412ms/step - auc: 0.8558 - loss: 0.4647 - val_auc: 0.8678 - val_loss: 0.4621
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 405ms/step - auc: 0.8529 - loss: 0.4649 - val_auc: 0.8664 - val_loss: 0.4953
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 404ms/step - auc: 0.8624 - loss: 0.4574 - val_auc: 0.8697 - val_loss: 0.4448
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 406ms/step - auc: 0.8457 - loss: 0.4775 - val_auc: 0.8653 - val_loss: 0.3798
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 413ms/step - auc: 0.8665 - loss: 0.4494 - val_auc: 0.8725 - val_loss: 0.4391
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 404ms/step - auc: 0.8760 - loss: 0.4357 - val_auc: 0.8733 - val_loss: 0.5215



hvflip on 128x128 images

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 161s 1s/step - auc: 0.6304 - loss: 0.6525 - val_auc: 0.8191 - val_loss: 0.4770
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 409ms/step - auc: 0.7913 - loss: 0.5528 - val_auc: 0.8382 - val_loss: 0.5297
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 409ms/step - auc: 0.7997 - loss: 0.5463 - val_auc: 0.8432 - val_loss: 0.5104
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 403ms/step - auc: 0.8078 - loss: 0.5217 - val_auc: 0.8458 - val_loss: 0.5007
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 399ms/step - auc: 0.8362 - loss: 0.5007 - val_auc: 0.8455 - val_loss: 0.4885
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 407ms/step - auc: 0.8091 - loss: 0.5029 - val_auc: 0.8504 - val_loss: 0.4771
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 46s 395ms/step - auc: 0.8241 - loss: 0.5006 - val_auc: 0.8538 - val_loss: 0.5127
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 46s 393ms/step - auc: 0.8235 - loss: 0.5038 - val_auc: 0.8547 - val_loss: 0.4945
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 412ms/step - auc: 0.8404 - loss: 0.4910 - val_auc: 0.8575 - val_loss: 0.4579
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 409ms/step - auc: 0.8363 - loss: 0.4886 - val_auc: 0.8599 - val_loss: 0.4858
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 46s 395ms/step - auc: 0.8600 - loss: 0.4623 - val_auc: 0.8602 - val_loss: 0.4718
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 402ms/step - auc: 0.8553 - loss: 0.4675 - val_auc: 0.8599 - val_loss: 0.5001
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 401ms/step - auc: 0.8587 - loss: 0.4631 - val_auc: 0.8611 - val_loss: 0.4772
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 415ms/step - auc: 0.8440 - loss: 0.4721 - val_auc: 0.8612 - val_loss: 0.4708
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 415ms/step - auc: 0.8613 - loss: 0.4555 - val_auc: 0.8613 - val_loss: 0.4669
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 412ms/step - auc: 0.8581 - loss: 0.4667 - val_auc: 0.8604 - val_loss: 0.3938
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 405ms/step - auc: 0.8582 - loss: 0.4705 - val_auc: 0.8616 - val_loss: 0.4819
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 45s 389ms/step - auc: 0.8503 - loss: 0.4685 - val_auc: 0.8615 - val_loss: 0.4871
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 46s 394ms/step - auc: 0.8571 - loss: 0.4668 - val_auc: 0.8626 - val_loss: 0.4513
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 411ms/step - auc: 0.8628 - loss: 0.4512 - val_auc: 0.8619 - val_loss: 0.4731


conditional hvflip, zoom, brightness on 128x128 images with 256 batch size with 1e-3 learning rate

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 160s 1s/step - auc: 0.6456 - loss: 0.6509 - val_auc: 0.8249 - val_loss: 0.5575
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 407ms/step - auc: 0.8043 - loss: 0.5419 - val_auc: 0.8411 - val_loss: 0.5219
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 402ms/step - auc: 0.8081 - loss: 0.5225 - val_auc: 0.8473 - val_loss: 0.5262
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 409ms/step - auc: 0.8236 - loss: 0.5162 - val_auc: 0.8505 - val_loss: 0.5080
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 408ms/step - auc: 0.8130 - loss: 0.5340 - val_auc: 0.8502 - val_loss: 0.5024
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 408ms/step - auc: 0.8255 - loss: 0.5114 - val_auc: 0.8586 - val_loss: 0.4711
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 411ms/step - auc: 0.8381 - loss: 0.5072 - val_auc: 0.8611 - val_loss: 0.4312
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 410ms/step - auc: 0.8279 - loss: 0.4916 - val_auc: 0.8618 - val_loss: 0.4386
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 416ms/step - auc: 0.8409 - loss: 0.4909 - val_auc: 0.8631 - val_loss: 0.4772
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 410ms/step - auc: 0.8513 - loss: 0.4660 - val_auc: 0.8631 - val_loss: 0.5169
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 414ms/step - auc: 0.8373 - loss: 0.4863 - val_auc: 0.8634 - val_loss: 0.4702
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 406ms/step - auc: 0.8584 - loss: 0.4638 - val_auc: 0.8637 - val_loss: 0.4717
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 415ms/step - auc: 0.8527 - loss: 0.4696 - val_auc: 0.8631 - val_loss: 0.4693
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 410ms/step - auc: 0.8470 - loss: 0.4771 - val_auc: 0.8633 - val_loss: 0.4620
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 408ms/step - auc: 0.8558 - loss: 0.4744 - val_auc: 0.8632 - val_loss: 0.4595
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 403ms/step - auc: 0.8525 - loss: 0.4684 - val_auc: 0.8636 - val_loss: 0.4319
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 405ms/step - auc: 0.8455 - loss: 0.4778 - val_auc: 0.8640 - val_loss: 0.3925
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 408ms/step - auc: 0.8494 - loss: 0.4726 - val_auc: 0.8628 - val_loss: 0.4276
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 411ms/step - auc: 0.8483 - loss: 0.4764 - val_auc: 0.8658 - val_loss: 0.4052
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 408ms/step - auc: 0.8511 - loss: 0.4616 - val_auc: 0.8632 - val_loss: 0.4560


conditional hvflip, zoom, brightness on 128x128 images with 128 batch size with 1e-3 learning rate on CPU

Epoch 1/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1064s 5s/step - auc: 0.6946 - loss: 0.6204 - val_auc: 0.8389 - val_loss: 0.5910
Epoch 2/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1000s 5s/step - auc: 0.7833 - loss: 0.5530 - val_auc: 0.8516 - val_loss: 0.4518
Epoch 3/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1058s 5s/step - auc: 0.8202 - loss: 0.5110 - val_auc: 0.8556 - val_loss: 0.4836
Epoch 4/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1009s 5s/step - auc: 0.8073 - loss: 0.5271 - val_auc: 0.8588 - val_loss: 0.4980
Epoch 5/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 998s 5s/step - auc: 0.8229 - loss: 0.5080 - val_auc: 0.8588 - val_loss: 0.4874
Epoch 6/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1019s 5s/step - auc: 0.8257 - loss: 0.4978 - val_auc: 0.8622 - val_loss: 0.4447
Epoch 7/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 997s 5s/step - auc: 0.8168 - loss: 0.4850 - val_auc: 0.8633 - val_loss: 0.4358
Epoch 8/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1029s 5s/step - auc: 0.8371 - loss: 0.4841 - val_auc: 0.8636 - val_loss: 0.3726
Epoch 9/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 984s 5s/step - auc: 0.8436 - loss: 0.4754 - val_auc: 0.8697 - val_loss: 0.4274
Epoch 10/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1060s 5s/step - auc: 0.8437 - loss: 0.4759 - val_auc: 0.8697 - val_loss: 0.4406
Epoch 11/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1023s 5s/step - auc: 0.8426 - loss: 0.4615 - val_auc: 0.8713 - val_loss: 0.5035
Epoch 12/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1007s 5s/step - auc: 0.8589 - loss: 0.4621 - val_auc: 0.8717 - val_loss: 0.4750
Epoch 13/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1009s 5s/step - auc: 0.8515 - loss: 0.4630 - val_auc: 0.8719 - val_loss: 0.4704
Epoch 14/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1022s 5s/step - auc: 0.8545 - loss: 0.4663 - val_auc: 0.8719 - val_loss: 0.4584
Epoch 15/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1001s 5s/step - auc: 0.8601 - loss: 0.4577 - val_auc: 0.8719 - val_loss: 0.4538
Epoch 16/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1001s 5s/step - auc: 0.8546 - loss: 0.4601 - val_auc: 0.8707 - val_loss: 0.4746
Epoch 17/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1040s 5s/step - auc: 0.8242 - loss: 0.4992 - val_auc: 0.8727 - val_loss: 0.3859
Epoch 18/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 983s 5s/step - auc: 0.8551 - loss: 0.4630 - val_auc: 0.8703 - val_loss: 0.3798
Epoch 19/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1054s 5s/step - auc: 0.8540 - loss: 0.4598 - val_auc: 0.8696 - val_loss: 0.3800
Epoch 20/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1001s 5s/step - auc: 0.8584 - loss: 0.4536 - val_auc: 0.8715 - val_loss: 0.4111



conditional hvflip, zoom, brightness on 512x512 images resized to 128x128 images with 256 batch size with 2e-3 learning rate

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 184s 1s/step - auc: 0.6487 - loss: 0.6368 - val_auc: 0.8456 - val_loss: 0.5196
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 422ms/step - auc: 0.8036 - loss: 0.5295 - val_auc: 0.8596 - val_loss: 0.4991
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 411ms/step - auc: 0.8214 - loss: 0.5088 - val_auc: 0.8639 - val_loss: 0.4653
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 429ms/step - auc: 0.8340 - loss: 0.5039 - val_auc: 0.8636 - val_loss: 0.4889
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 414ms/step - auc: 0.8203 - loss: 0.5221 - val_auc: 0.8641 - val_loss: 0.4841
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 427ms/step - auc: 0.8398 - loss: 0.4856 - val_auc: 0.8665 - val_loss: 0.5098
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 426ms/step - auc: 0.8401 - loss: 0.4982 - val_auc: 0.8644 - val_loss: 0.4838
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 421ms/step - auc: 0.8457 - loss: 0.4872 - val_auc: 0.8658 - val_loss: 0.3265
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 426ms/step - auc: 0.8448 - loss: 0.4761 - val_auc: 0.8699 - val_loss: 0.4454
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 414ms/step - auc: 0.8526 - loss: 0.4691 - val_auc: 0.8705 - val_loss: 0.4479
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 427ms/step - auc: 0.8573 - loss: 0.4602 - val_auc: 0.8701 - val_loss: 0.4330
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 422ms/step - auc: 0.8660 - loss: 0.4538 - val_auc: 0.8709 - val_loss: 0.4631
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 431ms/step - auc: 0.8605 - loss: 0.4527 - val_auc: 0.8709 - val_loss: 0.4704
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 425ms/step - auc: 0.8505 - loss: 0.4692 - val_auc: 0.8710 - val_loss: 0.4436
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 414ms/step - auc: 0.8611 - loss: 0.4444 - val_auc: 0.8712 - val_loss: 0.4380
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 426ms/step - auc: 0.8655 - loss: 0.4442 - val_auc: 0.8709 - val_loss: 0.4221
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 427ms/step - auc: 0.8445 - loss: 0.4968 - val_auc: 0.8712 - val_loss: 0.3747
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 412ms/step - auc: 0.8492 - loss: 0.4882 - val_auc: 0.8733 - val_loss: 0.3701
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 414ms/step - auc: 0.8677 - loss: 0.4409 - val_auc: 0.8727 - val_loss: 0.4737
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 409ms/step - auc: 0.8622 - loss: 0.4515 - val_auc: 0.8754 - val_loss: 0.4552


same as above again

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 186s 1s/step - auc: 0.6821 - loss: 0.6279 - val_auc: 0.8470 - val_loss: 0.5118
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 449ms/step - auc: 0.8004 - loss: 0.5345 - val_auc: 0.8575 - val_loss: 0.5165
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 436ms/step - auc: 0.8283 - loss: 0.4896 - val_auc: 0.8617 - val_loss: 0.4489
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 432ms/step - auc: 0.8150 - loss: 0.5188 - val_auc: 0.8624 - val_loss: 0.4903
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 443ms/step - auc: 0.8347 - loss: 0.4941 - val_auc: 0.8626 - val_loss: 0.4816
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 433ms/step - auc: 0.8286 - loss: 0.5123 - val_auc: 0.8641 - val_loss: 0.4307
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 440ms/step - auc: 0.8511 - loss: 0.4752 - val_auc: 0.8695 - val_loss: 0.4172
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 431ms/step - auc: 0.8450 - loss: 0.4786 - val_auc: 0.8716 - val_loss: 0.4642
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 438ms/step - auc: 0.8424 - loss: 0.4885 - val_auc: 0.8699 - val_loss: 0.4009
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 433ms/step - auc: 0.8431 - loss: 0.4747 - val_auc: 0.8689 - val_loss: 0.4369
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 431ms/step - auc: 0.8476 - loss: 0.4670 - val_auc: 0.8700 - val_loss: 0.4720
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 433ms/step - auc: 0.8532 - loss: 0.4767 - val_auc: 0.8697 - val_loss: 0.4905
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 429ms/step - auc: 0.8452 - loss: 0.4791 - val_auc: 0.8696 - val_loss: 0.4628
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 437ms/step - auc: 0.8521 - loss: 0.4676 - val_auc: 0.8698 - val_loss: 0.4401
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 449ms/step - auc: 0.8482 - loss: 0.4751 - val_auc: 0.8699 - val_loss: 0.4380
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 438ms/step - auc: 0.8590 - loss: 0.4557 - val_auc: 0.8667 - val_loss: 0.3854
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 441ms/step - auc: 0.8469 - loss: 0.4766 - val_auc: 0.8684 - val_loss: 0.4443
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 439ms/step - auc: 0.8559 - loss: 0.4690 - val_auc: 0.8716 - val_loss: 0.3888
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 432ms/step - auc: 0.8630 - loss: 0.4455 - val_auc: 0.8711 - val_loss: 0.4012
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 429ms/step - auc: 0.8559 - loss: 0.4616 - val_auc: 0.8734 - val_loss: 0.4067




conditional hvflip, zoom, brightness on 128x128 images resized to 128x128 images with 256 batch size with 2e-3 learning rate

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 144s 851ms/step - auc: 0.6643 - loss: 0.6465 - val_auc: 0.8462 - val_loss: 0.5622
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 37s 318ms/step - auc: 0.7995 - loss: 0.5466 - val_auc: 0.8588 - val_loss: 0.4369
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 310ms/step - auc: 0.8197 - loss: 0.5049 - val_auc: 0.8633 - val_loss: 0.4813
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 311ms/step - auc: 0.8366 - loss: 0.4941 - val_auc: 0.8631 - val_loss: 0.4805
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 307ms/step - auc: 0.8382 - loss: 0.4934 - val_auc: 0.8633 - val_loss: 0.4846
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 312ms/step - auc: 0.8265 - loss: 0.4997 - val_auc: 0.8662 - val_loss: 0.4217
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 306ms/step - auc: 0.8263 - loss: 0.5030 - val_auc: 0.8716 - val_loss: 0.4543
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 317ms/step - auc: 0.8433 - loss: 0.4788 - val_auc: 0.8716 - val_loss: 0.4101
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 39s 340ms/step - auc: 0.8502 - loss: 0.4661 - val_auc: 0.8686 - val_loss: 0.4509
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 34s 302ms/step - auc: 0.8521 - loss: 0.4766 - val_auc: 0.8722 - val_loss: 0.4005
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 305ms/step - auc: 0.8556 - loss: 0.4655 - val_auc: 0.8709 - val_loss: 0.4194
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 305ms/step - auc: 0.8534 - loss: 0.4635 - val_auc: 0.8725 - val_loss: 0.4555
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 44s 395ms/step - auc: 0.8583 - loss: 0.4542 - val_auc: 0.8721 - val_loss: 0.4802
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 42s 367ms/step - auc: 0.8598 - loss: 0.4548 - val_auc: 0.8721 - val_loss: 0.4605
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 46s 395ms/step - auc: 0.8685 - loss: 0.4400 - val_auc: 0.8721 - val_loss: 0.4516
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 305ms/step - auc: 0.8578 - loss: 0.4628 - val_auc: 0.8670 - val_loss: 0.4252
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 301ms/step - auc: 0.8699 - loss: 0.4471 - val_auc: 0.8746 - val_loss: 0.5404
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 304ms/step - auc: 0.8519 - loss: 0.4707 - val_auc: 0.8713 - val_loss: 0.3192
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 316ms/step - auc: 0.8551 - loss: 0.4680 - val_auc: 0.8661 - val_loss: 0.3484
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 307ms/step - auc: 0.8577 - loss: 0.4616 - val_auc: 0.8649 - val_loss: 0.3657


same as above with parallel loading

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 133s 799ms/step - auc: 0.6720 - loss: 0.6229 - val_auc: 0.8245 - val_loss: 0.4761
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 311ms/step - auc: 0.8086 - loss: 0.5185 - val_auc: 0.8477 - val_loss: 0.5440
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 37s 316ms/step - auc: 0.8083 - loss: 0.5301 - val_auc: 0.8527 - val_loss: 0.4744
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 317ms/step - auc: 0.8415 - loss: 0.4888 - val_auc: 0.8530 - val_loss: 0.5154
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 315ms/step - auc: 0.8347 - loss: 0.4899 - val_auc: 0.8532 - val_loss: 0.4900
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 308ms/step - auc: 0.8353 - loss: 0.4947 - val_auc: 0.8580 - val_loss: 0.3815
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 311ms/step - auc: 0.8283 - loss: 0.5029 - val_auc: 0.8632 - val_loss: 0.5132
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 318ms/step - auc: 0.8392 - loss: 0.4853 - val_auc: 0.8633 - val_loss: 0.4844
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 313ms/step - auc: 0.8528 - loss: 0.4694 - val_auc: 0.8685 - val_loss: 0.4517
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 313ms/step - auc: 0.8432 - loss: 0.4875 - val_auc: 0.8671 - val_loss: 0.4791
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 313ms/step - auc: 0.8610 - loss: 0.4608 - val_auc: 0.8672 - val_loss: 0.4743
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 37s 320ms/step - auc: 0.8528 - loss: 0.4797 - val_auc: 0.8664 - val_loss: 0.4744
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 315ms/step - auc: 0.8590 - loss: 0.4507 - val_auc: 0.8677 - val_loss: 0.4825
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 304ms/step - auc: 0.8591 - loss: 0.4614 - val_auc: 0.8674 - val_loss: 0.4589
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 313ms/step - auc: 0.8529 - loss: 0.4673 - val_auc: 0.8675 - val_loss: 0.4573
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 318ms/step - auc: 0.8582 - loss: 0.4640 - val_auc: 0.8681 - val_loss: 0.3490
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 303ms/step - auc: 0.8483 - loss: 0.4714 - val_auc: 0.8672 - val_loss: 0.4965
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 304ms/step - auc: 0.8638 - loss: 0.4551 - val_auc: 0.8664 - val_loss: 0.4498
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 317ms/step - auc: 0.8528 - loss: 0.4628 - val_auc: 0.8683 - val_loss: 0.4186
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 37s 328ms/step - auc: 0.8484 - loss: 0.4674 - val_auc: 0.8712 - val_loss: 0.4515



256 im size -> 128 resize, with 1e-3 LR and 128 batch size

Epoch 1/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 171s 614ms/step - auc: 0.7071 - loss: 0.6086 - val_auc: 0.8394 - val_loss: 0.5356
Epoch 2/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 41s 187ms/step - auc: 0.7921 - loss: 0.5274 - val_auc: 0.8491 - val_loss: 0.4546
Epoch 3/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 181ms/step - auc: 0.8033 - loss: 0.5177 - val_auc: 0.8547 - val_loss: 0.4993
Epoch 4/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 181ms/step - auc: 0.8300 - loss: 0.5001 - val_auc: 0.8555 - val_loss: 0.5041
Epoch 5/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 182ms/step - auc: 0.8290 - loss: 0.4977 - val_auc: 0.8553 - val_loss: 0.4741
Epoch 6/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 184ms/step - auc: 0.8335 - loss: 0.4850 - val_auc: 0.8602 - val_loss: 0.4323
Epoch 7/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 181ms/step - auc: 0.8284 - loss: 0.4825 - val_auc: 0.8606 - val_loss: 0.4651
Epoch 8/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 182ms/step - auc: 0.8405 - loss: 0.4826 - val_auc: 0.8610 - val_loss: 0.4767
Epoch 9/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 41s 186ms/step - auc: 0.8453 - loss: 0.4807 - val_auc: 0.8616 - val_loss: 0.3702
Epoch 10/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 43s 185ms/step - auc: 0.8547 - loss: 0.4706 - val_auc: 0.8619 - val_loss: 0.4018
Epoch 11/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 181ms/step - auc: 0.8389 - loss: 0.4728 - val_auc: 0.8639 - val_loss: 0.4610
Epoch 12/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 181ms/step - auc: 0.8520 - loss: 0.4640 - val_auc: 0.8652 - val_loss: 0.4747
Epoch 13/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 41s 189ms/step - auc: 0.8496 - loss: 0.4697 - val_auc: 0.8655 - val_loss: 0.4654
Epoch 14/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 39s 179ms/step - auc: 0.8383 - loss: 0.4744 - val_auc: 0.8654 - val_loss: 0.4495
Epoch 15/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 183ms/step - auc: 0.8504 - loss: 0.4556 - val_auc: 0.8655 - val_loss: 0.4469
Epoch 16/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 39s 180ms/step - auc: 0.8534 - loss: 0.4656 - val_auc: 0.8644 - val_loss: 0.4235
Epoch 17/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 42s 193ms/step - auc: 0.8524 - loss: 0.4458 - val_auc: 0.8647 - val_loss: 0.4654
Epoch 18/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 58s 266ms/step - auc: 0.8529 - loss: 0.4494 - val_auc: 0.8673 - val_loss: 0.3859
Epoch 19/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 48s 210ms/step - auc: 0.8441 - loss: 0.4730 - val_auc: 0.8676 - val_loss: 0.3958
Epoch 20/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 184ms/step - auc: 0.8460 - loss: 0.4684 - val_auc: 0.8680 - val_loss: 0.3813



256 im size -> 224 resize, with 1e-3 LR and 128 batch size


Epoch 1/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 187s 669ms/step - auc: 0.6229 - loss: 0.6494 - val_auc: 0.8291 - val_loss: 0.5450
Epoch 2/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 394ms/step - auc: 0.7942 - loss: 0.5478 - val_auc: 0.8447 - val_loss: 0.4660
Epoch 3/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 395ms/step - auc: 0.8162 - loss: 0.5078 - val_auc: 0.8492 - val_loss: 0.4767
Epoch 4/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 85s 394ms/step - auc: 0.8313 - loss: 0.4984 - val_auc: 0.8502 - val_loss: 0.5031
Epoch 5/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 85s 392ms/step - auc: 0.8377 - loss: 0.4916 - val_auc: 0.8502 - val_loss: 0.4753
Epoch 6/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 85s 393ms/step - auc: 0.8131 - loss: 0.5028 - val_auc: 0.8563 - val_loss: 0.4300
Epoch 7/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 398ms/step - auc: 0.8252 - loss: 0.5057 - val_auc: 0.8586 - val_loss: 0.4027
Epoch 8/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 398ms/step - auc: 0.8457 - loss: 0.4800 - val_auc: 0.8592 - val_loss: 0.4128
Epoch 9/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 398ms/step - auc: 0.8546 - loss: 0.4564 - val_auc: 0.8596 - val_loss: 0.4704
Epoch 10/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 401ms/step - auc: 0.8522 - loss: 0.4623 - val_auc: 0.8635 - val_loss: 0.4380
Epoch 11/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 87s 401ms/step - auc: 0.8552 - loss: 0.4640 - val_auc: 0.8640 - val_loss: 0.4486
Epoch 12/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 85s 395ms/step - auc: 0.8533 - loss: 0.4612 - val_auc: 0.8647 - val_loss: 0.4887
Epoch 13/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 399ms/step - auc: 0.8511 - loss: 0.4523 - val_auc: 0.8644 - val_loss: 0.4686
Epoch 14/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 399ms/step - auc: 0.8631 - loss: 0.4521 - val_auc: 0.8643 - val_loss: 0.4567
Epoch 15/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 85s 395ms/step - auc: 0.8637 - loss: 0.4538 - val_auc: 0.8643 - val_loss: 0.4527
Epoch 16/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 396ms/step - auc: 0.8512 - loss: 0.4628 - val_auc: 0.8612 - val_loss: 0.4275
Epoch 17/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 87s 403ms/step - auc: 0.8591 - loss: 0.4573 - val_auc: 0.8660 - val_loss: 0.4427
Epoch 18/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 397ms/step - auc: 0.8600 - loss: 0.4578 - val_auc: 0.8657 - val_loss: 0.3732
Epoch 19/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 87s 402ms/step - auc: 0.8551 - loss: 0.4513 - val_auc: 0.8675 - val_loss: 0.3805
Epoch 20/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 401ms/step - auc: 0.8694 - loss: 0.4458 - val_auc: 0.8647 - val_loss: 0.3911



BFCE with gamma=1.0, start LR = 2e-3

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 203s 1s/step - auc: 0.6484 - loss: 0.3384 - recall: 0.5367 - val_auc: 0.8448 - val_loss: 0.1644 - val_recall: 0.5932
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 53s 451ms/step - auc: 0.8024 - loss: 0.2652 - recall: 0.7113 - val_auc: 0.8561 - val_loss: 0.2535 - val_recall: 0.8220
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 54s 460ms/step - auc: 0.8170 - loss: 0.2608 - recall: 0.7208 - val_auc: 0.8611 - val_loss: 0.2471 - val_recall: 0.8220
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 441ms/step - auc: 0.8363 - loss: 0.2451 - recall: 0.7590 - val_auc: 0.8639 - val_loss: 0.2354 - val_recall: 0.7881
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 442ms/step - auc: 0.8459 - loss: 0.2454 - recall: 0.8020 - val_auc: 0.8635 - val_loss: 0.2285 - val_recall: 0.7881
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 439ms/step - auc: 0.8327 - loss: 0.2464 - recall: 0.7298 - val_auc: 0.8609 - val_loss: 0.1658 - val_recall: 0.6864
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 54s 468ms/step - auc: 0.8495 - loss: 0.2350 - recall: 0.7678 - val_auc: 0.8619 - val_loss: 0.2424 - val_recall: 0.8051
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 54s 467ms/step - auc: 0.8355 - loss: 0.2503 - recall: 0.7422 - val_auc: 0.8644 - val_loss: 0.2174 - val_recall: 0.7797
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 53s 453ms/step - auc: 0.8556 - loss: 0.2357 - recall: 0.7881 - val_auc: 0.8632 - val_loss: 0.2505 - val_recall: 0.8305
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 445ms/step - auc: 0.8555 - loss: 0.2330 - recall: 0.7990 - val_auc: 0.8741 - val_loss: 0.2359 - val_recall: 0.8305
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 435ms/step - auc: 0.8597 - loss: 0.2342 - recall: 0.7680 - val_auc: 0.8746 - val_loss: 0.2316 - val_recall: 0.8051
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 432ms/step - auc: 0.8667 - loss: 0.2268 - recall: 0.7780 - val_auc: 0.8764 - val_loss: 0.2068 - val_recall: 0.7881
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 437ms/step - auc: 0.8770 - loss: 0.2196 - recall: 0.8184 - val_auc: 0.8765 - val_loss: 0.2152 - val_recall: 0.7966
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 450ms/step - auc: 0.8599 - loss: 0.2323 - recall: 0.7870 - val_auc: 0.8765 - val_loss: 0.2209 - val_recall: 0.8136
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 54s 464ms/step - auc: 0.8593 - loss: 0.2335 - recall: 0.7948 - val_auc: 0.8766 - val_loss: 0.2227 - val_recall: 0.8136
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 439ms/step - auc: 0.8338 - loss: 0.2429 - recall: 0.7582 - val_auc: 0.8790 - val_loss: 0.1998 - val_recall: 0.7966
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 447ms/step - auc: 0.8581 - loss: 0.2280 - recall: 0.7895 - val_auc: 0.8750 - val_loss: 0.2114 - val_recall: 0.7966
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 448ms/step - auc: 0.8541 - loss: 0.2397 - recall: 0.8130 - val_auc: 0.8662 - val_loss: 0.2371 - val_recall: 0.8136
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 432ms/step - auc: 0.8481 - loss: 0.2391 - recall: 0.7710 - val_auc: 0.8686 - val_loss: 0.2239 - val_recall: 0.7966
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 431ms/step - auc: 0.8616 - loss: 0.2253 - recall: 0.7732 - val_auc: 0.8757 - val_loss: 0.2216 - val_recall: 0.8220


Image + tabular model - concatenate before dropout

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.747239	0.301899	0.036603	0.663090	0.853451	0.265591	0.052067		0.822034
1	0.826349	0.258185	0.048969	0.748927	0.866693	0.175795	0.078125		0.677966
2	0.823671	0.260655	0.049678	0.744635	0.874862	0.257943	0.057571		0.847458
3	0.849761	0.243283	0.054363	0.766094	0.876901	0.231347	0.061671		0.788136
4	0.849320	0.242788	0.055853	0.761803	0.877371	0.219476	0.067003		0.788136
5	0.840105	0.250049	0.051100	0.757511	0.877494	0.335253	0.044758		0.915254
6	0.838506	0.251152	0.051670	0.770386	0.876938	0.178760	0.085189		0.745763
7	0.849765	0.241889	0.053933	0.748927	0.871399	0.230582	0.063732		0.822034
8	0.862795	0.232336	0.057766	0.778970	0.873680	0.160897	0.087973		0.669492
9	0.857649	0.237696	0.057908	0.798283	0.873545	0.256342	0.057292		0.838983


Image + tabular model - batchnorm after image, concatenate before dropout

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.710663	0.454379	0.031542	0.643777	0.814260	0.160241	0.091525		0.457627
1	0.787757	0.350359	0.041946	0.697425	0.847563	0.195894	0.065443		0.694915
2	0.813923	0.303099	0.047424	0.744635	0.864152	0.186974	0.071778		0.745763
3	0.826817	0.291715	0.051286	0.766094	0.871535	0.189923	0.072020		0.737288
4	0.840975	0.267355	0.054335	0.763949	0.872870	0.200781	0.072998		0.788136
5	0.824820	0.298856	0.050726	0.757511	0.845612	0.280786	0.053241		0.779661
6	0.829982	0.289678	0.050757	0.755365	0.840595	0.299545	0.050505		0.762712
7	0.845036	0.273628	0.054005	0.768240	0.859556	0.231588	0.066470		0.762712
8	0.838169	0.279787	0.053846	0.766094	0.857804	0.247839	0.060732		0.745763
9	0.843753	0.268427	0.051923	0.761803	0.878809	0.227556	0.069767		0.838983


Image + tabular model - batchnorm after image, concatenate after dropout

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.712089	0.444161	0.032651	0.678112	0.833374	0.140377	0.099593		0.415254
1	0.794279	0.337128	0.044705	0.721030	0.853547	0.197473	0.064360		0.677966
2	0.807739	0.314566	0.045796	0.744635	0.867802	0.198254	0.067315		0.703390
3	0.836564	0.275778	0.051494	0.761803	0.877215	0.192902	0.070957		0.728814
4	0.822433	0.292250	0.051324	0.748927	0.879566	0.209068	0.070518		0.796610
5	0.807789	0.314600	0.046305	0.727468	0.859715	0.315460	0.054933		0.872881
6	0.819325	0.300469	0.046512	0.738197	0.853305	0.237421	0.064828		0.796610
7	0.830601	0.289354	0.050658	0.759657	0.870972	0.244876	0.066576		0.830508
8	0.843237	0.267351	0.052632	0.766094	0.877986	0.232629	0.068843		0.822034
9	0.845955	0.267935	0.054253	0.759657	0.873457	0.208893	0.072568		0.771186



Image + tabular model with additional dense(32) layer
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.727580	0.429228	0.034028	0.703863	0.829059	0.202859	0.061685		0.601695
1	0.827311	0.275014	0.047414	0.781116	0.844682	0.190405	0.072529		0.677966
2	0.854223	0.239407	0.050969	0.796137	0.862649	0.228301	0.058357		0.872881
3	0.876469	0.225665	0.055618	0.843348	0.871493	0.179914	0.074853		0.754237
4	0.881354	0.217590	0.059126	0.839056	0.871459	0.176038	0.075207		0.771186
5	0.906044	0.194592	0.068403	0.873391	0.877113	0.192510	0.071975		0.796610
6	0.909833	0.190861	0.070604	0.879828	0.874408	0.182010	0.072968		0.745763
7	0.910858	0.188510	0.070492	0.864807	0.877534	0.185243	0.076480		0.788136
8	0.916857	0.183946	0.071316	0.875537	0.878160	0.178930	0.077645		0.771186
9	0.922464	0.177563	0.073015	0.881974	0.878020	0.177652	0.077187		0.762712


Image + tabular model with additional dense(64)+batchnorm layer
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.727318	0.411975	0.033211	0.740343	0.837663	0.174136	0.084615		0.559322
1	0.835796	0.255477	0.047486	0.802575	0.872012	0.195075	0.078901		0.754237
2	0.869116	0.225893	0.055429	0.834764	0.869100	0.204929	0.071799		0.822034
3	0.881753	0.216516	0.059817	0.843348	0.869578	0.187227	0.079435		0.762712
4	0.896247	0.204314	0.064809	0.858369	0.869212	0.175313	0.083029		0.771186
5	0.903415	0.198107	0.071104	0.839056	0.877330	0.187319	0.077430		0.796610
6	0.907038	0.194041	0.070239	0.832618	0.875775	0.185547	0.075758		0.762712
7	0.926051	0.176404	0.077677	0.884120	0.876845	0.177482	0.080717		0.762712
8	0.928241	0.173410	0.079384	0.873391	0.877134	0.177323	0.082353		0.771186
9	0.927760	0.174300	0.080305	0.881974	0.876743	0.177675	0.082652		0.771186


Image + tabular model using Convnexttiny with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.738875	0.328061	0.032748	0.768240	0.781421	0.185613	0.059524		0.508475
1	0.838089	0.250887	0.046549	0.800429	0.832211	0.211641	0.059413		0.703390
2	0.860552	0.232833	0.050950	0.793991	0.854418	0.222913	0.060664		0.805085
3	0.884240	0.214796	0.059274	0.847640	0.856867	0.204866	0.064677		0.771186
4	0.896566	0.203038	0.063223	0.856223	0.859649	0.197019	0.065918		0.745763
5	0.904613	0.196443	0.067645	0.860515	0.866402	0.194490	0.066982		0.720339
6	0.923590	0.178342	0.077349	0.899142	0.866197	0.186563	0.069275		0.720339
7	0.924516	0.177302	0.077790	0.888412	0.866506	0.181377	0.071181		0.694915
8	0.934593	0.166257	0.083011	0.920601	0.866508	0.179115	0.070609		0.677966
9	0.936880	0.164511	0.082755	0.920601	0.866531	0.181260	0.070496		0.686441


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.739537	0.335821	0.033422	0.755365	0.834421	0.177406	0.083655		0.550847
1	0.841872	0.247703	0.049165	0.789700	0.843131	0.183238	0.075543		0.677966
2	0.849015	0.244201	0.053929	0.798283	0.867039	0.198550	0.070669		0.788136
3	0.882410	0.218603	0.061529	0.841202	0.877820	0.165199	0.080203		0.669492
4	0.891029	0.208976	0.061924	0.821888	0.884643	0.191764	0.075918		0.788136
5	0.903079	0.198540	0.067069	0.858369	0.883058	0.178495	0.077739		0.745763
6	0.906770	0.194995	0.070410	0.858369	0.883562	0.166370	0.085127		0.737288
7	0.910018	0.192692	0.071801	0.856223	0.882422	0.169150	0.082772		0.728814
8	0.925678	0.177315	0.077440	0.890558	0.881585	0.172789	0.078269		0.720339
9	0.921616	0.180480	0.076706	0.873391	0.881383	0.176003	0.078876		0.737288


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm 20epochs
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.749452	0.328545	0.035155	0.751073	0.833574	0.198402	0.078231		0.584746
1	0.829623	0.257921	0.048470	0.768240	0.860957	0.178539	0.083805		0.627119
2	0.867337	0.230309	0.056760	0.809013	0.862802	0.185025	0.078846		0.694915
3	0.879351	0.220330	0.061748	0.826180	0.875404	0.203661	0.073810		0.788136
4	0.886426	0.214016	0.063310	0.817597	0.865528	0.189342	0.073975		0.703390
5	0.907283	0.195176	0.072566	0.879828	0.867222	0.163788	0.082912		0.694915
6	0.908510	0.192428	0.073928	0.843348	0.874330	0.186920	0.076790		0.754237
7	0.908373	0.192576	0.075608	0.854077	0.870529	0.223607	0.061892		0.754237
8	0.928257	0.171749	0.080560	0.888412	0.871014	0.162979	0.082661		0.694915
9	0.925429	0.174833	0.083662	0.864807	0.867999	0.158791	0.088486		0.703390
10	0.929189	0.170862	0.088254	0.875537	0.866107	0.166246	0.083008		0.720339
11	0.934294	0.165790	0.089716	0.894850	0.873791	0.146897	0.091116		0.677966
12	0.934795	0.165001	0.092917	0.875537	0.873487	0.151603	0.085805		0.686441
13	0.938579	0.159562	0.091735	0.888412	0.872016	0.140812	0.093863		0.661017
14	0.941164	0.155985	0.093842	0.892704	0.876088	0.143219	0.092857		0.661017
15	0.950369	0.145090	0.101094	0.912017	0.872728	0.143402	0.093381		0.669492
16	0.945732	0.151474	0.101897	0.899142	0.871373	0.145860	0.091224		0.669492
17	0.943966	0.152604	0.099880	0.890558	0.872101	0.144399	0.091549		0.661017
18	0.951547	0.143954	0.102817	0.916309	0.872128	0.144398	0.091228		0.661017
19	0.951739	0.143025	0.103883	0.918455	0.871892	0.145446	0.090803		0.661017


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm cosine_decay_restarts 20epochs
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.732306	0.347797	0.033037	0.733906	0.794159	0.169527	0.068256		0.533898
1	0.834590	0.251499	0.045710	0.785408	0.850143	0.218173	0.062054		0.737288
2	0.856309	0.237276	0.052393	0.817597	0.860196	0.199043	0.066038		0.711864
3	0.883159	0.218555	0.059909	0.849785	0.867793	0.197883	0.071770		0.762712
4	0.891039	0.210981	0.060709	0.845494	0.867879	0.207243	0.067159		0.771186
5	0.864645	0.233352	0.058714	0.813305	0.853949	0.250885	0.056213		0.805085
6	0.883568	0.215352	0.061120	0.843348	0.860864	0.222580	0.062971		0.779661
7	0.888218	0.210142	0.063062	0.828326	0.877603	0.213737	0.070545		0.822034
8	0.906694	0.193540	0.068574	0.869099	0.880582	0.189728	0.075243		0.788136
9	0.914019	0.186191	0.072433	0.873391	0.876490	0.174360	0.079929		0.762712
10	0.923001	0.178126	0.077668	0.877682	0.878537	0.178798	0.081365		0.788136
11	0.923587	0.177672	0.081070	0.871245	0.876521	0.169507	0.082863		0.745763
12	0.933647	0.165279	0.083084	0.892704	0.874991	0.168520	0.080229		0.711864
13	0.936928	0.162773	0.084821	0.907725	0.875398	0.166167	0.081633		0.711864
14	0.938553	0.161042	0.087651	0.918455	0.875474	0.167964	0.080769		0.711864
15	0.910782	0.191505	0.078137	0.849785	0.857080	0.226948	0.063933		0.779661
16	0.905964	0.194025	0.071734	0.862661	0.852186	0.217756	0.064906		0.728814
17	0.914418	0.185964	0.074981	0.860515	0.859287	0.179018	0.077261		0.745763
18	0.911767	0.188509	0.075540	0.847640	0.868647	0.199113	0.068095		0.754237
19	0.921855	0.177959	0.077713	0.860515	0.857842	0.180701	0.076923		0.711864



with 256x256 resized images
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.747419	0.326396	0.036463	0.701717	0.846849	0.231382	0.063444		0.711864
1	0.836752	0.253954	0.051162	0.798283	0.851210	0.230143	0.060797		0.737288
2	0.858665	0.236325	0.054564	0.787554	0.864220	0.240560	0.057416		0.813559
3	0.878099	0.220079	0.060239	0.821888	0.877492	0.191836	0.072848		0.745763
4	0.886156	0.212726	0.063174	0.817597	0.871360	0.204293	0.066912		0.771186
5	0.903848	0.197598	0.069055	0.851931	0.886850	0.187075	0.078008		0.796610
6	0.916598	0.185380	0.077681	0.879828	0.879883	0.179236	0.080762		0.754237
7	0.920823	0.181809	0.080428	0.871245	0.879457	0.173687	0.079737		0.720339
8	0.922475	0.178940	0.081314	0.871245	0.879336	0.176426	0.077899		0.728814
9	0.927139	0.174477	0.080572	0.881974	0.879021	0.178296	0.076649		0.728814



Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm finetune with 64 batch size

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.754251	0.312937	0.035091	0.751073	0.861417	0.232315	0.059343		0.796610
1	0.835903	0.251321	0.048806	0.789700	0.870475	0.225576	0.061224		0.788136
2	0.860274	0.234583	0.054171	0.778970	0.868440	0.219889	0.062026		0.762712
3	0.883218	0.215873	0.061419	0.815451	0.873688	0.218033	0.064120		0.796610
4	0.892777	0.207348	0.066311	0.854077	0.873645	0.225841	0.062992		0.813559
5	0.902849	0.198973	0.068194	0.843348	0.875085	0.198686	0.069859		0.754237
6	0.909809	0.190634	0.071633	0.860515	0.876950	0.198373	0.071047		0.788136
7	0.919819	0.182135	0.077331	0.875537	0.876998	0.191955	0.074196		0.762712
8	0.921769	0.179201	0.080039	0.877682	0.876562	0.187254	0.075745		0.754237
9	0.926708	0.174456	0.081097	0.888412	0.876131	0.187438	0.075939		0.754237

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.880855	0.217782	0.062122	0.793991	0.872893	0.203335	0.065265		0.762712
1	0.899489	0.200690	0.066689	0.864807	0.882338	0.217627	0.068056		0.830508
2	0.913671	0.187781	0.071797	0.866953	0.858795	0.211506	0.065598		0.762712
3	0.934696	0.165542	0.080651	0.892704	0.871614	0.174518	0.079777		0.728814
4	0.942817	0.156044	0.089651	0.903434	0.871159	0.155393	0.089958		0.728814


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm finetune with 256 batch size

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.732085	0.346150	0.033799	0.731760	0.834497	0.233410	0.054820		0.737288
1	0.833523	0.253622	0.048396	0.789700	0.848678	0.198135	0.061588		0.644068
2	0.862237	0.233260	0.054078	0.798283	0.846724	0.183116	0.069973		0.669492
3	0.880978	0.217222	0.060761	0.815451	0.854124	0.187928	0.072300		0.703390
4	0.891616	0.208489	0.066145	0.834764	0.857645	0.186636	0.071924		0.703390
5	0.899201	0.202050	0.066838	0.839056	0.863173	0.182506	0.072680		0.703390
6	0.909206	0.193386	0.073623	0.843348	0.861592	0.178357	0.071889		0.661017
7	0.921773	0.182079	0.076449	0.877682	0.862132	0.170947	0.076329		0.669492
8	0.924643	0.177677	0.077477	0.877682	0.860545	0.173568	0.075670		0.669492
9	0.926197	0.176813	0.079655	0.890558	0.860457	0.176769	0.076279		0.694915

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.886577	0.214948	0.068663	0.806867	0.857931	0.210755	0.064444		0.737288
1	0.920658	0.181429	0.074507	0.875537	0.861494	0.181110	0.068284		0.644068
2	0.927132	0.174850	0.080008	0.881974	0.857888	0.167349	0.076476		0.669492
3	0.943453	0.157784	0.087132	0.927039	0.856923	0.173372	0.077277		0.711864
4	0.944362	0.156203	0.092685	0.916309	0.862220	0.150915	0.084071		0.644068


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm finetune with 64 batch size with dropout 0.3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.768221	0.297619	0.038749	0.768240	0.843908	0.290486	0.046685		0.847458
1	0.830589	0.254151	0.047319	0.789700	0.853306	0.230978	0.058259		0.771186
2	0.844365	0.245696	0.051135	0.802575	0.860203	0.226324	0.058710		0.771186
3	0.862008	0.231321	0.052617	0.804721	0.867639	0.210254	0.062987		0.754237
4	0.874739	0.221868	0.058420	0.804721	0.858690	0.223080	0.059607		0.720339
5	0.886282	0.213078	0.060682	0.828326	0.871829	0.188204	0.073883		0.728814
6	0.902344	0.199867	0.069413	0.849785	0.876688	0.188944	0.073024		0.720339
7	0.907493	0.193647	0.071021	0.854077	0.875265	0.187505	0.073276		0.720339
8	0.914393	0.187423	0.074067	0.877682	0.876304	0.183942	0.076450		0.737288
9	0.911238	0.190077	0.072445	0.851931	0.876006	0.184316	0.076585		0.737288


	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.871327	0.225876	0.062554	0.781116	0.862063	0.231866	0.059711		0.771186
1	0.902840	0.198774	0.066510	0.847640	0.867239	0.182873	0.073386		0.703390
2	0.920328	0.181775	0.073050	0.884120	0.867399	0.165448	0.080392		0.694915
3	0.929873	0.171242	0.078773	0.892704	0.870862	0.176119	0.082375		0.728814
4	0.941879	0.158592	0.086680	0.907725	0.870224	0.170887	0.079102		0.686441


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm finetune with 64 batch size with label_smoothing 0.05

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.734993	0.331415	0.033142	0.706009	0.841938	0.268185	0.049139		0.822034
1	0.824071	0.262398	0.047148	0.787554	0.862111	0.223510	0.059063		0.737288
2	0.853037	0.241554	0.053116	0.791846	0.879942	0.222744	0.063136		0.788136
3	0.871683	0.229187	0.057854	0.813305	0.875741	0.226183	0.063898		0.847458
4	0.889228	0.215094	0.063809	0.856223	0.876638	0.205863	0.066874		0.728814
5	0.905231	0.199719	0.069307	0.841202	0.875483	0.183887	0.078040		0.728814
6	0.906411	0.198248	0.072062	0.854077	0.875849	0.183150	0.076854		0.728814
7	0.920093	0.185517	0.078185	0.869099	0.879916	0.187349	0.075983		0.737288
8	0.915695	0.188809	0.076276	0.856223	0.880368	0.188476	0.076790		0.754237
9	0.919311	0.186382	0.079969	0.892704	0.880678	0.185005	0.078691		0.754237

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.874632	0.230745	0.064607	0.785408	0.863148	0.201380	0.070079		0.754237
1	0.917538	0.187955	0.074040	0.864807	0.872235	0.188314	0.077899		0.728814
2	0.917891	0.187304	0.075279	0.854077	0.863677	0.183454	0.075092		0.694915
3	0.927593	0.178274	0.082574	0.875537	0.877644	0.204525	0.067939		0.754237
4	0.948123	0.155689	0.091796	0.929185	0.875704	0.165066	0.086189		0.703390


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm finetune with 64 batch size with label_smoothing 0.1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.744174	0.322000	0.035304	0.718884	0.851954	0.253010	0.059682		0.762712
1	0.828056	0.262073	0.047904	0.755365	0.861611	0.257688	0.056086		0.796610
2	0.852242	0.245549	0.052287	0.787554	0.868310	0.243313	0.060645		0.796610
3	0.861072	0.239434	0.055489	0.798283	0.870396	0.232436	0.064473		0.813559
4	0.885890	0.221357	0.061512	0.834764	0.865819	0.210470	0.069767		0.762712
5	0.896949	0.211436	0.067124	0.839056	0.876656	0.216766	0.069733		0.796610
6	0.911115	0.199342	0.072978	0.851931	0.879549	0.199484	0.078231		0.779661
7	0.916121	0.194414	0.076923	0.862661	0.878846	0.193523	0.080108		0.754237
8	0.924119	0.186484	0.079648	0.873391	0.879180	0.191890	0.079323		0.754237
9	0.927612	0.183744	0.082282	0.881974	0.878971	0.190701	0.079351		0.745763

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.880834	0.226587	0.067827	0.802575	0.859894	0.211873	0.067124		0.745763
1	0.908870	0.202704	0.069341	0.860515	0.872794	0.203752	0.071542		0.762712
2	0.926032	0.185845	0.080959	0.884120	0.868729	0.188647	0.081951		0.711864
3	0.935652	0.175529	0.084281	0.890558	0.865712	0.180507	0.083333		0.720339
4	0.948842	0.160110	0.096137	0.918455	0.864698	0.181437	0.082745		0.694915


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm finetune with 64 batch size with label_smoothing 0.05, decay for finetuning corrected (based on finetune_epochs)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.735785	0.328375	0.033149	0.718884	0.838809	0.267181	0.044688		0.830508
1	0.833224	0.256345	0.047333	0.776824	0.866512	0.238930	0.063110		0.822034
2	0.852580	0.242573	0.051538	0.798283	0.866929	0.205139	0.067141		0.720339
3	0.871350	0.228743	0.058922	0.828326	0.873732	0.226126	0.066852		0.813559
4	0.887474	0.214761	0.063450	0.824034	0.872934	0.213059	0.067480		0.796610
5	0.892909	0.209408	0.063997	0.836910	0.877241	0.197371	0.077047		0.813559
6	0.902453	0.201652	0.068493	0.847640	0.881910	0.193819	0.079729		0.796610
7	0.919454	0.185716	0.076059	0.886266	0.878377	0.190646	0.080440		0.805085
8	0.925127	0.179882	0.078878	0.899142	0.879933	0.190758	0.080577		0.805085
9	0.927244	0.177527	0.079511	0.892704	0.879416	0.186088	0.083627		0.805085

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.879561	0.226800	0.066620	0.811159	0.868332	0.221308	0.061064		0.788136
1	0.910922	0.194757	0.070761	0.860515	0.865380	0.182536	0.076256		0.745763
2	0.934042	0.172401	0.081806	0.909871	0.877240	0.194414	0.076133		0.754237
3	0.944617	0.160913	0.089580	0.920601	0.879838	0.176126	0.083333		0.737288
4	0.949215	0.155123	0.094771	0.933476	0.879348	0.173568	0.084466		0.737288



Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm finetune with 64 batch size with label_smoothing 0.05, 
	decay for finetuning corrected (based on finetune_epochs), finetuning with same start lr as initial training

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.749789	0.315899	0.034980	0.723176	0.853420	0.247988	0.058680		0.813559
1	0.829496	0.256804	0.046929	0.757511	0.835633	0.241826	0.052894		0.720339
2	0.855147	0.240568	0.054240	0.811159	0.856045	0.249654	0.055357		0.788136
3	0.870939	0.227470	0.057664	0.834764	0.861538	0.236526	0.057392		0.779661
4	0.889153	0.212827	0.063495	0.830472	0.863100	0.226141	0.061947		0.771186
5	0.891764	0.210050	0.063851	0.836910	0.868012	0.198051	0.071605		0.737288
6	0.909960	0.195851	0.072810	0.856223	0.867942	0.185714	0.075288		0.720339
7	0.917039	0.187586	0.075556	0.866953	0.868402	0.189195	0.076450		0.737288
8	0.921236	0.184096	0.078636	0.871245	0.869398	0.186382	0.076923		0.728814
9	0.923284	0.181425	0.078917	0.869099	0.869790	0.183436	0.077555		0.720339

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.776253	0.312026	0.043940	0.693133	0.853880	0.265882	0.049630		0.796610
1	0.841921	0.252917	0.051927	0.766094	0.869191	0.226522	0.065333		0.830508
2	0.881893	0.221957	0.062351	0.839056	0.884838	0.239997	0.067327		0.864407
3	0.901098	0.205923	0.066542	0.841202	0.891689	0.205351	0.077105		0.830508
4	0.923039	0.186367	0.074986	0.881974	0.894985	0.203912	0.079545		0.830508


start_lr=1e-3 warmup_target=3e-3 alpha=1e-4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.757138	0.314190	0.036386	0.746781	0.805860	0.272871	0.044926		0.720339
1	0.802633	0.275155	0.042190	0.727468	0.854622	0.260967	0.051491		0.805085
2	0.839527	0.251757	0.049369	0.781116	0.860032	0.267591	0.053073		0.805085
3	0.863451	0.233697	0.055923	0.817597	0.871258	0.210192	0.067732		0.754237
4	0.871153	0.227757	0.057384	0.815451	0.876729	0.243336	0.059281		0.838983
5	0.887448	0.215304	0.064952	0.845494	0.877385	0.190793	0.085878		0.762712
6	0.894637	0.208269	0.066479	0.811159	0.880699	0.192661	0.075666		0.745763
7	0.905653	0.198692	0.070507	0.856223	0.883905	0.187201	0.079181		0.754237
8	0.914174	0.191057	0.075561	0.851931	0.882498	0.186658	0.075540		0.711864
9	0.916435	0.188868	0.076411	0.862661	0.884232	0.184398	0.077063		0.720339


start_lr=1e-3 warmup_target=5e-3 alpha=1e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.738895	0.322081	0.033400	0.751073	0.813865	0.292963	0.047212		0.796610
1	0.811658	0.269686	0.045335	0.776824	0.847714	0.247615	0.054176		0.813559
2	0.827163	0.259778	0.047348	0.768240	0.868446	0.232515	0.060399		0.822034
3	0.857876	0.238694	0.052881	0.789700	0.860808	0.256215	0.056265		0.822034
4	0.859661	0.238047	0.053434	0.774678	0.863945	0.214778	0.063875		0.762712
5	0.864262	0.231699	0.054743	0.811159	0.874675	0.223076	0.061975		0.813559
6	0.877242	0.222913	0.059203	0.806867	0.866805	0.223617	0.062249		0.788136
7	0.893745	0.210517	0.065260	0.849785	0.880175	0.203139	0.068098		0.779661
8	0.908760	0.195257	0.069454	0.862661	0.881670	0.187602	0.076588		0.745763
9	0.918217	0.186402	0.075000	0.869099	0.881986	0.190471	0.078809		0.762712



start_lr=1e-3 warmup_target=3e-3 alpha=(1e-4/3e-3) (final_target=1e-4)
1e-3 to 3e-3 to 1e-4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.746771	0.310892	0.032179	0.789700	0.832049	0.263441	0.046534		0.813559
1	0.821413	0.262724	0.043125	0.798283	0.850258	0.251965	0.051706		0.822034
2	0.842680	0.248604	0.047805	0.813305	0.858810	0.226705	0.057989		0.796610
3	0.845897	0.245679	0.049960	0.813305	0.855780	0.233566	0.054367		0.796610
4	0.856759	0.237153	0.051116	0.811159	0.868803	0.231968	0.058096		0.796610
5	0.881796	0.218602	0.058105	0.847640	0.859419	0.198284	0.068779		0.754237
6	0.889001	0.212425	0.062997	0.849785	0.861184	0.218800	0.060852		0.762712
7	0.904457	0.200322	0.068193	0.856223	0.869718	0.193125	0.071309		0.720339
8	0.906760	0.197930	0.073333	0.849785	0.872624	0.194329	0.075360		0.754237
9	0.915458	0.188811	0.074889	0.869099	0.873852	0.188106	0.074742		0.737288


start_lr=1e-3 warmup_target=5e-3 alpha=(1e-3/5e-3) (final_target=1e-3)
1e-3 to 5e-3 to 1e-3

	auc	loss	precision	recall	val_auc	val_loss	val_precision	val_recall
0	0.738117	0.328837	0.034926	0.746781	0.811803	0.293274	0.043191	0.830508
1	0.826458	0.261270	0.048014	0.770386	0.808222	0.301380	0.041961	0.754237
2	0.838109	0.253143	0.049600	0.772532	0.843677	0.248660	0.056483	0.745763
3	0.845838	0.245880	0.049668	0.770386	0.851595	0.262876	0.052784	0.771186
4	0.849386	0.242935	0.051004	0.796137	0.860560	0.210553	0.064151	0.720339
5	0.856322	0.238547	0.053878	0.800429	0.870070	0.219013	0.066527	0.805085
6	0.883574	0.217395	0.060784	0.821888	0.871457	0.180631	0.074800	0.711864
7	0.888423	0.212799	0.062400	0.836910	0.873423	0.193172	0.070132	0.720339
8	0.907350	0.196570	0.070823	0.860515	0.881130	0.179995	0.077059	0.737288
9	0.909969	0.193721	0.072324	0.866953	0.880777	0.182944	0.075993	0.745763


1e-3 to 1e-2 to 1e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.746518	0.318423	0.034802	0.725322	0.825794	0.273965	0.047856		0.813559
1	0.814541	0.267417	0.043206	0.740343	0.853145	0.253219	0.053215		0.813559
2	0.835772	0.254370	0.049775	0.783262	0.856761	0.228203	0.065809		0.737288
3	0.846527	0.246804	0.051387	0.783262	0.864460	0.238682	0.058599		0.779661
4	0.848887	0.245260	0.053485	0.785408	0.858849	0.169571	0.090113		0.610169
5	0.852699	0.242830	0.052952	0.766094	0.866194	0.187827	0.074202		0.728814
6	0.873036	0.226411	0.057958	0.811159	0.880037	0.204189	0.071594		0.788136
7	0.891920	0.209975	0.064976	0.830472	0.885495	0.214279	0.069920		0.813559
8	0.899031	0.204087	0.067621	0.841202	0.889060	0.203015	0.075337		0.805085
9	0.910976	0.192216	0.073024	0.834764	0.888726	0.195198	0.075331		0.771186


1e-3 to 5e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740014	0.322364	0.033848	0.706009	0.793083	0.309429	0.042174		0.762712
1	0.818121	0.266512	0.046852	0.761803	0.833800	0.280669	0.049552		0.796610
2	0.832644	0.257503	0.049902	0.761803	0.860678	0.253817	0.055931		0.771186
3	0.859019	0.239223	0.057358	0.800429	0.837817	0.310548	0.047328		0.788136
4	0.861220	0.237093	0.054778	0.798283	0.844372	0.212853	0.063150		0.720339
5	0.858053	0.239383	0.055623	0.781116	0.855210	0.280455	0.052839		0.796610
6	0.880657	0.220423	0.061179	0.819743	0.855152	0.250730	0.057070		0.762712
7	0.877348	0.222337	0.059906	0.817597	0.856144	0.246030	0.056527		0.822034
8	0.889607	0.212376	0.062796	0.826180	0.855736	0.217382	0.062412		0.754237
9	0.885263	0.215678	0.063791	0.834764	0.860739	0.193229	0.070234		0.711864


1e-3 to 1e-4 

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.719406	0.354666	0.031955	0.729614	0.830636	0.288253	0.047642		0.855932
1	0.825407	0.262515	0.046269	0.798283	0.855781	0.261168	0.053326		0.855932
2	0.850859	0.243583	0.051937	0.811159	0.857025	0.226845	0.065052		0.796610
3	0.859169	0.236194	0.053046	0.781116	0.857997	0.230656	0.059050		0.779661
4	0.875506	0.223713	0.057372	0.813305	0.868139	0.220000	0.067749		0.813559
5	0.894026	0.209368	0.063641	0.830472	0.872635	0.217393	0.065688		0.796610
6	0.899882	0.204586	0.068589	0.862661	0.866035	0.212790	0.066717		0.745763
7	0.912625	0.192564	0.071834	0.869099	0.871856	0.202202	0.069433		0.737288
8	0.916268	0.189373	0.073766	0.856223	0.873800	0.198245	0.070617		0.737288
9	0.919425	0.185512	0.075415	0.866953	0.874848	0.192278	0.074136		0.745763


1e-3 to 5e-3 to 5e-6

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.748701	0.326667	0.033576	0.731760	0.846645	0.260026	0.050706		0.822034
1	0.811196	0.270688	0.042932	0.776824	0.831995	0.275020	0.046237		0.796610
2	0.841064	0.249531	0.050422	0.793991	0.869728	0.246439	0.057333		0.805085
3	0.843499	0.248997	0.051617	0.787554	0.870262	0.259808	0.057613		0.830508
4	0.864799	0.233080	0.057156	0.817597	0.856282	0.213302	0.066327		0.771186
5	0.877548	0.223721	0.059669	0.828326	0.869329	0.223399	0.066761		0.796610
6	0.884437	0.217318	0.061132	0.839056	0.875247	0.217212	0.066230		0.771186
7	0.893957	0.208505	0.064976	0.841202	0.863330	0.197994	0.074074		0.728814
8	0.900903	0.202630	0.069538	0.845494	0.869271	0.217744	0.065539		0.788136
9	0.913998	0.191181	0.074277	0.871245	0.869657	0.199611	0.070258		0.762712


1e-3 to 1e-2 to 5e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.735769	0.335557	0.034610	0.746781	0.834074	0.287127	0.045695		0.805085
1	0.806559	0.273673	0.043393	0.755365	0.843609	0.269929	0.047809		0.813559
2	0.834610	0.254783	0.048573	0.770386	0.848905	0.246966	0.057070		0.762712
3	0.846817	0.245258	0.049207	0.778970	0.871215	0.215259	0.066713		0.805085
4	0.844721	0.249129	0.051742	0.761803	0.860595	0.264334	0.055724		0.779661
5	0.862678	0.235818	0.055387	0.785408	0.873749	0.295180	0.047170		0.847458
6	0.867079	0.233230	0.057203	0.815451	0.884119	0.174224	0.083930		0.694915
7	0.879699	0.219458	0.058572	0.821888	0.884769	0.262781	0.049342		0.889831
8	0.882483	0.217863	0.058929	0.847640	0.881007	0.210351	0.070853		0.745763
9	0.888632	0.211843	0.061655	0.841202	0.887141	0.192364	0.076222		0.779661


5e-3 to 1e-2 to 1e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.758691	0.306146	0.036249	0.701717	0.859744	0.270729	0.048654		0.872881
1	0.833777	0.255935	0.049056	0.763949	0.860588	0.244997	0.051917		0.745763
2	0.844002	0.247322	0.052549	0.791846	0.864390	0.211219	0.069656		0.737288
3	0.851886	0.241816	0.052593	0.778970	0.865782	0.212146	0.068356		0.754237
4	0.862812	0.233600	0.055388	0.787554	0.870931	0.194017	0.075306		0.728814
5	0.866405	0.231459	0.056719	0.796137	0.878194	0.207782	0.071317		0.771186
6	0.878703	0.224189	0.063259	0.804721	0.869234	0.271316	0.055743		0.838983
7	0.882464	0.218923	0.061953	0.834764	0.883803	0.201839	0.072093		0.788136
8	0.899418	0.203990	0.067001	0.830472	0.886252	0.209584	0.069424		0.796610
9	0.905183	0.198048	0.070038	0.841202	0.884184	0.205469	0.071266		0.796610
	

1e-3 to 1e-2 to 1e-4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.734459	0.319617	0.031993	0.733906	0.838710	0.270890	0.047572		0.813559
1	0.813055	0.267070	0.043011	0.763949	0.858574	0.260820	0.051808		0.813559
2	0.829591	0.257286	0.045217	0.759657	0.857219	0.265399	0.052822		0.864407
3	0.839887	0.250525	0.049850	0.783262	0.871225	0.226607	0.066914		0.762712
4	0.853167	0.243111	0.054070	0.776824	0.873881	0.184780	0.080913		0.661017
5	0.850691	0.241891	0.052389	0.804721	0.876501	0.237580	0.067350		0.779661
6	0.872945	0.226336	0.058233	0.821888	0.871409	0.178588	0.084608		0.703390
7	0.887103	0.215151	0.061631	0.817597	0.873771	0.214554	0.068182		0.762712
8	0.891085	0.213152	0.063833	0.841202	0.885969	0.206300	0.070527		0.805085
9	0.906495	0.197477	0.069264	0.858369	0.886675	0.191066	0.074790		0.754237

------------------------------------------------------------------------------------------------------------
------best------

1e-3 to 1e-2 to 1e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.746518	0.318423	0.034802	0.725322	0.825794	0.273965	0.047856		0.813559
1	0.814541	0.267417	0.043206	0.740343	0.853145	0.253219	0.053215		0.813559
2	0.835772	0.254370	0.049775	0.783262	0.856761	0.228203	0.065809		0.737288
3	0.846527	0.246804	0.051387	0.783262	0.864460	0.238682	0.058599		0.779661
4	0.848887	0.245260	0.053485	0.785408	0.858849	0.169571	0.090113		0.610169
5	0.852699	0.242830	0.052952	0.766094	0.866194	0.187827	0.074202		0.728814
6	0.873036	0.226411	0.057958	0.811159	0.880037	0.204189	0.071594		0.788136
7	0.891920	0.209975	0.064976	0.830472	0.885495	0.214279	0.069920		0.813559
8	0.899031	0.204087	0.067621	0.841202	0.889060	0.203015	0.075337		0.805085
9	0.910976	0.192216	0.073024	0.834764	0.888726	0.195198	0.075331		0.771186

------------------------------------------------------------------------------------------------------------

1e-3 to 1e-2 (5 epochs) to 1e-3 (15 epochs)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.729652	0.335525	0.034486	0.725322	0.829353	0.285550	0.046956		0.771186
1	0.812287	0.271022	0.046356	0.742489	0.816647	0.305724	0.047811		0.805085
2	0.843603	0.250568	0.053110	0.789700	0.842103	0.261252	0.058290		0.762712
3	0.836509	0.253108	0.050622	0.759657	0.842022	0.276122	0.052066		0.779661
4	0.853164	0.241842	0.053525	0.798283	0.859851	0.211696	0.066090		0.711864
5	0.857368	0.237740	0.053411	0.804721	0.863787	0.235112	0.060887		0.779661
6	0.869917	0.229577	0.056935	0.806867	0.874442	0.172602	0.085202		0.644068
7	0.862547	0.234965	0.054074	0.796137	0.864538	0.223019	0.061252		0.779661
8	0.874816	0.224172	0.056029	0.817597	0.863608	0.258534	0.057797		0.813559
9	0.883653	0.217760	0.060556	0.864807	0.865043	0.215108	0.063003		0.796610
10	0.883523	0.219065	0.061344	0.832618	0.861996	0.258427	0.055948		0.805085
11	0.892929	0.209301	0.062922	0.819743	0.875552	0.179618	0.083571		0.745763
12	0.898207	0.205509	0.065459	0.845494	0.877010	0.202735	0.071086		0.754237
13	0.901965	0.201644	0.068077	0.856223	0.870617	0.173607	0.087318		0.711864
14	0.907789	0.195951	0.071507	0.843348	0.883916	0.196417	0.070379		0.771186
15	0.914328	0.190759	0.074691	0.856223	0.880893	0.190156	0.072864		0.737288
16	0.920247	0.183186	0.074667	0.890558	0.880787	0.170856	0.079074		0.694915
17	0.924885	0.177630	0.081229	0.890558	0.884135	0.177267	0.078652		0.711864
18	0.931549	0.170542	0.084091	0.892704	0.884311	0.169864	0.083416		0.711864
19	0.922839	0.180049	0.080182	0.869099	0.881785	0.196242	0.074979		0.754237


1e-3 to 1e-2 (5 epochs) to 1e-4 (15 epochs)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.734726	0.330987	0.034602	0.729614	0.840796	0.277976	0.046500		0.805085
1	0.820833	0.264306	0.047001	0.761803	0.852400	0.254252	0.055524		0.822034
2	0.845280	0.248824	0.053201	0.802575	0.841389	0.236716	0.058543		0.728814
3	0.843678	0.247000	0.050466	0.766094	0.860122	0.252322	0.058421		0.796610
4	0.857188	0.241094	0.056960	0.802575	0.858158	0.276148	0.058191		0.779661
5	0.864578	0.233042	0.058730	0.789700	0.852716	0.223977	0.067946		0.720339
6	0.866648	0.234414	0.061340	0.787554	0.871233	0.226398	0.071429		0.745763
7	0.876345	0.226604	0.065622	0.809013	0.876815	0.275862	0.058217		0.813559
8	0.873229	0.228410	0.061944	0.806867	0.875082	0.179548	0.079397		0.669492
9	0.890367	0.212643	0.066826	0.839056	0.863488	0.200759	0.067599		0.737288
10	0.884673	0.216488	0.063589	0.830472	0.878160	0.244351	0.062865		0.822034
11	0.882091	0.221051	0.063484	0.821888	0.874571	0.171455	0.088071		0.669492
12	0.899775	0.203269	0.068892	0.828326	0.879505	0.209991	0.071210		0.788136
13	0.900992	0.204094	0.069644	0.843348	0.881479	0.179328	0.079926		0.728814
14	0.910199	0.193333	0.074150	0.856223	0.871341	0.181769	0.080492		0.720339
15	0.917382	0.185749	0.076484	0.862661	0.877463	0.216653	0.068095		0.754237
16	0.923488	0.180238	0.081690	0.879828	0.878751	0.179669	0.080229		0.711864
17	0.924162	0.178357	0.080484	0.884120	0.876511	0.189940	0.077063		0.720339
18	0.933842	0.168099	0.085536	0.881974	0.877231	0.181605	0.080374		0.728814
19	0.928320	0.174034	0.085093	0.881974	0.877281	0.178535	0.081262		0.720339


1e-3 to 1e-2 (5 epochs) to 1e-3 (5 epochs) + 10 epochs

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.742740	0.311941	0.032915	0.793991	0.843634	0.234350	0.054184		0.762712
1	0.825272	0.259651	0.046395	0.793991	0.861118	0.270730	0.055268		0.813559
2	0.815390	0.265238	0.044731	0.766094	0.876588	0.231769	0.063471		0.796610
3	0.843364	0.248089	0.051308	0.774678	0.867534	0.236806	0.061489		0.805085
4	0.858414	0.235987	0.051033	0.811159	0.863857	0.270136	0.044379		0.889831
5	0.857471	0.240962	0.056105	0.778970	0.861652	0.223065	0.060541		0.720339
6	0.869709	0.231240	0.057704	0.813305	0.837356	0.226755	0.058780		0.677966
7	0.876996	0.222180	0.058714	0.811159	0.872048	0.225457	0.062374		0.788136
8	0.893990	0.210335	0.065882	0.841202	0.881244	0.185600	0.076991		0.737288
9	0.909603	0.193808	0.072694	0.845494	0.880407	0.192641	0.075567		0.762712
10	0.911686	0.191805	0.073952	0.866953	0.881396	0.207119	0.070661		0.779661
11	0.914225	0.190322	0.075267	0.877682	0.882030	0.173442	0.083955		0.762712
12	0.918598	0.185197	0.077113	0.869099	0.878452	0.190210	0.075214		0.745763
13	0.918857	0.185868	0.079196	0.879828	0.878830	0.184991	0.077617		0.728814
14	0.914628	0.187997	0.075024	0.841202	0.879817	0.195004	0.071317		0.771186
15	0.920065	0.186159	0.080116	0.886266	0.871672	0.183317	0.076854		0.720339
16	0.920472	0.182644	0.078020	0.869099	0.877847	0.186996	0.074678		0.737288
17	0.922695	0.181373	0.079465	0.866953	0.876926	0.173912	0.083170		0.720339
18	0.919652	0.184293	0.078891	0.873391	0.875809	0.180174	0.075893		0.720339
19	0.928181	0.174038	0.082384	0.884120	0.875275	0.184312	0.077063		0.720339


1e-3 to 1e-2 (5 epochs) to 1e-4 (5 epochs) + 10 epochs

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.725520	0.358713	0.033584	0.699571	0.841828	0.265102	0.052632		0.796610
1	0.817195	0.265818	0.045118	0.757511	0.861706	0.282545	0.048622		0.822034
2	0.835881	0.254751	0.051122	0.791846	0.854791	0.248122	0.054054		0.796610
3	0.853363	0.241801	0.053556	0.785408	0.864856	0.197187	0.068684		0.711864
4	0.855120	0.238418	0.052418	0.774678	0.872861	0.245961	0.051715		0.855932
5	0.861790	0.234243	0.055581	0.789700	0.878955	0.230391	0.062338		0.813559
6	0.874311	0.225187	0.059728	0.809013	0.859722	0.232175	0.060407		0.779661
7	0.882545	0.218587	0.061010	0.824034	0.881041	0.217258	0.069834		0.822034
8	0.897604	0.205566	0.068663	0.856223	0.885154	0.206709	0.073783		0.796610
9	0.910194	0.196378	0.072711	0.869099	0.885159	0.198489	0.076222		0.779661
10	0.921376	0.184088	0.076671	0.903434	0.885427	0.195895	0.078680		0.788136
11	0.910077	0.194457	0.074759	0.866953	0.884553	0.194842	0.077447		0.771186
12	0.918300	0.186402	0.075024	0.854077	0.884509	0.191045	0.079407		0.771186
13	0.909276	0.196519	0.075167	0.847640	0.884074	0.191411	0.078584		0.771186
14	0.911302	0.193220	0.075465	0.854077	0.883446	0.190861	0.080490		0.779661
15	0.919713	0.185358	0.077010	0.873391	0.883144	0.189002	0.079505		0.762712
16	0.909490	0.193580	0.075095	0.847640	0.883545	0.191293	0.079654		0.779661
17	0.916660	0.188381	0.075051	0.860515	0.882489	0.190231	0.078856		0.771186
18	0.913974	0.190320	0.076894	0.862661	0.882468	0.188302	0.079646		0.762712
19	0.918606	0.186062	0.077547	0.881974	0.883199	0.188882	0.080035		0.771186


---------------------------------------------------------------------------------------------------------------------
trying out other optimizers

1e-3 to 1e-2 to 1e-3 with Nadam

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740903	0.327795	0.034630	0.729614	0.824755	0.287951	0.045410		0.779661
1	0.819902	0.263311	0.044428	0.759657	0.832896	0.273410	0.044455		0.771186
2	0.835227	0.256768	0.051444	0.787554	0.863440	0.267313	0.054484		0.813559
3	0.848138	0.247822	0.054134	0.785408	0.869429	0.245613	0.062384		0.855932
4	0.859326	0.239276	0.056223	0.813305	0.863900	0.197656	0.068935		0.669492
5	0.863321	0.234176	0.056688	0.793991	0.845671	0.208447	0.069172		0.686441
6	0.877479	0.222461	0.060277	0.821888	0.845275	0.338299	0.043146		0.813559
7	0.882628	0.218429	0.059944	0.830472	0.874043	0.194594	0.073269		0.771186
8	0.898071	0.204539	0.065587	0.843348	0.870836	0.197527	0.068598		0.762712
9	0.909499	0.194844	0.071216	0.873391	0.873853	0.199385	0.068759		0.779661


1e-3 to 1e-2 to 1e-3 with AdamW

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.742033	0.324246	0.033630	0.712446	0.860278	0.264708	0.053444		0.822034
1	0.826509	0.260175	0.047817	0.766094	0.852345	0.254041	0.051684		0.754237
2	0.841888	0.249599	0.049201	0.766094	0.837558	0.244821	0.055626		0.745763
3	0.854522	0.239694	0.054596	0.793991	0.848738	0.214869	0.062215		0.694915
4	0.856871	0.240568	0.056758	0.787554	0.856514	0.200931	0.074685		0.652542
5	0.856598	0.239698	0.054971	0.796137	0.878626	0.205114	0.078498		0.779661
6	0.875083	0.224620	0.060334	0.806867	0.862515	0.249326	0.061862		0.822034
7	0.887922	0.215192	0.064764	0.832618	0.875113	0.194648	0.080279		0.779661
8	0.896426	0.207321	0.069163	0.828326	0.881385	0.211199	0.072659		0.822034
9	0.907436	0.199436	0.071112	0.860515	0.881596	0.210204	0.078464		0.796610



1e-3 to 1e-2 to 1e-4 with Nadam

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.719043	0.350834	0.031574	0.718884	0.819153	0.250742	0.048622		0.762712
1	0.802500	0.276963	0.041920	0.740343	0.857719	0.243463	0.057533		0.822034
2	0.843275	0.248901	0.050067	0.798283	0.837940	0.258538	0.048253		0.737288
3	0.843584	0.246416	0.048923	0.809013	0.876616	0.209718	0.067472		0.805085
4	0.857815	0.236360	0.051927	0.806867	0.865021	0.265524	0.049371		0.864407
5	0.854098	0.241356	0.053102	0.815451	0.863864	0.214183	0.068726		0.754237
6	0.879743	0.222567	0.061686	0.843348	0.882520	0.174958	0.086139		0.737288
7	0.882130	0.218909	0.061146	0.824034	0.858698	0.229675	0.065903		0.779661
8	0.899143	0.204342	0.066508	0.841202	0.878495	0.194858	0.075517		0.805085
9	0.907254	0.196594	0.071214	0.864807	0.880113	0.195002	0.073930		0.805085



1e-3 to 1e-2 to 1e-4 with AdamW

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.745340	0.319788	0.035093	0.727468	0.845059	0.245020	0.054184		0.762712
1	0.809303	0.269096	0.044055	0.748927	0.857430	0.246424	0.056724		0.754237
2	0.841333	0.250106	0.050135	0.798283	0.846906	0.243345	0.055407		0.703390
3	0.853738	0.244247	0.056126	0.809013	0.845944	0.323681	0.047314		0.813559
4	0.850340	0.245519	0.055165	0.791846	0.861559	0.218091	0.069468		0.686441
5	0.856141	0.239376	0.054589	0.787554	0.877396	0.214125	0.072934		0.762712
6	0.874031	0.228138	0.059894	0.798283	0.875411	0.188818	0.076856		0.745763
7	0.880653	0.221447	0.061928	0.813305	0.872428	0.214687	0.067548		0.779661
8	0.896582	0.207927	0.066632	0.834764	0.878374	0.200222	0.071371		0.745763
9	0.908744	0.195882	0.071172	0.851931	0.878181	0.194687	0.073272		0.745763

---------------------------------------------------------------------------------------------------------------------


1e-3 to 1e-2 to 1e-3 train and finetune

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.718454	0.337264	0.031579	0.688841	0.839180	0.280609	0.046696		0.796610
1	0.811501	0.270514	0.044037	0.742489	0.848490	0.292658	0.045353		0.864407
2	0.841494	0.250041	0.050745	0.789700	0.864778	0.229142	0.061497		0.779661
3	0.850172	0.242880	0.052707	0.793991	0.869132	0.199704	0.071429		0.703390
4	0.862422	0.237303	0.057772	0.804721	0.849587	0.232009	0.062274		0.728814
5	0.872572	0.226980	0.058897	0.806867	0.862514	0.235907	0.066171		0.754237
6	0.878133	0.224822	0.062765	0.826180	0.881104	0.190530	0.081937		0.745763
7	0.881845	0.219478	0.062370	0.836910	0.888462	0.177880	0.077547		0.728814
8	0.900957	0.202544	0.066281	0.836910	0.888030	0.197761	0.073092		0.771186
9	0.907978	0.194343	0.071378	0.869099	0.890576	0.198280	0.073815		0.805085

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.804912	0.286837	0.049034	0.718884	0.852441	0.225733	0.067753		0.669492
1	0.829956	0.261734	0.051121	0.753219	0.847767	0.269721	0.048864		0.728814
2	0.844371	0.249536	0.051304	0.785408	0.876668	0.204330	0.076165		0.720339
3	0.867285	0.230263	0.055499	0.817597	0.889116	0.212104	0.072112		0.830508
4	0.902101	0.201753	0.064975	0.851931	0.897099	0.204490	0.074534		0.813559


1e-3 to 1e-2 to 1e-3 train and 0.1trainLR finetune

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.739812	0.334685	0.035517	0.716738	0.815062	0.278164	0.046282		0.754237
1	0.806421	0.275871	0.045714	0.740343	0.834495	0.274433	0.046811		0.771186
2	0.827593	0.261835	0.050710	0.751073	0.862166	0.233694	0.064945		0.745763
3	0.852768	0.242430	0.053184	0.783262	0.848317	0.212353	0.065600		0.694915
4	0.851952	0.242735	0.053469	0.778970	0.865780	0.186058	0.083857		0.677966
5	0.874440	0.227383	0.062427	0.804721	0.854708	0.173476	0.076459		0.644068
6	0.874743	0.227736	0.061815	0.798283	0.868951	0.177940	0.081430		0.694915
7	0.897835	0.207146	0.071285	0.834764	0.855166	0.262183	0.058787		0.805085
8	0.901022	0.202659	0.068737	0.845494	0.871435	0.203401	0.071901		0.737288
9	0.902605	0.200888	0.070362	0.839056	0.874118	0.194293	0.073213		0.720339

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.868210	0.236132	0.062103	0.776824	0.858650	0.223026	0.064964		0.754237
1	0.883604	0.219801	0.063981	0.828326	0.838687	0.208346	0.065128		0.669492
2	0.878839	0.223701	0.062661	0.836910	0.871623	0.224764	0.067274		0.813559
3	0.907072	0.198979	0.070500	0.849785	0.883099	0.203687	0.076987		0.788136
4	0.929133	0.177948	0.080877	0.894850	0.891392	0.193952	0.082888		0.788136


1e-3 to 1e-2 to 1e-4 train and finetune

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.730415	0.337912	0.032895	0.725322	0.841085	0.251203	0.051963		0.762712
1	0.815488	0.266187	0.044469	0.791846	0.846123	0.285583	0.046696		0.796610
2	0.832146	0.258544	0.048175	0.787554	0.871138	0.252216	0.054710		0.822034
3	0.849889	0.243508	0.050048	0.789700	0.853910	0.275830	0.048300		0.830508
4	0.846573	0.245537	0.050871	0.796137	0.865472	0.230697	0.061350		0.762712
5	0.863466	0.233855	0.053842	0.793991	0.869487	0.190617	0.072646		0.686441
6	0.871801	0.227632	0.057100	0.815451	0.889517	0.202399	0.066398		0.838983
7	0.884235	0.216939	0.060644	0.828326	0.881832	0.195318	0.072858		0.771186
8	0.902774	0.199660	0.067864	0.843348	0.886052	0.208874	0.069385		0.822034
9	0.911550	0.191672	0.069808	0.858369	0.886198	0.199585	0.074074		0.813559

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.805364	0.290706	0.046945	0.746781	0.825647	0.259418	0.049267		0.711864
1	0.825322	0.260748	0.048391	0.781116	0.852576	0.292701	0.052243		0.838983
2	0.838112	0.253092	0.051060	0.770386	0.860999	0.192215	0.073529		0.677966
3	0.876495	0.225973	0.057509	0.828326	0.891882	0.198935	0.080870		0.788136
4	0.900323	0.205382	0.067503	0.843348	0.900768	0.208581	0.074501		0.822034


1e-3 to 1e-2 to 1e-4 train and 0.1trainLR finetune

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.745499	0.328495	0.036160	0.721030	0.829900	0.279266	0.050830		0.830508
1	0.812901	0.272295	0.046729	0.751073	0.830546	0.293826	0.048212		0.788136
2	0.843402	0.250155	0.053312	0.787554	0.841022	0.245790	0.057355		0.720339
3	0.852624	0.243883	0.054642	0.776824	0.848432	0.223506	0.059406		0.711864
4	0.857926	0.240537	0.056830	0.791846	0.872536	0.206006	0.068342		0.737288
5	0.855250	0.240224	0.053666	0.778970	0.862333	0.253113	0.058967		0.822034
6	0.862625	0.235433	0.056846	0.787554	0.864310	0.209218	0.066667		0.737288
7	0.891040	0.211830	0.068135	0.843348	0.868920	0.224750	0.066139		0.779661
8	0.899616	0.204038	0.067198	0.832618	0.882721	0.190250	0.076278		0.771186
9	0.908107	0.197020	0.074288	0.856223	0.881838	0.193720	0.075063		0.762712

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.869180	0.236038	0.064575	0.763949	0.849604	0.220838	0.064112		0.737288
1	0.864470	0.238529	0.060742	0.783262	0.848990	0.194506	0.071086		0.754237
2	0.879872	0.223733	0.062660	0.839056	0.880689	0.190628	0.077601		0.745763
3	0.907775	0.200601	0.071630	0.871245	0.883625	0.205570	0.072498		0.779661
4	0.933554	0.174882	0.080376	0.918455	0.888026	0.194222	0.078856		0.771186


1e-3 to 1e-2 to 1e-4 train and 0.1trainLR finetune, label_smoothing=0.1 for finetune

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.742095	0.333696	0.035104	0.714592	0.857339	0.267139	0.053485		0.838983
1	0.814171	0.270055	0.046623	0.774678	0.836759	0.245849	0.053204		0.745763
2	0.833977	0.257365	0.049408	0.787554	0.871395	0.233761	0.057927		0.805085
3	0.848219	0.244680	0.051832	0.783262	0.863705	0.238865	0.059773		0.847458
4	0.861957	0.236628	0.055217	0.798283	0.864328	0.217335	0.061737		0.771186
5	0.863927	0.233332	0.056110	0.809013	0.869575	0.199615	0.072526		0.720339
6	0.872447	0.228943	0.059132	0.821888	0.874132	0.193186	0.076923		0.728814
7	0.882777	0.217749	0.059595	0.809013	0.881300	0.238208	0.061200		0.847458
8	0.893765	0.207300	0.063778	0.836910	0.887093	0.208665	0.069187		0.822034
9	0.904916	0.198301	0.067607	0.854077	0.885534	0.192873	0.071542		0.762712

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.855960	0.248621	0.057540	0.746781	0.859320	0.213482	0.062500		0.737288
1	0.873852	0.229225	0.058148	0.836910	0.856973	0.210026	0.059249		0.694915
2	0.880428	0.224871	0.057760	0.809013	0.868521	0.214916	0.065598		0.762712
3	0.898173	0.208392	0.062610	0.841202	0.886824	0.200470	0.074186		0.830508
4	0.939932	0.175075	0.078428	0.929185	0.889572	0.192049	0.080311		0.788136


1e-3 to 1e-2 to 1e-4 train and 0.1trainLR finetune, label_smoothing=0.2 for finetune

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.738165	0.319048	0.033409	0.725322	0.806508	0.306419	0.039224		0.805085
1	0.806699	0.271762	0.041662	0.727468	0.847105	0.260438	0.051368		0.779661
2	0.837418	0.254960	0.048950	0.770386	0.862809	0.216307	0.069767		0.762712
3	0.857571	0.240598	0.054078	0.781116	0.855795	0.236838	0.054118		0.779661
4	0.857981	0.241938	0.055144	0.798283	0.864322	0.218497	0.069157		0.737288
5	0.855397	0.239705	0.052438	0.793991	0.885615	0.188048	0.076923		0.728814
6	0.866548	0.232404	0.055951	0.791846	0.867038	0.199125	0.078329		0.762712
7	0.884552	0.217365	0.060985	0.826180	0.877207	0.246554	0.060024		0.847458
8	0.893482	0.210084	0.065792	0.839056	0.883752	0.201248	0.073896		0.779661
9	0.916669	0.188402	0.074061	0.892704	0.881417	0.197147	0.074713		0.771186

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.872493	0.244198	0.062899	0.783262	0.848185	0.294465	0.047667		0.805085
1	0.865828	0.248307	0.058089	0.798283	0.848199	0.242542	0.060309		0.728814
2	0.872959	0.246142	0.060528	0.802575	0.847727	0.277291	0.051627		0.779661
3	0.908462	0.215821	0.066490	0.862661	0.887929	0.220434	0.073446		0.771186
4	0.929856	0.198431	0.078004	0.905579	0.887530	0.209873	0.077586		0.762712




---------------------------------------------------------------------------------------------------------------------
age=0 when missing

1e-3 to 1e-2 to 1e-4 with age=0 when missing

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740964	0.326256	0.035305	0.746781	0.811396	0.282018	0.040602		0.754237
1	0.815965	0.265774	0.044808	0.757511	0.843544	0.268508	0.051762		0.796610
2	0.831979	0.256521	0.049685	0.761803	0.845033	0.263149	0.053549		0.728814
3	0.840581	0.250975	0.051736	0.783262	0.867543	0.241446	0.061580		0.779661
4	0.862036	0.232462	0.052838	0.811159	0.858444	0.168832	0.077164		0.627119
5	0.849651	0.245513	0.054640	0.768240	0.864908	0.222783	0.068285		0.762712
6	0.871761	0.227828	0.059046	0.802575	0.865649	0.286823	0.055104		0.805085
7	0.884065	0.216541	0.060446	0.809013	0.883234	0.196429	0.074513		0.745763
8	0.899660	0.204178	0.069368	0.841202	0.882627	0.192128	0.077911		0.771186
9	0.911583	0.193519	0.075539	0.864807	0.880973	0.186329	0.077408		0.728814


1e-3 to 1e-2 to 1e-3 with age=0 when missing

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.736504	0.343029	0.034031	0.718884	0.826223	0.252656	0.048683		0.720339
1	0.797988	0.279673	0.042316	0.733906	0.856698	0.249813	0.057214		0.779661
2	0.844059	0.249345	0.051546	0.804721	0.838810	0.205263	0.063281		0.686441
3	0.834562	0.255553	0.048633	0.774678	0.866685	0.228253	0.062246		0.779661
4	0.857307	0.240720	0.054000	0.806867	0.870858	0.241531	0.062083		0.788136
5	0.863449	0.234503	0.056372	0.798283	0.865977	0.231739	0.061704		0.779661
6	0.878349	0.225050	0.063053	0.826180	0.879097	0.220496	0.067608		0.771186
7	0.883544	0.218872	0.061389	0.815451	0.883520	0.209622	0.067529		0.796610
8	0.894216	0.208155	0.065963	0.845494	0.875141	0.196882	0.070644		0.762712
9	0.901240	0.203207	0.067938	0.841202	0.875800	0.199352	0.071828		0.762712


age=0 when missing seems to give comparable results to age=-10 when missing.
We'll keep age=-10 when missing since its different from existing values and easier to visualize.
---------------------------------------------------------------------------------------------------------------------


1e-3 to 1e-2 to 1e-4 train and finetune from L287

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.748443	0.320503	0.034577	0.763949	0.844963	0.254211	0.050463		0.830508
1	0.807744	0.274963	0.043400	0.774678	0.863454	0.224787	0.060056		0.728814
2	0.831407	0.257672	0.047877	0.778970	0.851789	0.209657	0.059286		0.703390
3	0.845252	0.246132	0.048057	0.796137	0.873208	0.207941	0.063814		0.720339
4	0.855642	0.238647	0.052428	0.785408	0.865987	0.245535	0.057486		0.771186
5	0.865196	0.233115	0.055418	0.815451	0.860939	0.285763	0.053723		0.855932
6	0.872806	0.230322	0.060481	0.815451	0.874009	0.250093	0.057298		0.805085
7	0.878641	0.223622	0.061502	0.834764	0.882306	0.223029	0.067391		0.788136
8	0.905941	0.199117	0.070336	0.858369	0.887317	0.177958	0.081671		0.728814
9	0.908721	0.195662	0.073596	0.843348	0.887808	0.190684	0.075214		0.745763

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.794508	0.295062	0.047783	0.714592	0.857485	0.218449	0.062500		0.728814
1	0.828577	0.259383	0.047167	0.778970	0.885783	0.276716	0.059675		0.838983
2	0.866599	0.232413	0.055360	0.811159	0.885422	0.253627	0.061327		0.830508
3	0.901753	0.201065	0.066938	0.845494	0.896531	0.182229	0.081953		0.796610
4	0.936179	0.166997	0.087145	0.907725	0.909204	0.179758	0.088342		0.822034


1e-3 to 1e-2 to 1e-4 train and finetune 0.1LR L287

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.718783	0.345454	0.030897	0.710300	0.825611	0.266222	0.047286		0.745763
1	0.820201	0.263550	0.044784	0.774678	0.849011	0.262098	0.048640		0.788136
2	0.833257	0.256762	0.047693	0.787554	0.855344	0.208722	0.062300		0.661017
3	0.846915	0.246396	0.050705	0.787554	0.867257	0.180903	0.083415		0.720339
4	0.857821	0.240078	0.053699	0.806867	0.858226	0.206310	0.072330		0.728814
5	0.860658	0.236730	0.054701	0.809013	0.875197	0.207784	0.072190		0.745763
6	0.868369	0.229326	0.056277	0.809013	0.857934	0.241490	0.060241		0.805085
7	0.884170	0.217603	0.060492	0.828326	0.881040	0.207771	0.069242		0.805085
8	0.896240	0.209172	0.069074	0.849785	0.880860	0.206452	0.070242		0.788136
9	0.914131	0.191087	0.073461	0.875537	0.881923	0.190147	0.075567		0.762712

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.839292	0.265845	0.051414	0.753219	0.845379	0.298211	0.043568		0.889831
1	0.851496	0.247672	0.051937	0.811159	0.865527	0.236529	0.053422		0.813559
2	0.874943	0.223973	0.052660	0.847640	0.865567	0.234665	0.056404		0.813559
3	0.924614	0.183455	0.074377	0.896996	0.892607	0.182378	0.079963		0.737288
4	0.953596	0.148815	0.098861	0.931330	0.902789	0.183625	0.087977		0.762712


1e-3 to 1e-2 to 1e-4 train and finetune full

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.739894	0.314360	0.033202	0.742489	0.820594	0.256801	0.043499		0.762712
1	0.809332	0.270157	0.042831	0.759657	0.854829	0.239060	0.056796		0.754237
2	0.837906	0.252396	0.048334	0.781116	0.857351	0.229262	0.061048		0.779661
3	0.845475	0.250066	0.053825	0.789700	0.857566	0.255967	0.059607		0.796610
4	0.855336	0.239343	0.053406	0.802575	0.874070	0.209418	0.062711		0.788136
5	0.847449	0.244790	0.051110	0.785408	0.857629	0.315715	0.044731		0.881356
6	0.864587	0.233752	0.054861	0.800429	0.872540	0.233756	0.061431		0.822034
7	0.880950	0.220393	0.060549	0.832618	0.871982	0.205277	0.068576		0.771186
8	0.900051	0.202895	0.068555	0.841202	0.878237	0.190792	0.075314		0.762712
9	0.904199	0.198149	0.068237	0.845494	0.878377	0.191097	0.074074		0.762712

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.772311	0.305892	0.039874	0.703863	0.790052	0.317542	0.038023		0.762712
1	0.780146	0.288063	0.036827	0.768240	0.835889	0.166915	0.091062		0.457627
2	0.787775	0.278158	0.038798	0.761803	0.822584	0.481434	0.062715		0.618644
3	0.821982	0.259026	0.042349	0.832618	0.853821	0.233378	0.051196		0.779661
4	0.835640	0.248764	0.045868	0.821888	0.849644	0.302463	0.045475		0.838983



1e-3 to 1e-2 to 1e-4 train and finetune 0.1LR full

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.733785	0.338203	0.033787	0.693133	0.794743	0.294229	0.041379		0.711864
1	0.809509	0.274224	0.045253	0.723176	0.852511	0.256651	0.057803		0.762712
2	0.841796	0.250850	0.050869	0.772532	0.829852	0.239935	0.051939		0.635593
3	0.858272	0.238146	0.054968	0.802575	0.852084	0.235217	0.062907		0.737288
4	0.854499	0.241383	0.053666	0.785408	0.859470	0.216430	0.064715		0.711864
5	0.867306	0.233780	0.059614	0.809013	0.869349	0.198698	0.075686		0.677966
6	0.881273	0.221793	0.062993	0.821888	0.857053	0.204251	0.068163		0.694915
7	0.892821	0.209694	0.065078	0.826180	0.864548	0.193075	0.071307		0.711864
8	0.902083	0.201107	0.066794	0.826180	0.876686	0.195870	0.067353		0.720339
9	0.913790	0.189861	0.073676	0.854077	0.875041	0.193761	0.069421		0.711864

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.801707	0.303207	0.045493	0.699571	0.846123	0.234237	0.055000		0.745763
1	0.835319	0.257986	0.047373	0.787554	0.820932	0.258792	0.050535		0.720339
2	0.832120	0.258584	0.048119	0.787554	0.827885	0.234859	0.051609		0.720339
3	0.864787	0.231089	0.051447	0.843348	0.879039	0.224539	0.062704		0.813559
4	0.903353	0.202797	0.059759	0.894850	0.896639	0.223916	0.064249		0.855932



1e-3 to 1e-2 to 1e-4 train & finetune 0.01LR full

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740048	0.328520	0.033433	0.723176	0.840180	0.286881	0.045760		0.864407
1	0.812138	0.269711	0.044536	0.766094	0.847234	0.297485	0.045065		0.847458
2	0.839186	0.248845	0.048215	0.793991	0.863489	0.206798	0.066717		0.754237
3	0.837409	0.255331	0.050352	0.783262	0.871274	0.249754	0.059364		0.822034
4	0.853750	0.243150	0.054821	0.800429	0.865451	0.274428	0.053496		0.855932
5	0.857502	0.239211	0.054097	0.800429	0.863732	0.178907	0.086560		0.644068
6	0.872836	0.226432	0.058069	0.806867	0.880919	0.208757	0.074313		0.779661
7	0.887524	0.215888	0.066964	0.836910	0.874572	0.204979	0.072555		0.779661
8	0.905004	0.197828	0.068846	0.854077	0.884347	0.198848	0.071931		0.779661
9	0.905466	0.199504	0.068845	0.845494	0.883832	0.190384	0.076475		0.779661

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.730181	0.405675	0.043464	0.557940	0.810104	0.250142	0.051866		0.694915
1	0.833914	0.265242	0.052679	0.746781	0.854097	0.254036	0.056683		0.805085
2	0.891315	0.210232	0.058643	0.860515	0.863016	0.221421	0.065852		0.822034
3	0.920199	0.188823	0.064395	0.920601	0.865101	0.224591	0.064043		0.813559
4	0.921354	0.185194	0.067883	0.909871	0.864271	0.227729	0.063428		0.796610


1e-4 to 1e-3 to 1e-5 train full model

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.691417	0.375337	0.026559	0.839056	0.784354	0.399513	0.029551		0.847458
1	0.810541	0.274754	0.035536	0.901288	0.837402	0.303819	0.037961		0.889831
2	0.848570	0.246153	0.044599	0.869099	0.856456	0.286496	0.047641		0.864407
3	0.850406	0.242918	0.046484	0.832618	0.825147	0.270553	0.043436		0.762712
4	0.842589	0.249679	0.049876	0.778970	0.790659	0.260236	0.048980		0.508475
5	0.817968	0.266787	0.044265	0.787554	0.852625	0.262708	0.054069		0.822034
6	0.841748	0.248568	0.046708	0.854077	0.868196	0.244140	0.049164		0.847458
7	0.879339	0.220789	0.052026	0.892704	0.874488	0.199858	0.065248		0.779661
8	0.902157	0.202840	0.059860	0.881974	0.877229	0.208223	0.064057		0.762712
9	0.919118	0.187534	0.064649	0.912017	0.882200	0.210509	0.062458		0.796610


1e-3 to 1e-2 to 1e-4 train full model

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.677873	0.345282	0.029222	0.766094	0.797638	0.355359	0.034138		0.898305
1	0.758583	0.296823	0.035700	0.746781	0.738865	0.333772	0.029758		0.771186
2	0.758709	0.296276	0.034986	0.740343	0.848127	0.295235	0.035026		0.923729
3	0.773053	0.286712	0.034427	0.774678	0.822238	0.259894	0.053395		0.686441
4	0.765210	0.299093	0.036573	0.751073	0.825434	0.260187	0.054274		0.677966
5	0.785330	0.283731	0.039418	0.738197	0.834068	1.201722	0.039066		0.822034
6	0.809022	0.267513	0.040207	0.785408	0.850398	0.558433	0.040361		0.872881
7	0.816290	0.263293	0.041857	0.804721	0.837940	0.241441	0.049135		0.745763
8	0.829959	0.253733	0.044622	0.804721	0.857589	0.320464	0.044797		0.813559
9	0.840297	0.247429	0.046525	0.839056	0.852458	0.259133	0.044580		0.805085



1e-3 to 1e-2 to 1e-4 train from L287-------------------------------------------15m 47s

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.735566	0.336837	0.035651	0.731760	0.847990	0.270732	0.051048		0.805085
1	0.826661	0.261202	0.047148	0.798283	0.866667	0.254198	0.051697		0.838983
2	0.844602	0.250923	0.053494	0.791846	0.878454	0.181828	0.067003		0.788136
3	0.856897	0.247854	0.056646	0.811159	0.860607	0.192731	0.069299		0.728814
4	0.879824	0.220303	0.055208	0.851931	0.881800	0.220910	0.059834		0.855932
5	0.888143	0.215267	0.060740	0.856223	0.869470	0.219312	0.057681		0.805085
6	0.898379	0.203825	0.063625	0.879828	0.875462	0.123165	0.111801		0.610169
7	0.926619	0.172188	0.078282	0.903434	0.892652	0.160784	0.081877		0.754237
8	0.944533	0.151207	0.089641	0.909871	0.892246	0.149670	0.099109		0.754237
9	0.954488	0.138643	0.105819	0.924893	0.898956	0.149488	0.097826		0.762712



1e-3 to 1e-2 to 1e-4 train from L287 with LS0.1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740364	0.315464	0.032878	0.800429	0.769971	0.235298	0.053772		0.567797
1	0.828411	0.262655	0.045734	0.813305	0.847348	0.221789	0.055419		0.762712
2	0.856306	0.244375	0.053759	0.813305	0.866774	0.266271	0.059701		0.813559
3	0.875047	0.230827	0.057315	0.839056	0.883615	0.226268	0.058291		0.872881
4	0.895061	0.212466	0.064235	0.869099	0.842634	0.177326	0.058039		0.737288
5	0.893952	0.212373	0.061840	0.879828	0.886708	0.140292	0.097651		0.669492
6	0.883346	0.221869	0.060122	0.847640	0.887131	0.280485	0.050306		0.906780
7	0.925521	0.182558	0.079204	0.905579	0.894723	0.154333	0.101466		0.762712
8	0.950936	0.151027	0.105599	0.922747	0.890166	0.153115	0.102771		0.754237
9	0.965637	0.127806	0.126970	0.933476	0.891704	0.153228	0.111663		0.762712


1e-3 to 1e-2 to 1e-4 train from L287 with LS0.2

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.733618	0.331816	0.033179	0.736051	0.845574	0.296985	0.047091		0.864407
1	0.814415	0.276160	0.043702	0.763949	0.844212	0.266248	0.054100		0.805085
2	0.833733	0.265841	0.048938	0.781116	0.853729	0.243177	0.058056		0.754237
3	0.852710	0.253460	0.052170	0.791846	0.851386	0.212968	0.072595		0.677966
4	0.849938	0.255711	0.052930	0.800429	0.863673	0.214354	0.072212		0.669492
5	0.858383	0.248485	0.052938	0.800429	0.871216	0.216924	0.071485		0.762712
6	0.870081	0.244337	0.059877	0.817597	0.878354	0.228062	0.070136		0.788136
7	0.884337	0.233545	0.064032	0.824034	0.883983	0.197887	0.084746		0.762712
8	0.904312	0.218472	0.071003	0.843348	0.884543	0.223719	0.074367		0.796610
9	0.914128	0.209234	0.075806	0.873391	0.887463	0.213742	0.075331		0.771186


---------------best-----------------------------------------------------------------------------
1e-3 to 1e-2 to 1e-3 train from L287

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.743814	0.312814	0.033256	0.766094	0.807275	0.230231	0.057912		0.601695
1	0.840404	0.248911	0.049712	0.815451	0.836363	0.298436	0.045176		0.813559
2	0.850338	0.246135	0.050459	0.789700	0.882379	0.207414	0.062016		0.813559
3	0.875496	0.223017	0.056674	0.845494	0.878740	0.248063	0.066423		0.771186
4	0.881220	0.228249	0.061521	0.834764	0.877370	0.319934	0.033463		0.949153
5	0.880714	0.221626	0.062197	0.854077	0.888946	0.171618	0.079597		0.737288
6	0.916245	0.186486	0.073431	0.871245	0.888444	0.172978	0.076724		0.754237
7	0.932088	0.168861	0.087453	0.903434	0.899933	0.180871	0.076733		0.788136
8	0.952218	0.138786	0.102041	0.933476	0.900829	0.160035	0.097236		0.805085
9	0.966133	0.119628	0.127089	0.946352	0.902654	0.150843	0.102247		0.771186
-------------------------------------------------------------------------------------------------


1e-3 to 1e-2 to 1e-3 train from L287 with LS0.1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.726467	0.348954	0.033900	0.723176	0.797385	0.755736	0.034722		0.932203
1	0.806112	0.277035	0.043376	0.751073	0.853132	0.257921	0.056638		0.788136
2	0.846651	0.252730	0.050520	0.813305	0.854101	0.279407	0.047437		0.838983
3	0.859800	0.243919	0.057191	0.796137	0.878958	0.270862	0.056786		0.847458
4	0.870558	0.233976	0.057461	0.819743	0.873270	0.353580	0.055747		0.822034
5	0.878648	0.226286	0.058477	0.830472	0.876694	0.157187	0.093051		0.669492
6	0.912585	0.197559	0.072980	0.879828	0.882136	0.177267	0.073920		0.754237
7	0.915493	0.193559	0.078065	0.879828	0.895376	0.139662	0.113260		0.694915
8	0.946742	0.155523	0.103639	0.922747	0.873864	0.152520	0.095181		0.669492
9	0.962627	0.132478	0.123357	0.946352	0.891131	0.150107	0.112033		0.686441


1e-3 to 1e-2 to 1e-3 train from L287 with LS0.2

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740510	0.321465	0.033808	0.763949	0.784524	0.369006	0.049229		0.703390
1	0.808010	0.282482	0.043579	0.766094	0.866563	0.261090	0.062918		0.796610
2	0.826625	0.272935	0.049225	0.783262	0.872396	0.253645	0.057661		0.889831
3	0.849854	0.259135	0.051063	0.819743	0.888991	0.236457	0.069820		0.788136
4	0.879995	0.236762	0.060691	0.841202	0.882860	0.228335	0.065619		0.813559
5	0.865806	0.248181	0.054670	0.824034	0.877026	0.223334	0.069804		0.754237
6	0.903845	0.216614	0.070104	0.851931	0.877420	0.233406	0.062301		0.830508
7	0.914971	0.205865	0.071615	0.881974	0.884870	0.215828	0.066396		0.830508
8	0.939221	0.181632	0.088199	0.914163	0.904132	0.175651	0.087536		0.779661
9	0.953184	0.164612	0.104919	0.929185	0.895459	0.194461	0.080799		0.788136


1e-3 to 1e-2 to 1e-3 train from L287 with 20 epoch - 5 epoch warmup, 5 epoch decay	
1e-3 to 1e-2 to 1e-3 train from L287 20ep (5W 5D)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.755949	0.308439	0.035585	0.761803	0.860272	0.305775	0.044244		0.830508
1	0.809167	0.270613	0.043063	0.763949	0.882891	0.269391	0.045668		0.906780
2	0.824497	0.260509	0.044555	0.787554	0.822330	0.202081	0.079818		0.593220
3	0.844720	0.245149	0.049468	0.817597	0.887350	0.163182	0.076068		0.754237
4	0.869417	0.228485	0.054518	0.845494	0.896386	0.229833	0.065445		0.847458
5	0.885519	0.214204	0.058261	0.875537	0.898394	0.123859	0.103399		0.618644
6	0.907116	0.194011	0.067730	0.892704	0.895794	0.169557	0.074544		0.796610
7	0.923828	0.185065	0.082189	0.886266	0.906956	0.185441	0.084071		0.805085
8	0.927331	0.170048	0.080891	0.912017	0.899814	0.193931	0.066937		0.838983
9	0.940402	0.159109	0.091298	0.916309	0.910334	0.151294	0.095528		0.796610
10	0.953620	0.140531	0.105962	0.942060	0.916006	0.142147	0.113350		0.762712
11	0.958817	0.131024	0.123941	0.942060	0.912757	0.152213	0.110840		0.771186
12	0.965034	0.121483	0.130487	0.937768	0.912130	0.156549	0.101604		0.805085
13	0.964436	0.121957	0.126659	0.942060	0.909946	0.155852	0.093814		0.771186
14	0.971566	0.105543	0.139079	0.959227	0.910339	0.114831	0.134400		0.711864
15	0.971574	0.109562	0.150956	0.948498	0.903838	0.126372	0.118881		0.720339
16	0.973067	0.104589	0.164641	0.959227	0.900173	0.115490	0.126853		0.652542
17	0.975347	0.101526	0.160422	0.946352	0.902471	0.095972	0.145038		0.644068
18	0.976984	0.098858	0.175088	0.959227	0.900227	0.134356	0.112299		0.711864
19	0.979771	0.090348	0.168539	0.965665	0.904066	0.115231	0.124019		0.669492


1e-3 to 1e-2 to 1e-3 train from L287 with 20 epoch - 10 epoch warmup, 10 epoch decay
1e-3 to 1e-2 to 1e-3 train from L287 20ep(10W10D)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.734801	0.335056	0.033970	0.710300	0.857148	0.270576	0.055682		0.830508
1	0.831132	0.257517	0.047669	0.781116	0.861451	0.208420	0.055051		0.771186
2	0.828876	0.268301	0.052217	0.768240	0.848216	0.293563	0.048072		0.813559
3	0.843658	0.250553	0.049479	0.804721	0.855564	0.217311	0.067390		0.737288
4	0.866586	0.231723	0.056311	0.826180	0.886789	0.223669	0.057803		0.847458
5	0.876198	0.224324	0.059508	0.845494	0.872335	0.264068	0.050025		0.864407
6	0.888292	0.213864	0.062372	0.849785	0.895573	0.184872	0.077295		0.813559
7	0.899648	0.203617	0.065357	0.849785	0.881107	0.222621	0.066618		0.771186
8	0.899505	0.208277	0.067864	0.843348	0.866536	0.157066	0.096324		0.644068
9	0.894163	0.210874	0.064324	0.856223	0.879909	0.193015	0.067321		0.813559
10	0.911622	0.190914	0.069197	0.884120	0.887773	0.224131	0.064473		0.813559
11	0.914366	0.191744	0.076624	0.888412	0.889865	0.186623	0.078097		0.737288
12	0.936871	0.163858	0.091520	0.905579	0.890644	0.132391	0.104946		0.737288
13	0.946177	0.149202	0.091600	0.905579	0.889043	0.131891	0.104575		0.677966
14	0.952620	0.139768	0.103918	0.933476	0.887653	0.143720	0.092841		0.703390
15	0.948930	0.150048	0.105393	0.914163	0.891059	0.137995	0.104922		0.686441
16	0.970472	0.112975	0.144177	0.937768	0.894791	0.114335	0.126168		0.686441
17	0.974846	0.103898	0.156648	0.950644	0.893521	0.135152	0.117560		0.669492
18	0.978131	0.096002	0.168118	0.952790	0.892422	0.116833	0.117834		0.627119
19	0.982332	0.086108	0.178304	0.969957	0.887709	0.118020	0.123457		0.593220


1e-3 to 1e-2 to 1e-3 train from L287 with 20 epoch - 5 epoch warmup, 15 epoch decay, finetune with full at 0.1LR of train
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.730537	0.323065	0.031432	0.751073	0.811395	0.234587	0.048808		0.728814
1	0.789600	0.289790	0.040630	0.781116	0.757167	0.232212	0.049324		0.618644
2	0.829375	0.259966	0.046173	0.796137	0.875740	0.226592	0.066860		0.779661
3	0.849422	0.243109	0.047560	0.828326	0.882965	0.247292	0.050693		0.898305
4	0.860530	0.236546	0.051964	0.826180	0.891005	0.185875	0.069435		0.822034
5	0.882045	0.217441	0.057340	0.839056	0.886561	0.200254	0.073409		0.762712
6	0.897226	0.203731	0.064017	0.860515	0.890751	0.188431	0.074736		0.779661
7	0.912890	0.188771	0.071516	0.879828	0.863957	0.198738	0.065401		0.788136
8	0.907097	0.193594	0.065141	0.873391	0.892975	0.215323	0.060750		0.864407
9	0.930750	0.170933	0.085619	0.884120	0.875448	0.190360	0.065186		0.771186
10	0.935047	0.164967	0.087705	0.907725	0.899024	0.170774	0.088645		0.754237
11	0.946708	0.150642	0.103565	0.916309	0.898981	0.138357	0.115922		0.703390
12	0.957424	0.133616	0.118817	0.922747	0.891427	0.161994	0.088547		0.779661
13	0.965283	0.122066	0.131949	0.927039	0.893057	0.113219	0.122835		0.661017
14	0.970296	0.111180	0.141142	0.933476	0.891201	0.093367	0.140206		0.576271
15	0.979376	0.092914	0.165182	0.954936	0.898011	0.135580	0.113128		0.686441
16	0.979114	0.095117	0.178202	0.961373	0.895498	0.113634	0.125673		0.593220
17	0.979905	0.093585	0.169739	0.948498	0.897831	0.108571	0.137218		0.618644
18	0.985265	0.079747	0.204016	0.959227	0.899124	0.119389	0.129032		0.644068
19	0.988762	0.069514	0.227638	0.972103	0.895567	0.108780	0.141684		0.584746

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.894091	0.320571	0.100134	0.643777	0.889237	0.132555	0.093509		0.720339
1	0.877915	0.262128	0.068966	0.781116	0.857150	0.107574	0.120735		0.389830
2	0.855325	0.240790	0.052515	0.815451	0.888931	0.207713	0.061608		0.805085
3	0.923353	0.179670	0.071623	0.903434	0.895788	0.154819	0.090909		0.737288
4	0.956239	0.142532	0.094928	0.939914	0.887950	0.143086	0.096894		0.661017



1e-3 to 1e-2 to 1e-4 train from L287 with 20 epoch - 5 epoch warmup, 15 epoch decay, finetune with full at 0.1LR of train
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.729598	0.325999	0.031719	0.761803	0.808930	0.407185	0.044444		0.830508
1	0.812385	0.273037	0.044969	0.740343	0.862749	0.283678	0.053664		0.788136
2	0.824902	0.262794	0.049907	0.748927	0.866434	0.265009	0.054696		0.898305
3	0.861943	0.237035	0.056392	0.819743	0.816280	0.498364	0.043228		0.889831
4	0.862569	0.235489	0.054843	0.834764	0.879365	0.286428	0.070073		0.813559
5	0.867949	0.231886	0.056917	0.817597	0.851993	0.271242	0.052721		0.788136
6	0.888181	0.212113	0.061361	0.866953	0.844547	0.176111	0.060890		0.661017
7	0.896203	0.208324	0.068829	0.856223	0.879225	0.204652	0.069909		0.779661
8	0.911944	0.188550	0.070279	0.871245	0.853924	0.207690	0.068635		0.728814
9	0.923657	0.175048	0.076441	0.890558	0.813722	0.130645	0.066102		0.330508
10	0.925506	0.176266	0.082041	0.896996	0.859847	0.180288	0.075278		0.745763
11	0.945207	0.150270	0.093348	0.918455	0.867664	0.136376	0.098750		0.669492
12	0.947576	0.148193	0.096076	0.914163	0.872546	0.124845	0.107285		0.686441
13	0.962359	0.122505	0.122958	0.952790	0.893071	0.129559	0.098131		0.711864
14	0.966521	0.119453	0.128693	0.944206	0.884608	0.129609	0.105195		0.686441
15	0.977092	0.098158	0.152877	0.963519	0.875403	0.113489	0.120521		0.627119
16	0.980351	0.090456	0.172883	0.954936	0.886228	0.122044	0.115556		0.661017
17	0.979904	0.092567	0.170937	0.959227	0.885055	0.105001	0.128250		0.627119
18	0.981633	0.088721	0.175246	0.957082	0.885880	0.111508	0.131667		0.669492
19	0.985851	0.078390	0.180328	0.967811	0.883051	0.107704	0.132246		0.618644

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.893177	0.305971	0.100955	0.680257	0.873018	0.135977	0.088435		0.661017
1	0.908245	0.205181	0.078399	0.824034	0.822266	0.182463	0.057949		0.661017
2	0.885673	0.226024	0.063833	0.828326	0.879559	0.231945	0.057790		0.864407
3	0.944309	0.154444	0.084400	0.933476	0.898626	0.175886	0.085960		0.762712
4	0.969721	0.120785	0.113241	0.950644	0.892497	0.151047	0.093787		0.677966


1e-3 to 1e-2 to 1e-3 train from L287 with 10 epoch, finetune with full model 1e-4 to 1e-3 to 1e-4
1e-3_1e-2_1e-3 train from L287 10E, finetune 0.1LR

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.737494	0.331926	0.035355	0.723176	0.784720	0.424848	0.047671		0.737288
1	0.812840	0.271972	0.044679	0.791846	0.824778	0.214061	0.062598		0.677966
2	0.841681	0.251816	0.050466	0.789700	0.861595	0.202123	0.075646		0.694915
3	0.839054	0.257387	0.051546	0.793991	0.809535	0.221978	0.050388		0.771186
4	0.853064	0.246766	0.054340	0.804721	0.857187	0.240207	0.053523		0.830508
5	0.872084	0.231155	0.057354	0.866953	0.879698	0.248555	0.056027		0.838983
6	0.894034	0.207503	0.060415	0.843348	0.868991	0.194069	0.085106		0.745763
7	0.914662	0.187505	0.073179	0.884120	0.870296	0.167680	0.076480		0.788136
8	0.935569	0.162707	0.084131	0.905579	0.886426	0.154206	0.089268		0.754237
9	0.947940	0.149556	0.098825	0.920601	0.896300	0.151993	0.103865		0.728814

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.880901	0.245700	0.075728	0.753219	0.787773	0.201622	0.053371		0.644068
1	0.865291	0.245669	0.060379	0.793991	0.864072	0.185421	0.063187		0.779661
2	0.839009	0.249997	0.046747	0.811159	0.870062	0.210488	0.055770		0.855932
3	0.867349	0.225828	0.049047	0.877682	0.886770	0.245303	0.049812		0.898305
4	0.893168	0.205179	0.054099	0.907725	0.888286	0.225497	0.060552		0.855932


1e-3 to 1e-2 to 1e-4 train from L287 with 10 epoch, finetune with full model 1e-4 to 1e-3 to 1e-5
1e-3_1e-2_1e-4 train from L287 10E, finetune 0.1LR

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.751535	0.320048	0.035332	0.729614	0.691542	0.648981	0.033699		0.754237
1	0.842122	0.248067	0.048264	0.811159	0.819613	0.328051	0.041704		0.779661
2	0.854329	0.240431	0.051651	0.815451	0.866244	0.164864	0.084287		0.686441
3	0.856920	0.239293	0.053393	0.824034	0.882941	0.245052	0.064327		0.838983
4	0.887719	0.215242	0.062975	0.854077	0.880437	0.171186	0.072816		0.762712
5	0.893626	0.208858	0.063537	0.862661	0.884175	0.203629	0.072347		0.762712
6	0.907959	0.197785	0.074353	0.869099	0.899109	0.167782	0.085377		0.796610
7	0.935691	0.164983	0.088507	0.905579	0.895356	0.177763	0.082465		0.805085
8	0.958872	0.133183	0.110246	0.942060	0.900808	0.152037	0.094668		0.737288
9	0.969033	0.114012	0.127383	0.946352	0.911145	0.147143	0.107399		0.762712

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.877674	0.252662	0.073020	0.759657	0.858155	0.157568	0.081165		0.661017
1	0.904877	0.198838	0.072070	0.877682	0.825490	0.160261	0.066800		0.567797
2	0.869804	0.231437	0.057049	0.824034	0.881237	0.218014	0.056845		0.830508
3	0.920804	0.182125	0.070014	0.879828	0.898035	0.184521	0.083730		0.745763
4	0.959444	0.140562	0.099908	0.935622	0.904356	0.168804	0.092324		0.754237


------------------------------------------------best--------------------------------------------------------

1e-3 to 1e-2 to 1e-4 train from L287 with 10 epoch, finetune with L66 with 1e-4 to 1e-3 to 1e-5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.739364	0.325165	0.035199	0.723176	0.839993	0.310122	0.049596		0.779661
1	0.814731	0.269792	0.044705	0.768240	0.803840	0.243165	0.048618		0.805085
2	0.828077	0.258947	0.046776	0.804721	0.873619	0.247589	0.047752		0.872881
3	0.844181	0.251530	0.051932	0.787554	0.888869	0.299582	0.053668		0.923729
4	0.885165	0.217105	0.062321	0.841202	0.882497	0.195677	0.075421		0.720339
5	0.889662	0.216268	0.063213	0.856223	0.877085	0.184769	0.077132		0.720339
6	0.904283	0.199754	0.069377	0.860515	0.886727	0.146325	0.091013		0.677966
7	0.926851	0.174764	0.082038	0.894850	0.902844	0.181839	0.079038		0.779661
8	0.949366	0.147589	0.099672	0.914163	0.906462	0.139802	0.104167		0.720339
9	0.968168	0.118449	0.131618	0.944206	0.904937	0.147554	0.107053		0.720339

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.894538	0.231390	0.085372	0.763949	0.877392	0.181795	0.065280		0.771186
1	0.921218	0.181502	0.078155	0.879828	0.892342	0.135416	0.114493		0.669492
2	0.888609	0.218733	0.069073	0.804721	0.882141	0.308441	0.056977		0.830508
3	0.949474	0.151680	0.097275	0.927039	0.879192	0.137838	0.118980		0.711864
4	0.977532	0.109725	0.131357	0.969957	0.915032	0.131077	0.133433		0.754237
------------------------------------------------------------------------------------------------------------


1e-3 to 1e-2 to 1e-4 train from L287 with 10 epoch, finetune with L154 with 1e-4 to 1e-3 to 1e-5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.743805	0.329260	0.034894	0.733906	0.840568	0.291819	0.059607		0.720339
1	0.824830	0.269452	0.049702	0.768240	0.849983	1.811106	0.061238		0.796610
2	0.834502	0.257390	0.048173	0.772532	0.864165	0.212038	0.068114		0.771186
3	0.840066	0.261458	0.048644	0.800429	0.880668	0.243974	0.054126		0.855932
4	0.861851	0.239207	0.053581	0.815451	0.882677	0.221790	0.054889		0.813559
5	0.883650	0.220344	0.060797	0.847640	0.875372	0.169561	0.088174		0.720339
6	0.899584	0.200179	0.064883	0.869099	0.892010	0.146166	0.089583		0.728814
7	0.926300	0.172510	0.077599	0.890558	0.886445	0.163434	0.078415		0.788136
8	0.952502	0.140089	0.104935	0.939914	0.889482	0.146501	0.099299		0.720339
9	0.965743	0.120071	0.123703	0.946352	0.892722	0.148326	0.101449		0.711864

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.925244	0.186283	0.091959	0.854077	0.825963	0.156568	0.064035		0.618644
1	0.915532	0.188931	0.071960	0.871245	0.876402	0.136100	0.102921		0.627119
2	0.936531	0.162073	0.086207	0.912017	0.875072	0.127895	0.101626		0.635593
3	0.967557	0.117226	0.122040	0.939914	0.882759	0.144476	0.095006		0.661017
4	0.983804	0.087897	0.173262	0.967811	0.871850	0.121147	0.116013		0.601695



1e-3 to 1e-2 to 1e-4 train from L66 with 10 epoch

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.737427	0.320168	0.033838	0.776824	0.548078	0.292705	0.023788		0.220339
1	0.791721	0.276260	0.039515	0.789700	0.874904	0.240717	0.057692		0.864407
2	0.808511	0.271490	0.041789	0.796137	0.864605	0.252228	0.069498		0.762712
3	0.832592	0.261945	0.046684	0.811159	0.866442	0.252088	0.058714		0.805085
4	0.842247	0.245219	0.046584	0.832618	0.790061	3.171585	0.041258		0.855932
5	0.814177	0.266464	0.042505	0.811159	0.860725	0.294283	0.042728		0.881356
6	0.858690	0.237783	0.050536	0.819743	0.882533	0.207681	0.065407		0.762712
7	0.867990	0.225621	0.052958	0.860515	0.893166	0.205085	0.059333		0.813559
8	0.890663	0.206081	0.056503	0.881974	0.893854	0.190606	0.066955		0.788136
9	0.915282	0.185336	0.067749	0.922747	0.891923	0.180557	0.072803		0.737288


smaller architecture (d16->d8) 1e-3 to 1e-2 to 1e-4 train from L287 with 10 epoch, finetune with L66 with 1e-4 to 1e-3 to 1e-5  

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.754469	0.303373	0.035298	0.727468	0.795827	0.446328	0.055280		0.661017
1	0.816132	0.268286	0.047415	0.759657	0.878478	0.288580	0.052632		0.889831
2	0.836836	0.252576	0.048182	0.778970	0.826275	0.353294	0.036631		0.822034
3	0.846659	0.245782	0.052344	0.800429	0.883289	0.207343	0.085655		0.703390
4	0.867066	0.230979	0.056946	0.815451	0.872195	0.230067	0.060013		0.754237
5	0.887950	0.214489	0.061593	0.819743	0.883597	0.172122	0.080348		0.703390
6	0.899130	0.202608	0.064121	0.843348	0.896432	0.216199	0.065906		0.881356
7	0.932008	0.169919	0.083064	0.881974	0.900968	0.144515	0.104116		0.728814
8	0.955856	0.139782	0.113361	0.924893	0.900706	0.153942	0.100711		0.720339
9	0.968223	0.119563	0.126711	0.933476	0.903094	0.150005	0.107143		0.737288

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.901152	0.221848	0.087715	0.776824	0.843950	0.215323	0.057029		0.728814
1	0.914111	0.194502	0.081984	0.858369	0.839476	0.182841	0.087010		0.601695
2	0.907813	0.194149	0.068555	0.851931	0.890656	0.184276	0.072831		0.754237
3	0.944146	0.153323	0.081250	0.920601	0.896921	0.173644	0.089041		0.771186
4	0.975733	0.111730	0.123518	0.961373	0.898216	0.152593	0.098837		0.720339


l2(1e-4) reg in D128 and D16

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.730535	0.350634	0.031763	0.746781	0.831277	0.301273	0.033045		0.872881
1	0.827946	0.286850	0.045257	0.759657	0.742398	0.303824	0.044807		0.559322
2	0.855926	0.265681	0.053305	0.804721	0.874232	0.279806	0.048372		0.881356
3	0.877530	0.242444	0.057466	0.828326	0.895377	0.151978	0.100883		0.677966
4	0.890601	0.227675	0.062353	0.854077	0.871275	0.258837	0.059381		0.796610
5	0.887196	0.230236	0.062802	0.836910	0.882134	0.188951	0.085127		0.737288
6	0.913965	0.201406	0.073158	0.881974	0.894080	0.121060	0.138434		0.644068
7	0.927426	0.184156	0.079063	0.890558	0.892313	0.169097	0.098039		0.720339
8	0.952961	0.150827	0.104000	0.920601	0.905430	0.145000	0.115033		0.745763
9	0.969254	0.123687	0.131406	0.946352	0.904801	0.140500	0.117729		0.720339

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.906912	0.206886	0.076512	0.830472	0.880593	0.212048	0.059291		0.822034
1	0.925404	0.183088	0.079821	0.881974	0.862967	0.153072	0.098413		0.525424
2	0.920864	0.187649	0.076147	0.890558	0.862313	0.150383	0.081292		0.618644
3	0.960901	0.138016	0.106680	0.935622	0.890074	0.142116	0.113821		0.711864
4	0.979710	0.104529	0.166227	0.946352	0.889962	0.124165	0.127807		0.627119


l2(1e-4) reg in D4 and D128 and D16

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.746965	0.356597	0.036272	0.753219	0.810736	0.303005	0.050932		0.694915
1	0.831982	0.286150	0.046834	0.774678	0.849343	0.224123	0.063927		0.711864
2	0.847965	0.273152	0.051659	0.798283	0.820456	0.968324	0.069307		0.652542
3	0.865440	0.258061	0.054847	0.821888	0.883524	0.232579	0.074017		0.813559
4	0.860476	0.261970	0.054844	0.817597	0.878106	0.210284	0.078899		0.728814
5	0.860571	0.260140	0.055206	0.828326	0.882398	0.159896	0.100765		0.669492
6	0.887376	0.231851	0.062301	0.841202	0.887097	0.241007	0.068454		0.822034
7	0.912331	0.201524	0.071707	0.866953	0.899669	0.225381	0.077778		0.771186
8	0.941079	0.166906	0.094688	0.914163	0.904864	0.193010	0.095689		0.771186
9	0.951658	0.149166	0.098723	0.929185	0.905784	0.176656	0.097208		0.796610

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.884460	0.237790	0.070355	0.783262	0.819592	0.130104	0.101727		0.449153
1	0.895997	0.220846	0.070204	0.834764	0.828861	0.641764	0.051514		0.822034
2	0.908313	0.201173	0.069624	0.869099	0.865477	0.174427	0.083166		0.703390
3	0.934551	0.176364	0.083992	0.912017	0.903181	0.204670	0.087703		0.822034
4	0.975185	0.122723	0.124585	0.965665	0.892810	0.171773	0.100599		0.711864


l2(1e-3) reg in D4 and D128 and D16

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.768350	0.535866	0.035984	0.787554	0.848251	0.414959	0.054845		0.762712
1	0.823810	0.379791	0.046230	0.785408	0.848017	0.275625	0.082237		0.635593
2	0.840942	0.317069	0.050761	0.793991	0.808606	1.605693	0.046154		0.813559
3	0.844704	0.314828	0.048976	0.811159	0.861335	0.301888	0.066109		0.737288
4	0.850777	0.308519	0.054305	0.802575	0.862505	0.279652	0.053799		0.822034
5	0.867222	0.306498	0.058046	0.819743	0.880374	0.247203	0.080366		0.669492
6	0.899978	0.250968	0.068213	0.858369	0.897740	0.156046	0.206226		0.449153
7	0.923268	0.220457	0.082151	0.894850	0.882687	0.237899	0.077750		0.796610
8	0.949316	0.169497	0.099459	0.907725	0.898616	0.145913	0.107098		0.728814
9	0.965392	0.134026	0.124894	0.950644	0.899365	0.153614	0.106117		0.720339


l2(1e-5) reg in D4 and D128 and D16

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740786	0.324734	0.036013	0.710300	0.809791	0.318580	0.048342		0.728814
1	0.809016	0.277082	0.043344	0.751073	0.847855	0.255866	0.051165		0.762712
2	0.836957	0.256007	0.047119	0.789700	0.847709	0.258130	0.062004		0.728814
3	0.862059	0.240522	0.055342	0.804721	0.884902	0.152567	0.092031		0.694915
4	0.879009	0.224370	0.057715	0.841202	0.884857	0.181763	0.068130		0.762712
5	0.875771	0.229120	0.057068	0.845494	0.882850	0.215027	0.064891		0.805085
6	0.910773	0.198564	0.071703	0.881974	0.881034	0.158817	0.095758		0.669492
7	0.911591	0.197019	0.075783	0.877682	0.905390	0.196135	0.084897		0.805085
8	0.946697	0.154894	0.097794	0.922747	0.900092	0.157873	0.096296		0.771186
9	0.955693	0.142178	0.108215	0.935622	0.902404	0.156852	0.098291		0.779661


----------------------------------------------------------

decided that l2(1e-4) reg in D4 and D128 and D16 is best in terms of stability and least diff b/w trainauc and validauc

----------------------------------------------------------

finetune 1e-5_1e-4_1e-5 with 4 warmup EP

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.741455	0.349729	0.034185	0.738197	0.778768	0.501286	0.034688		0.838983
1	0.819537	0.297423	0.046889	0.781116	0.873173	0.329927	0.047354		0.864407
2	0.860246	0.264538	0.056376	0.802575	0.869503	0.282142	0.063028		0.822034
3	0.867804	0.258968	0.057786	0.815451	0.844160	0.226630	0.077001		0.644068
4	0.880211	0.247274	0.060957	0.847640	0.828608	0.165753	0.077691		0.593220
5	0.858726	0.257108	0.050360	0.811159	0.895192	0.187372	0.085151		0.694915
6	0.893468	0.226540	0.062375	0.866953	0.893831	0.198997	0.080205		0.796610
7	0.925889	0.190220	0.077954	0.873391	0.889626	0.190832	0.083948		0.771186
8	0.951033	0.155816	0.106965	0.922747	0.894901	0.157421	0.105598		0.703390
9	0.962726	0.133987	0.125000	0.933476	0.899222	0.154066	0.105521		0.728814

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.908948	0.229372	0.099187	0.759657	0.889188	0.147075	0.099744		0.661017
1	0.942015	0.166006	0.105169	0.881974	0.900773	0.164585	0.092593		0.720339
2	0.959356	0.141000	0.110825	0.924893	0.895622	0.158780	0.093182		0.694915
3	0.962285	0.136552	0.120565	0.933476	0.896913	0.154055	0.111258		0.711864
4	0.977431	0.110324	0.149188	0.946352	0.902730	0.132582	0.124242		0.694915


finetune 1e-5_1e-4_1e-5 with 3 warmup EP

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.729050	0.359102	0.033747	0.688841	0.813578	0.383584	0.064774		0.618644
1	0.822809	0.299660	0.049538	0.770386	0.800844	0.662856	0.041685		0.805085
2	0.841474	0.278316	0.052277	0.798283	0.884413	0.332997	0.052277		0.855932
3	0.865895	0.254384	0.058383	0.819743	0.886091	0.205203	0.066294		0.805085
4	0.868258	0.250999	0.056515	0.828326	0.855556	0.212343	0.059157		0.796610
5	0.878624	0.237349	0.058060	0.834764	0.878928	0.160576	0.094007		0.677966
6	0.908006	0.209258	0.073393	0.864807	0.891884	0.160362	0.097041		0.694915
7	0.924189	0.190921	0.080935	0.869099	0.878201	0.140408	0.120661		0.618644
8	0.946050	0.165817	0.099718	0.912017	0.899995	0.153231	0.109415		0.728814
9	0.965625	0.134122	0.123972	0.937768	0.900160	0.142629	0.111111		0.677966

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.907003	0.231734	0.098618	0.781116	0.878867	0.166057	0.084052		0.661017
1	0.943555	0.165962	0.103919	0.899142	0.882703	0.161562	0.091647		0.669492
2	0.960624	0.141314	0.113908	0.942060	0.873628	0.141729	0.099573		0.593220
3	0.970692	0.125637	0.129222	0.944206	0.880198	0.149339	0.104735		0.618644
4	0.982137	0.103727	0.154449	0.972103	0.885711	0.148432	0.110783		0.635593


finetune 1e-6_1e-5_1e-6 with 4 warmup EP

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.746420	0.348676	0.034469	0.740343	0.842961	0.348009	0.040075		0.906780
1	0.832939	0.282055	0.046689	0.789700	0.772819	0.239719	0.055804		0.635593
2	0.846466	0.272855	0.052551	0.806867	0.875908	0.198031	0.078292		0.745763
3	0.856799	0.262437	0.054373	0.800429	0.777704	0.474640	0.043043		0.728814
4	0.863323	0.258229	0.054990	0.834764	0.875311	0.178575	0.085170		0.720339
5	0.877159	0.240509	0.057995	0.813305	0.865409	0.178015	0.090036		0.635593
6	0.893321	0.222202	0.062122	0.860515	0.866901	0.200497	0.071778		0.745763
7	0.923274	0.192813	0.079738	0.888412	0.886913	0.129658	0.113176		0.567797
8	0.951843	0.153728	0.105341	0.918455	0.903205	0.132593	0.110807		0.686441
9	0.968952	0.121413	0.131703	0.959227	0.897907	0.149122	0.109141		0.677966

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.900641	0.254026	0.098156	0.708154	0.840187	0.155313	0.080311		0.525424
1	0.916238	0.222408	0.104332	0.759657	0.848936	0.167256	0.079439		0.576271
2	0.928728	0.194744	0.107335	0.813305	0.858075	0.167817	0.081655		0.618644
3	0.934789	0.181240	0.107094	0.839056	0.865938	0.144466	0.093750		0.610169
4	0.941398	0.169399	0.108876	0.871245	0.867595	0.141254	0.093176		0.601695



------------------------------------------------------------------------------------------------------
After dataset creation update, LR 1e-3 to 1e-2 to 1e-4 with l2reg (1e-4) on 3 dense layers, and stage2 with 3 epoch warmup and with 0.1LR

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.756457	0.339716	0.036079	0.751073	0.837646	0.369851	0.044360		0.889831
1	0.815656	0.299101	0.046314	0.746781	0.877965	0.281927	0.061818		0.864407
2	0.829180	0.282704	0.045749	0.802575	0.873585	0.393937	0.049289		0.881356
3	0.855569	0.262863	0.055238	0.809013	0.863158	0.229114	0.061276		0.822034
4	0.860268	0.259582	0.056920	0.815451	0.819356	0.589080	0.050619		0.796610
5	0.845125	0.271364	0.051750	0.802575	0.864839	0.188560	0.082136		0.677966
6	0.882757	0.233310	0.058911	0.845494	0.893319	0.213089	0.065942		0.822034
7	0.911003	0.205237	0.071168	0.877682	0.900759	0.187078	0.079610		0.830508
8	0.939607	0.170336	0.092560	0.907725	0.903689	0.157035	0.101925		0.762712
9	0.956450	0.144458	0.115815	0.933476	0.905307	0.152620	0.111111		0.745763

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.901349	0.221164	0.082648	0.819743	0.879825	0.142076	0.089758		0.661017
1	0.916411	0.194154	0.078619	0.864807	0.879287	0.217032	0.066291		0.796610
2	0.919889	0.188968	0.076880	0.884120	0.870874	0.157881	0.081612		0.669492
3	0.905096	0.209565	0.073646	0.860515	0.888957	0.186669	0.071055		0.805085
4	0.930475	0.181998	0.075589	0.916309	0.895929	0.194852	0.080205		0.796610


After dataset creation update, LR 1e-3 to 1e-2 to 1e-4 with l2reg (1e-4) on 3 dense layers, and stage2 with 4 epoch warmup and with 0.1LR

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.736958	0.347074	0.032921	0.759657	0.826679	0.384315	0.055420		0.771186
1	0.821425	0.288400	0.045944	0.787554	0.876574	0.219492	0.071651		0.779661
2	0.828512	0.287536	0.047916	0.757511	0.812667	0.282552	0.044265		0.788136
3	0.847886	0.274727	0.057984	0.802575	0.874343	0.256371	0.057669		0.822034
4	0.875693	0.245464	0.061496	0.821888	0.868499	0.193409	0.068362		0.728814
5	0.859189	0.259879	0.052133	0.815451	0.888523	0.184482	0.082936		0.737288
6	0.886375	0.233317	0.060741	0.847640	0.877455	0.177013	0.089186		0.677966
7	0.911182	0.214199	0.072103	0.869099	0.900042	0.198540	0.081010		0.788136
8	0.943012	0.167946	0.093612	0.912017	0.901251	0.120252	0.120000		0.635593
9	0.956582	0.147104	0.107826	0.931330	0.898420	0.165791	0.095815		0.737288

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.901500	0.222523	0.081327	0.804721	0.894760	0.158437	0.087125		0.762712
1	0.934782	0.173607	0.084680	0.888412	0.846350	0.160999	0.074866		0.593220
2	0.912751	0.198164	0.075076	0.851931	0.881709	0.167090	0.091109		0.703390
3	0.912476	0.201938	0.074763	0.862661	0.867212	0.173976	0.075000		0.686441
4	0.957071	0.149751	0.100207	0.933476	0.899557	0.159762	0.107239		0.677966



Without Tabular input -> dense(4) with LR 1e-3 to 1e-2 to 1e-4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.723489	0.360661	0.031414	0.744635	0.808905	0.213877	0.071281		0.584746
1	0.832341	0.284629	0.045272	0.796137	0.765216	0.330448	0.033026		0.728814
2	0.849334	0.265948	0.050392	0.800429	0.872577	0.232846	0.055071		0.855932
3	0.864904	0.250259	0.052168	0.839056	0.886312	0.291660	0.045788		0.898305
4	0.873742	0.244839	0.056193	0.830472	0.869124	0.187790	0.077578		0.694915
5	0.894217	0.225359	0.064459	0.851931	0.894053	0.205718	0.083015		0.737288
6	0.914067	0.200984	0.072479	0.877682	0.863474	0.335152	0.042783		0.906780
7	0.921786	0.193894	0.076173	0.888412	0.871740	0.167185	0.086279		0.703390
8	0.951032	0.152121	0.099377	0.924893	0.907042	0.138179	0.110553		0.745763
9	0.962785	0.132817	0.110999	0.942060	0.903766	0.150441	0.100111		0.762712


again same

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.757950	0.337392	0.033201	0.789700	0.832507	0.181971	0.078086		0.525424
1	0.802824	0.307537	0.041094	0.796137	0.839632	0.275978	0.044364		0.830508
2	0.839441	0.276423	0.045868	0.832618	0.824848	0.273236	0.043843		0.796610
3	0.843235	0.281065	0.051055	0.804721	0.882697	0.251507	0.079096		0.711864
4	0.852845	0.273827	0.049782	0.834764	0.879847	0.185350	0.093708		0.593220
5	0.874912	0.253485	0.057521	0.847640	0.861517	0.214297	0.061634		0.754237
6	0.888011	0.231423	0.060507	0.875537	0.886519	0.193278	0.076068		0.754237
7	0.921689	0.194242	0.075779	0.907725	0.899439	0.186240	0.081308		0.822034
8	0.941200	0.165795	0.087560	0.933476	0.899531	0.161650	0.092386		0.771186
9	0.960097	0.140598	0.109531	0.942060	0.901901	0.162279	0.099435		0.745763


full train - settings same as above

	auc			loss		precision	recall
0	0.764989	0.328454	0.036045	0.760274
1	0.842815	0.278580	0.050506	0.785959
2	0.864511	0.256204	0.057291	0.813356
3	0.876023	0.241506	0.059846	0.825342
4	0.886776	0.229450	0.062630	0.821918
5	0.869418	0.260301	0.055518	0.840753
6	0.888638	0.244179	0.060539	0.849315
7	0.921239	0.201241	0.078204	0.876712
8	0.947892	0.161428	0.097054	0.919521
9	0.960507	0.137463	0.110324	0.933219

private score : 0.8723, public score : 0.8840



dropout 0.4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.717414	0.367418	0.032555	0.727468	0.805925	0.548483	0.046334		0.771186
1	0.827725	0.294745	0.049130	0.793991	0.881750	0.257544	0.058270		0.838983
2	0.822080	0.292981	0.046668	0.802575	0.844809	0.219894	0.073582		0.703390
3	0.855379	0.271578	0.052924	0.817597	0.881397	0.230498	0.074894		0.745763
4	0.876923	0.247156	0.057973	0.809013	0.866961	0.231307	0.064214		0.754237
5	0.879122	0.239361	0.060303	0.828326	0.904570	0.154568	0.098940		0.711864
6	0.902374	0.217467	0.067164	0.869099	0.894410	0.180408	0.072363		0.796610
7	0.922515	0.195979	0.075673	0.899142	0.894853	0.148486	0.100599		0.711864
8	0.944173	0.166782	0.098745	0.912017	0.897873	0.175767	0.094512		0.788136
9	0.964930	0.133938	0.123249	0.944206	0.894587	0.162505	0.109069		0.754237


dropout 0.4 LR 1e-3_1e-2_1e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.730829	0.362229	0.033383	0.727468	0.794151	0.388695	0.050790		0.762712
1	0.803364	0.313434	0.045081	0.776824	0.844788	0.255693	0.067315		0.703390
2	0.840150	0.282203	0.050131	0.781116	0.828657	0.228656	0.070577		0.652542
3	0.856149	0.269929	0.053640	0.804721	0.846377	0.234019	0.062404		0.686441
4	0.869142	0.251184	0.055798	0.824034	0.895652	0.263457	0.057860		0.898305
5	0.889644	0.229598	0.060496	0.854077	0.890273	0.232650	0.070896		0.805085
6	0.907405	0.207825	0.067650	0.875537	0.885410	0.209572	0.060373		0.796610
7	0.915571	0.202458	0.074270	0.873391	0.901326	0.201592	0.079299		0.805085
8	0.950316	0.156951	0.103374	0.927039	0.901616	0.144688	0.106061		0.711864
9	0.961091	0.139647	0.123404	0.933476	0.903143	0.152729	0.100578		0.737288


saturation 0.7, 1.3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.720578	0.368829	0.033547	0.708154	0.825300	0.458595	0.042010		0.864407
1	0.827681	0.291079	0.047407	0.800429	0.868708	0.286952	0.053163		0.847458
2	0.805303	0.306287	0.041443	0.746781	0.822179	0.280814	0.045966		0.796610
3	0.850029	0.276542	0.051268	0.811159	0.877247	0.235589	0.062500		0.805085
4	0.859247	0.262494	0.051188	0.836910	0.884262	0.215430	0.077181		0.779661
5	0.882178	0.247306	0.058485	0.849785	0.881273	0.178401	0.096894		0.661017
6	0.896601	0.226369	0.065854	0.869099	0.893001	0.156712	0.097156		0.694915
7	0.926361	0.191404	0.077150	0.899142	0.879003	0.167877	0.082700		0.737288
8	0.941187	0.172454	0.090655	0.903434	0.895975	0.145266	0.104798		0.703390
9	0.960387	0.142958	0.115415	0.946352	0.895760	0.147931	0.100350		0.728814


saturation 0.5, 1.5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.731714	0.355658	0.033826	0.727468	0.850447	0.314518	0.051651		0.822034
1	0.821075	0.299228	0.046855	0.776824	0.861001	0.276412	0.065434		0.779661
2	0.847427	0.281558	0.053372	0.821888	0.874196	0.425085	0.057946		0.855932
3	0.833186	0.286384	0.045449	0.796137	0.866728	0.244459	0.059673		0.805085
4	0.864911	0.257300	0.055701	0.819743	0.861618	0.207419	0.082377		0.669492
5	0.867736	0.250378	0.053088	0.828326	0.889851	0.198524	0.084259		0.771186
6	0.885694	0.230601	0.060076	0.877682	0.895645	0.209646	0.071480		0.838983
7	0.909009	0.210933	0.069536	0.884120	0.889558	0.178076	0.090000		0.686441
8	0.938914	0.173379	0.090948	0.920601	0.897181	0.177109	0.084762		0.754237
9	0.961278	0.136908	0.117033	0.937768	0.897687	0.142015	0.097254		0.720339

saturation 0.5, 1.5 again

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.758782	0.340858	0.039038	0.721030	0.768939	0.655839	0.053302		0.567797
1	0.810987	0.302237	0.045495	0.759657	0.834554	0.316779	0.044719		0.796610
2	0.835125	0.282383	0.049133	0.802575	0.844150	0.220351	0.097983		0.576271
3	0.851409	0.270222	0.052508	0.813305	0.873208	0.296326	0.058824		0.898305
4	0.857543	0.271293	0.053383	0.802575	0.889223	0.205342	0.091981		0.661017
5	0.875996	0.253570	0.058788	0.834764	0.889952	0.253790	0.059941		0.855932
6	0.900055	0.224258	0.066802	0.847640	0.874695	0.205394	0.060361		0.822034
7	0.922493	0.195646	0.078703	0.890558	0.900109	0.190593	0.078577		0.805085
8	0.938608	0.175215	0.093996	0.896996	0.897429	0.140701	0.106061		0.652542
9	0.959005	0.142326	0.116329	0.935622	0.897986	0.126809	0.118644		0.652542


saturation 0.8, 1.2

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.752029	0.344879	0.034136	0.763949	0.845523	0.278615	0.048711		0.864407
1	0.811184	0.303537	0.044174	0.770386	0.832950	0.276426	0.053959		0.779661
2	0.843711	0.277615	0.052460	0.793991	0.873121	0.262603	0.063087		0.796610
3	0.826296	0.297309	0.046897	0.783262	0.814991	0.344471	0.032781		0.838983
4	0.856568	0.263329	0.054325	0.789700	0.855304	0.286436	0.043199		0.855932
5	0.866327	0.251824	0.052833	0.830472	0.866274	0.214978	0.098972		0.652542
6	0.896532	0.227629	0.068488	0.849785	0.883796	0.185756	0.082759		0.711864
7	0.924160	0.195253	0.079626	0.894850	0.883848	0.180273	0.084586		0.762712
8	0.941556	0.172360	0.094667	0.914163	0.891169	0.173646	0.092998		0.720339
9	0.960093	0.144439	0.112819	0.929185	0.896352	0.146129	0.109756		0.686441


saturation 0.9, 1.1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.736090	0.355532	0.033941	0.718884	0.857991	0.299859	0.054264		0.830508
1	0.806230	0.303618	0.043126	0.766094	0.728274	0.490316	0.034374		0.830508
2	0.846783	0.274765	0.051001	0.798283	0.885806	0.261138	0.060160		0.830508
3	0.861958	0.259480	0.052916	0.839056	0.853166	0.227359	0.064715		0.711864
4	0.828971	0.283517	0.045960	0.806867	0.848986	0.300100	0.066453		0.703390
5	0.871988	0.251713	0.055894	0.826180	0.880335	0.173306	0.099476		0.644068
6	0.880603	0.243848	0.059236	0.834764	0.887967	0.244431	0.066051		0.788136
7	0.893978	0.223547	0.060363	0.871245	0.875262	0.236524	0.060936		0.805085
8	0.920294	0.195591	0.074870	0.896996	0.887899	0.262811	0.082272		0.711864
9	0.945107	0.163048	0.093492	0.924893	0.896734	0.209423	0.083184		0.788136


saturation 0.8, 1.5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.749916	0.348657	0.035855	0.740343	0.816876	0.348513	0.039778		0.788136
1	0.820456	0.293670	0.047039	0.761803	0.830227	0.342822	0.054499		0.728814
2	0.851339	0.272886	0.051415	0.806867	0.857628	0.276737	0.048000		0.864407
3	0.864691	0.255965	0.053401	0.839056	0.868353	0.332276	0.085682		0.644068
4	0.853006	0.271139	0.051050	0.824034	0.876033	0.235487	0.058407		0.838983
5	0.884853	0.240630	0.061228	0.845494	0.884054	0.181591	0.078112		0.771186
6	0.903681	0.215227	0.065881	0.875537	0.903023	0.142934	0.111255		0.728814
7	0.930805	0.183649	0.083267	0.901288	0.888540	0.168181	0.090367		0.771186
8	0.959946	0.141586	0.115293	0.939914	0.875966	0.146062	0.101430		0.661017
9	0.967374	0.125922	0.130949	0.950644	0.897662	0.119096	0.124417		0.677966

saturation 0.8, 1.5 again

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.731267	0.358535	0.032688	0.789700	0.800093	0.232680	0.076223		0.567797
1	0.807914	0.309327	0.040271	0.804721	0.851315	0.391931	0.037196		0.881356
2	0.845709	0.278523	0.049107	0.826180	0.849941	0.236775	0.066347		0.703390
3	0.857370	0.265294	0.050800	0.824034	0.850215	0.227415	0.052836		0.805085
4	0.857746	0.263670	0.050566	0.824034	0.866814	0.184531	0.069914		0.754237
5	0.875066	0.252516	0.054796	0.860515	0.845564	0.240566	0.054886		0.694915
6	0.893011	0.232038	0.063022	0.858369	0.880425	0.216356	0.063492		0.779661
7	0.920770	0.198335	0.074126	0.909871	0.886765	0.204833	0.072917		0.771186
8	0.940700	0.170601	0.083445	0.931330	0.896492	0.151376	0.092677		0.686441
9	0.956509	0.145076	0.098569	0.931330	0.890398	0.164863	0.091097		0.745763



saturation 1.0, 1.5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.730615	0.352826	0.033330	0.733906	0.836259	0.247875	0.054391		0.703390
1	0.827069	0.290761	0.045449	0.789700	0.852627	0.332434	0.047222		0.864407
2	0.838553	0.287436	0.049451	0.802575	0.849702	0.303669	0.056364		0.788136
3	0.866013	0.257791	0.055395	0.824034	0.865094	0.256997	0.059615		0.788136
4	0.864795	0.265220	0.055865	0.817597	0.875953	0.215703	0.075928		0.745763
5	0.882561	0.240434	0.057302	0.860515	0.890355	0.223341	0.061736		0.855932
6	0.903135	0.222811	0.071127	0.866953	0.878106	0.248838	0.064238		0.822034
7	0.919183	0.206174	0.079655	0.871245	0.898347	0.220274	0.075358		0.847458
8	0.951561	0.155676	0.104922	0.937768	0.898915	0.155427	0.103666		0.694915
9	0.960437	0.142699	0.123861	0.933476	0.899754	0.152904	0.105528		0.711864




Using saturation 1.0, 1.5 for now 

CLAHE with clip_limit=0.01

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.724970	0.363374	0.032985	0.703863	0.799358	0.334594	0.038945		0.788136
1	0.810915	0.304576	0.045365	0.738197	0.828407	0.325470	0.059923		0.788136
2	0.839564	0.285904	0.052893	0.800429	0.858207	0.276413	0.058278		0.745763
3	0.870863	0.254418	0.057486	0.841202	0.879607	0.179671	0.095293		0.703390
4	0.886741	0.236250	0.061026	0.849785	0.863494	0.231222	0.070579		0.754237
5	0.890688	0.230242	0.060463	0.851931	0.875322	0.235262	0.058426		0.805085
6	0.902510	0.219383	0.068489	0.871245	0.892422	0.152589	0.110193		0.677966
7	0.933079	0.182403	0.087117	0.914163	0.877438	0.157290	0.087196		0.669492
8	0.955103	0.149553	0.107872	0.929185	0.886631	0.155034	0.097814		0.720339
9	0.969551	0.122337	0.140640	0.952790	0.876358	0.130703	0.117355		0.601695


CLAHE with clip_limit=5e-3 with clip before&after

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.729654	0.357176	0.034176	0.742489	0.810559	0.522135	0.036808		0.855932
1	0.823706	0.296573	0.046114	0.804721	0.838967	0.216062	0.073149		0.694915
2	0.834977	0.285374	0.049840	0.802575	0.833182	0.245277	0.058591		0.754237
3	0.861111	0.260372	0.053659	0.819743	0.838356	0.252136	0.070845		0.661017
4	0.844389	0.272891	0.051003	0.813305	0.873153	0.310725	0.043174		0.889831
5	0.879402	0.238310	0.057930	0.843348	0.886929	0.252308	0.054802		0.889831
6	0.899343	0.220687	0.068127	0.847640	0.870702	0.204802	0.081614		0.737288
7	0.918546	0.199897	0.079246	0.884120	0.878117	0.178702	0.083749		0.711864
8	0.946529	0.163807	0.101505	0.912017	0.890128	0.131810	0.124615		0.686441
9	0.963057	0.136381	0.129295	0.920601	0.897851	0.116703	0.134948		0.661017


CLAHE with clip_limit=5e-3 with clip before&after LR 3e-3_1e-2_3e-3
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.721571	0.357083	0.031361	0.723176	0.745573	1.131984	0.037154		0.796610
1	0.793362	0.317454	0.040953	0.748927	0.858339	0.348348	0.041128		0.889831
2	0.818144	0.299114	0.048383	0.783262	0.867615	0.236910	0.077922		0.711864
3	0.855108	0.265158	0.052155	0.828326	0.868633	0.224851	0.069145		0.788136
4	0.866537	0.259088	0.052675	0.824034	0.856635	0.235790	0.062187		0.737288
5	0.879227	0.248359	0.059477	0.849785	0.855620	0.193276	0.070819		0.703390
6	0.883739	0.252195	0.060292	0.849785	0.874589	0.166903	0.093525		0.661017
7	0.906432	0.228448	0.070923	0.881974	0.884476	0.269789	0.077261		0.745763
8	0.929700	0.200906	0.083267	0.899142	0.892164	0.198419	0.093288		0.694915
9	0.947154	0.170776	0.101646	0.914163	0.891207	0.178136	0.089662		0.720339


saturation 1.2, 1.5 without CLAHE

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.761167	0.330207	0.035504	0.748927	0.833611	0.376093	0.052876		0.771186
1	0.824209	0.290358	0.048499	0.776824	0.832896	0.340212	0.048604		0.796610
2	0.848001	0.273639	0.049921	0.809013	0.874732	0.243612	0.069510		0.745763
3	0.867230	0.253643	0.054210	0.826180	0.868250	0.243705	0.061370		0.805085
4	0.876290	0.252631	0.058639	0.843348	0.875315	0.255738	0.048685		0.847458
5	0.875227	0.247545	0.056797	0.845494	0.876988	0.297413	0.049953		0.898305
6	0.895026	0.238501	0.066622	0.856223	0.890502	0.238737	0.070433		0.813559
7	0.918436	0.209292	0.073916	0.892704	0.894147	0.187591	0.088975		0.779661
8	0.945910	0.169546	0.101891	0.924893	0.901382	0.143788	0.113869		0.661017
9	0.959698	0.143833	0.121305	0.933476	0.902772	0.150533	0.105660		0.711864


saturation 1.2, 1.5 modified aug method

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.769961	0.328909	0.035866	0.813305	0.844904	0.278510	0.066879		0.711864
1	0.824560	0.289156	0.045664	0.763949	0.882429	0.276925	0.065052		0.796610
2	0.852625	0.268983	0.053210	0.800429	0.843500	0.250106	0.058978		0.762712
3	0.863336	0.264088	0.055821	0.826180	0.888102	0.294469	0.049097		0.898305
4	0.882331	0.242481	0.059955	0.856223	0.878323	0.204749	0.080897		0.703390
5	0.888721	0.242111	0.064142	0.843348	0.847518	0.262045	0.053664		0.788136
6	0.901244	0.232945	0.070798	0.860515	0.884567	0.186943	0.082652		0.771186
7	0.919530	0.202431	0.074982	0.873391	0.884260	0.178586	0.085437		0.745763
8	0.954127	0.154948	0.103083	0.918455	0.899681	0.168299	0.092119		0.762712
9	0.969631	0.123857	0.133434	0.946352	0.907166	0.139209	0.113402		0.745763


CLAHE with clip_limit=5e-3 with clip before&after with replace_zeros with saturation 1.2, 1.5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.744127	0.353201	0.037177	0.738197	0.748092	0.579202	0.040549		0.576271
1	0.820680	0.300298	0.046939	0.761803	0.869075	0.310587	0.042410		0.906780
2	0.858064	0.269492	0.054191	0.828326	0.872284	0.254851	0.055620		0.813559
3	0.860118	0.261934	0.050319	0.830472	0.862088	0.211651	0.085366		0.652542
4	0.886576	0.247654	0.064277	0.839056	0.863567	0.170746	0.085620		0.661017
5	0.876149	0.245229	0.054960	0.847640	0.853417	0.194839	0.069420		0.669492
6	0.890478	0.243325	0.063559	0.869099	0.886990	0.189073	0.091319		0.686441
7	0.924635	0.194364	0.076866	0.890558	0.862909	0.137957	0.095989		0.567797
8	0.947112	0.161653	0.098833	0.927039	0.891574	0.135874	0.109459		0.686441
9	0.964075	0.133822	0.119440	0.933476	0.892737	0.118397	0.119874		0.644068


CLAHE with clip_limit=5e-3 with clip before&after with add_small_random with saturation 1.2, 1.5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.727981	0.353666	0.033225	0.701717	0.792266	0.759229	0.054396		0.618644
1	0.807759	0.300767	0.047218	0.746781	0.843568	0.222484	0.070812		0.694915
2	0.862609	0.262814	0.055776	0.813305	0.859354	0.176195	0.094148		0.627119
3	0.841561	0.290616	0.053623	0.800429	0.844377	0.262105	0.061422		0.754237
4	0.864460	0.268154	0.058948	0.834764	0.872530	0.223874	0.070804		0.754237
5	0.877487	0.249837	0.062185	0.821888	0.860874	0.223230	0.060339		0.754237
6	0.910937	0.212219	0.074121	0.873391	0.880904	0.155423	0.100132		0.644068
7	0.930811	0.187278	0.084173	0.914163	0.864627	0.223717	0.072156		0.720339
8	0.947815	0.163802	0.098706	0.916309	0.888477	0.112000	0.131387		0.610169
9	0.969840	0.123993	0.141587	0.957082	0.883879	0.134398	0.107776		0.669492


age_approx scaled

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.741112	0.349615	0.033529	0.697425	0.835267	0.400554	0.041058		0.881356
1	0.816029	0.298595	0.047172	0.776824	0.867185	0.295865	0.057681		0.830508
2	0.853935	0.272725	0.055095	0.813305	0.862185	0.283106	0.044975		0.830508
3	0.869088	0.256805	0.056911	0.826180	0.891159	0.228982	0.066291		0.796610
4	0.873323	0.249562	0.056658	0.845494	0.892062	0.211199	0.078553		0.754237
5	0.884100	0.234614	0.057560	0.828326	0.887741	0.180138	0.084008		0.703390
6	0.906534	0.210873	0.071186	0.899142	0.866991	0.129810	0.118859		0.635593
7	0.925482	0.212698	0.082571	0.890558	0.886749	0.169407	0.104922		0.686441
8	0.942855	0.183384	0.094828	0.920601	0.898834	0.144096	0.108497		0.703390
9	0.966563	0.141319	0.120906	0.950644	0.898545	0.144442	0.113703		0.661017


age_approx scaled again

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.730848	0.354868	0.032305	0.712446	0.797364	0.302987	0.045309		0.720339
1	0.794240	0.315173	0.045905	0.725322	0.849540	0.317800	0.044796		0.838983
2	0.841833	0.279565	0.052843	0.791846	0.864632	0.347955	0.048918		0.881356
3	0.849937	0.274895	0.051378	0.796137	0.866400	0.288399	0.046305		0.838983
4	0.871898	0.252616	0.059718	0.826180	0.845686	0.209341	0.063345		0.754237
5	0.884360	0.239128	0.059857	0.826180	0.861736	0.247497	0.054991		0.779661
6	0.903890	0.218059	0.069627	0.864807	0.888552	0.148320	0.100765		0.669492
7	0.928705	0.188229	0.080832	0.884120	0.893570	0.157514	0.098765		0.745763
8	0.951966	0.155581	0.105238	0.931330	0.897470	0.152580	0.095402		0.703390
9	0.969009	0.123977	0.132092	0.959227	0.901668	0.141805	0.112861		0.728814


age_approx scaled full train

	auc			loss		precision	recall
0	0.736511	0.349758	0.032967	0.765411
1	0.826873	0.294608	0.046343	0.787671
2	0.842908	0.284267	0.050998	0.813356
3	0.850645	0.272813	0.050614	0.804795
4	0.858615	0.265913	0.054630	0.803082
5	0.860524	0.261993	0.055457	0.801370
6	0.886200	0.238755	0.063056	0.825342
7	0.916255	0.207239	0.073900	0.888699
8	0.934388	0.180894	0.085374	0.919521
9	0.950248	0.155676	0.102795	0.938356

private score : 0.8709, public score : 0.8824