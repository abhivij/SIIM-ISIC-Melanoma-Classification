hvflip+zoom+brightness on 128x128 images

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 182s 1s/step - auc: 0.6308 - loss: 0.6506 - val_auc: 0.8321 - val_loss: 0.4352
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 61s 514ms/step - auc: 0.7792 - loss: 0.5632 - val_auc: 0.8408 - val_loss: 0.5263
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 58s 485ms/step - auc: 0.8114 - loss: 0.5239 - val_auc: 0.8489 - val_loss: 0.5414
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 58s 489ms/step - auc: 0.8040 - loss: 0.5293 - val_auc: 0.8514 - val_loss: 0.4967
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 487ms/step - auc: 0.8045 - loss: 0.5296 - val_auc: 0.8514 - val_loss: 0.4961
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 486ms/step - auc: 0.8056 - loss: 0.5316 - val_auc: 0.8568 - val_loss: 0.4827
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 484ms/step - auc: 0.8201 - loss: 0.5061 - val_auc: 0.8562 - val_loss: 0.4919
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 59s 495ms/step - auc: 0.8328 - loss: 0.4992 - val_auc: 0.8599 - val_loss: 0.4471
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 485ms/step - auc: 0.8331 - loss: 0.5027 - val_auc: 0.8600 - val_loss: 0.4057
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 59s 501ms/step - auc: 0.8343 - loss: 0.4942 - val_auc: 0.8598 - val_loss: 0.4155
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 482ms/step - auc: 0.8376 - loss: 0.4872 - val_auc: 0.8608 - val_loss: 0.4481
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 482ms/step - auc: 0.8306 - loss: 0.4995 - val_auc: 0.8631 - val_loss: 0.4469
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 481ms/step - auc: 0.8398 - loss: 0.4846 - val_auc: 0.8636 - val_loss: 0.4352
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 489ms/step - auc: 0.8174 - loss: 0.5189 - val_auc: 0.8637 - val_loss: 0.4461
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 58s 497ms/step - auc: 0.8387 - loss: 0.4848 - val_auc: 0.8638 - val_loss: 0.4457
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 481ms/step - auc: 0.8489 - loss: 0.4775 - val_auc: 0.8660 - val_loss: 0.3871
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 481ms/step - auc: 0.8304 - loss: 0.4949 - val_auc: 0.8664 - val_loss: 0.4597
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 484ms/step - auc: 0.8333 - loss: 0.5116 - val_auc: 0.8653 - val_loss: 0.3330
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 58s 492ms/step - auc: 0.8374 - loss: 0.4982 - val_auc: 0.8643 - val_loss: 0.3587
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 482ms/step - auc: 0.8314 - loss: 0.4817 - val_auc: 0.8684 - val_loss: 0.4580


hvflip+zoom on 128x128 images

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 168s 1s/step - auc: 0.6503 - loss: 0.6602 - val_auc: 0.8145 - val_loss: 0.5016
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 58s 490ms/step - auc: 0.7716 - loss: 0.5570 - val_auc: 0.8367 - val_loss: 0.5140
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 490ms/step - auc: 0.7914 - loss: 0.5482 - val_auc: 0.8453 - val_loss: 0.5162
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 56s 479ms/step - auc: 0.8069 - loss: 0.5323 - val_auc: 0.8468 - val_loss: 0.4821
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 480ms/step - auc: 0.8122 - loss: 0.5142 - val_auc: 0.8470 - val_loss: 0.4817
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 83s 486ms/step - auc: 0.8159 - loss: 0.5184 - val_auc: 0.8486 - val_loss: 0.3904
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 483ms/step - auc: 0.8183 - loss: 0.5098 - val_auc: 0.8552 - val_loss: 0.4280
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 482ms/step - auc: 0.8079 - loss: 0.5190 - val_auc: 0.8553 - val_loss: 0.4170
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 479ms/step - auc: 0.8314 - loss: 0.5007 - val_auc: 0.8602 - val_loss: 0.4947
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 478ms/step - auc: 0.8344 - loss: 0.4937 - val_auc: 0.8606 - val_loss: 0.4874
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 56s 475ms/step - auc: 0.8419 - loss: 0.4882 - val_auc: 0.8618 - val_loss: 0.4822
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 483ms/step - auc: 0.8380 - loss: 0.4947 - val_auc: 0.8604 - val_loss: 0.4620
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 56s 475ms/step - auc: 0.8277 - loss: 0.4969 - val_auc: 0.8615 - val_loss: 0.4672
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 58s 494ms/step - auc: 0.8429 - loss: 0.4865 - val_auc: 0.8615 - val_loss: 0.4455
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 56s 480ms/step - auc: 0.8276 - loss: 0.4976 - val_auc: 0.8615 - val_loss: 0.4454
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 487ms/step - auc: 0.8370 - loss: 0.4904 - val_auc: 0.8635 - val_loss: 0.4244
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 55s 470ms/step - auc: 0.8314 - loss: 0.4952 - val_auc: 0.8623 - val_loss: 0.4271
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 481ms/step - auc: 0.8378 - loss: 0.4774 - val_auc: 0.8649 - val_loss: 0.4562
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 489ms/step - auc: 0.8641 - loss: 0.4571 - val_auc: 0.8646 - val_loss: 0.4762
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 57s 487ms/step - auc: 0.8469 - loss: 0.4800 - val_auc: 0.8656 - val_loss: 0.3690



hvflip+brightness on 128x128 images

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 153s 967ms/step - auc: 0.6657 - loss: 0.6349 - val_auc: 0.8322 - val_loss: 0.4638
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 417ms/step - auc: 0.7713 - loss: 0.5749 - val_auc: 0.8467 - val_loss: 0.5453
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 411ms/step - auc: 0.7895 - loss: 0.5543 - val_auc: 0.8522 - val_loss: 0.5487
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 412ms/step - auc: 0.8166 - loss: 0.5167 - val_auc: 0.8544 - val_loss: 0.5156
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 410ms/step - auc: 0.8216 - loss: 0.5265 - val_auc: 0.8547 - val_loss: 0.5130
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 410ms/step - auc: 0.8230 - loss: 0.5105 - val_auc: 0.8607 - val_loss: 0.5157
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 425ms/step - auc: 0.8193 - loss: 0.5079 - val_auc: 0.8635 - val_loss: 0.4831
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 408ms/step - auc: 0.7996 - loss: 0.5333 - val_auc: 0.8658 - val_loss: 0.4564
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 424ms/step - auc: 0.8303 - loss: 0.5047 - val_auc: 0.8643 - val_loss: 0.5601
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 402ms/step - auc: 0.8369 - loss: 0.5010 - val_auc: 0.8666 - val_loss: 0.4529
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 417ms/step - auc: 0.8511 - loss: 0.4758 - val_auc: 0.8676 - val_loss: 0.4727
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 423ms/step - auc: 0.8482 - loss: 0.4742 - val_auc: 0.8675 - val_loss: 0.4809
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 418ms/step - auc: 0.8478 - loss: 0.4795 - val_auc: 0.8679 - val_loss: 0.4864
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 410ms/step - auc: 0.8553 - loss: 0.4680 - val_auc: 0.8680 - val_loss: 0.4709
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 408ms/step - auc: 0.8508 - loss: 0.4788 - val_auc: 0.8679 - val_loss: 0.4698
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 405ms/step - auc: 0.8358 - loss: 0.4986 - val_auc: 0.8642 - val_loss: 0.4745
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 420ms/step - auc: 0.8555 - loss: 0.4627 - val_auc: 0.8670 - val_loss: 0.4637
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 406ms/step - auc: 0.8407 - loss: 0.4982 - val_auc: 0.8688 - val_loss: 0.5120
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 426ms/step - auc: 0.8523 - loss: 0.4716 - val_auc: 0.8707 - val_loss: 0.4264
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 417ms/step - auc: 0.8568 - loss: 0.4727 - val_auc: 0.8715 - val_loss: 0.4863


hvflip+brightness on 128x128 images with 2e-3 learning rate

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 165s 1s/step - auc: 0.6185 - loss: 0.6587 - val_auc: 0.8419 - val_loss: 0.5078
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 418ms/step - auc: 0.7965 - loss: 0.5372 - val_auc: 0.8579 - val_loss: 0.4606
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 406ms/step - auc: 0.8204 - loss: 0.5053 - val_auc: 0.8614 - val_loss: 0.4190
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 405ms/step - auc: 0.8200 - loss: 0.5128 - val_auc: 0.8638 - val_loss: 0.5001
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 403ms/step - auc: 0.8295 - loss: 0.4976 - val_auc: 0.8635 - val_loss: 0.4854
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 404ms/step - auc: 0.8247 - loss: 0.5058 - val_auc: 0.8615 - val_loss: 0.5285
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 411ms/step - auc: 0.8374 - loss: 0.4881 - val_auc: 0.8586 - val_loss: 0.4960
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 405ms/step - auc: 0.8378 - loss: 0.5007 - val_auc: 0.8625 - val_loss: 0.5594
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 401ms/step - auc: 0.8481 - loss: 0.4828 - val_auc: 0.8667 - val_loss: 0.4722
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 412ms/step - auc: 0.8378 - loss: 0.4764 - val_auc: 0.8663 - val_loss: 0.5165
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 407ms/step - auc: 0.8472 - loss: 0.4704 - val_auc: 0.8671 - val_loss: 0.4387
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 402ms/step - auc: 0.8459 - loss: 0.4838 - val_auc: 0.8676 - val_loss: 0.4316
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 403ms/step - auc: 0.8550 - loss: 0.4680 - val_auc: 0.8670 - val_loss: 0.4679
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 403ms/step - auc: 0.8496 - loss: 0.4779 - val_auc: 0.8681 - val_loss: 0.4684
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 412ms/step - auc: 0.8558 - loss: 0.4647 - val_auc: 0.8678 - val_loss: 0.4621
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 405ms/step - auc: 0.8529 - loss: 0.4649 - val_auc: 0.8664 - val_loss: 0.4953
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 404ms/step - auc: 0.8624 - loss: 0.4574 - val_auc: 0.8697 - val_loss: 0.4448
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 406ms/step - auc: 0.8457 - loss: 0.4775 - val_auc: 0.8653 - val_loss: 0.3798
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 413ms/step - auc: 0.8665 - loss: 0.4494 - val_auc: 0.8725 - val_loss: 0.4391
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 404ms/step - auc: 0.8760 - loss: 0.4357 - val_auc: 0.8733 - val_loss: 0.5215



hvflip on 128x128 images

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 161s 1s/step - auc: 0.6304 - loss: 0.6525 - val_auc: 0.8191 - val_loss: 0.4770
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 409ms/step - auc: 0.7913 - loss: 0.5528 - val_auc: 0.8382 - val_loss: 0.5297
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 409ms/step - auc: 0.7997 - loss: 0.5463 - val_auc: 0.8432 - val_loss: 0.5104
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 403ms/step - auc: 0.8078 - loss: 0.5217 - val_auc: 0.8458 - val_loss: 0.5007
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 399ms/step - auc: 0.8362 - loss: 0.5007 - val_auc: 0.8455 - val_loss: 0.4885
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 407ms/step - auc: 0.8091 - loss: 0.5029 - val_auc: 0.8504 - val_loss: 0.4771
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 46s 395ms/step - auc: 0.8241 - loss: 0.5006 - val_auc: 0.8538 - val_loss: 0.5127
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 46s 393ms/step - auc: 0.8235 - loss: 0.5038 - val_auc: 0.8547 - val_loss: 0.4945
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 412ms/step - auc: 0.8404 - loss: 0.4910 - val_auc: 0.8575 - val_loss: 0.4579
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 409ms/step - auc: 0.8363 - loss: 0.4886 - val_auc: 0.8599 - val_loss: 0.4858
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 46s 395ms/step - auc: 0.8600 - loss: 0.4623 - val_auc: 0.8602 - val_loss: 0.4718
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 402ms/step - auc: 0.8553 - loss: 0.4675 - val_auc: 0.8599 - val_loss: 0.5001
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 401ms/step - auc: 0.8587 - loss: 0.4631 - val_auc: 0.8611 - val_loss: 0.4772
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 415ms/step - auc: 0.8440 - loss: 0.4721 - val_auc: 0.8612 - val_loss: 0.4708
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 415ms/step - auc: 0.8613 - loss: 0.4555 - val_auc: 0.8613 - val_loss: 0.4669
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 412ms/step - auc: 0.8581 - loss: 0.4667 - val_auc: 0.8604 - val_loss: 0.3938
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 405ms/step - auc: 0.8582 - loss: 0.4705 - val_auc: 0.8616 - val_loss: 0.4819
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 45s 389ms/step - auc: 0.8503 - loss: 0.4685 - val_auc: 0.8615 - val_loss: 0.4871
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 46s 394ms/step - auc: 0.8571 - loss: 0.4668 - val_auc: 0.8626 - val_loss: 0.4513
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 411ms/step - auc: 0.8628 - loss: 0.4512 - val_auc: 0.8619 - val_loss: 0.4731


conditional hvflip, zoom, brightness on 128x128 images with 256 batch size with 1e-3 learning rate

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 160s 1s/step - auc: 0.6456 - loss: 0.6509 - val_auc: 0.8249 - val_loss: 0.5575
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 407ms/step - auc: 0.8043 - loss: 0.5419 - val_auc: 0.8411 - val_loss: 0.5219
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 402ms/step - auc: 0.8081 - loss: 0.5225 - val_auc: 0.8473 - val_loss: 0.5262
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 409ms/step - auc: 0.8236 - loss: 0.5162 - val_auc: 0.8505 - val_loss: 0.5080
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 408ms/step - auc: 0.8130 - loss: 0.5340 - val_auc: 0.8502 - val_loss: 0.5024
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 408ms/step - auc: 0.8255 - loss: 0.5114 - val_auc: 0.8586 - val_loss: 0.4711
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 411ms/step - auc: 0.8381 - loss: 0.5072 - val_auc: 0.8611 - val_loss: 0.4312
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 410ms/step - auc: 0.8279 - loss: 0.4916 - val_auc: 0.8618 - val_loss: 0.4386
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 416ms/step - auc: 0.8409 - loss: 0.4909 - val_auc: 0.8631 - val_loss: 0.4772
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 410ms/step - auc: 0.8513 - loss: 0.4660 - val_auc: 0.8631 - val_loss: 0.5169
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 414ms/step - auc: 0.8373 - loss: 0.4863 - val_auc: 0.8634 - val_loss: 0.4702
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 406ms/step - auc: 0.8584 - loss: 0.4638 - val_auc: 0.8637 - val_loss: 0.4717
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 415ms/step - auc: 0.8527 - loss: 0.4696 - val_auc: 0.8631 - val_loss: 0.4693
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 410ms/step - auc: 0.8470 - loss: 0.4771 - val_auc: 0.8633 - val_loss: 0.4620
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 408ms/step - auc: 0.8558 - loss: 0.4744 - val_auc: 0.8632 - val_loss: 0.4595
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 403ms/step - auc: 0.8525 - loss: 0.4684 - val_auc: 0.8636 - val_loss: 0.4319
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 405ms/step - auc: 0.8455 - loss: 0.4778 - val_auc: 0.8640 - val_loss: 0.3925
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 408ms/step - auc: 0.8494 - loss: 0.4726 - val_auc: 0.8628 - val_loss: 0.4276
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 411ms/step - auc: 0.8483 - loss: 0.4764 - val_auc: 0.8658 - val_loss: 0.4052
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 47s 408ms/step - auc: 0.8511 - loss: 0.4616 - val_auc: 0.8632 - val_loss: 0.4560


conditional hvflip, zoom, brightness on 128x128 images with 128 batch size with 1e-3 learning rate on CPU

Epoch 1/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1064s 5s/step - auc: 0.6946 - loss: 0.6204 - val_auc: 0.8389 - val_loss: 0.5910
Epoch 2/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1000s 5s/step - auc: 0.7833 - loss: 0.5530 - val_auc: 0.8516 - val_loss: 0.4518
Epoch 3/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1058s 5s/step - auc: 0.8202 - loss: 0.5110 - val_auc: 0.8556 - val_loss: 0.4836
Epoch 4/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1009s 5s/step - auc: 0.8073 - loss: 0.5271 - val_auc: 0.8588 - val_loss: 0.4980
Epoch 5/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 998s 5s/step - auc: 0.8229 - loss: 0.5080 - val_auc: 0.8588 - val_loss: 0.4874
Epoch 6/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1019s 5s/step - auc: 0.8257 - loss: 0.4978 - val_auc: 0.8622 - val_loss: 0.4447
Epoch 7/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 997s 5s/step - auc: 0.8168 - loss: 0.4850 - val_auc: 0.8633 - val_loss: 0.4358
Epoch 8/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1029s 5s/step - auc: 0.8371 - loss: 0.4841 - val_auc: 0.8636 - val_loss: 0.3726
Epoch 9/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 984s 5s/step - auc: 0.8436 - loss: 0.4754 - val_auc: 0.8697 - val_loss: 0.4274
Epoch 10/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1060s 5s/step - auc: 0.8437 - loss: 0.4759 - val_auc: 0.8697 - val_loss: 0.4406
Epoch 11/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1023s 5s/step - auc: 0.8426 - loss: 0.4615 - val_auc: 0.8713 - val_loss: 0.5035
Epoch 12/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1007s 5s/step - auc: 0.8589 - loss: 0.4621 - val_auc: 0.8717 - val_loss: 0.4750
Epoch 13/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1009s 5s/step - auc: 0.8515 - loss: 0.4630 - val_auc: 0.8719 - val_loss: 0.4704
Epoch 14/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1022s 5s/step - auc: 0.8545 - loss: 0.4663 - val_auc: 0.8719 - val_loss: 0.4584
Epoch 15/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1001s 5s/step - auc: 0.8601 - loss: 0.4577 - val_auc: 0.8719 - val_loss: 0.4538
Epoch 16/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1001s 5s/step - auc: 0.8546 - loss: 0.4601 - val_auc: 0.8707 - val_loss: 0.4746
Epoch 17/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1040s 5s/step - auc: 0.8242 - loss: 0.4992 - val_auc: 0.8727 - val_loss: 0.3859
Epoch 18/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 983s 5s/step - auc: 0.8551 - loss: 0.4630 - val_auc: 0.8703 - val_loss: 0.3798
Epoch 19/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1054s 5s/step - auc: 0.8540 - loss: 0.4598 - val_auc: 0.8696 - val_loss: 0.3800
Epoch 20/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 1001s 5s/step - auc: 0.8584 - loss: 0.4536 - val_auc: 0.8715 - val_loss: 0.4111



conditional hvflip, zoom, brightness on 512x512 images resized to 128x128 images with 256 batch size with 2e-3 learning rate

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 184s 1s/step - auc: 0.6487 - loss: 0.6368 - val_auc: 0.8456 - val_loss: 0.5196
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 422ms/step - auc: 0.8036 - loss: 0.5295 - val_auc: 0.8596 - val_loss: 0.4991
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 411ms/step - auc: 0.8214 - loss: 0.5088 - val_auc: 0.8639 - val_loss: 0.4653
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 429ms/step - auc: 0.8340 - loss: 0.5039 - val_auc: 0.8636 - val_loss: 0.4889
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 414ms/step - auc: 0.8203 - loss: 0.5221 - val_auc: 0.8641 - val_loss: 0.4841
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 427ms/step - auc: 0.8398 - loss: 0.4856 - val_auc: 0.8665 - val_loss: 0.5098
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 426ms/step - auc: 0.8401 - loss: 0.4982 - val_auc: 0.8644 - val_loss: 0.4838
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 421ms/step - auc: 0.8457 - loss: 0.4872 - val_auc: 0.8658 - val_loss: 0.3265
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 426ms/step - auc: 0.8448 - loss: 0.4761 - val_auc: 0.8699 - val_loss: 0.4454
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 414ms/step - auc: 0.8526 - loss: 0.4691 - val_auc: 0.8705 - val_loss: 0.4479
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 427ms/step - auc: 0.8573 - loss: 0.4602 - val_auc: 0.8701 - val_loss: 0.4330
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 422ms/step - auc: 0.8660 - loss: 0.4538 - val_auc: 0.8709 - val_loss: 0.4631
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 431ms/step - auc: 0.8605 - loss: 0.4527 - val_auc: 0.8709 - val_loss: 0.4704
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 425ms/step - auc: 0.8505 - loss: 0.4692 - val_auc: 0.8710 - val_loss: 0.4436
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 414ms/step - auc: 0.8611 - loss: 0.4444 - val_auc: 0.8712 - val_loss: 0.4380
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 426ms/step - auc: 0.8655 - loss: 0.4442 - val_auc: 0.8709 - val_loss: 0.4221
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 427ms/step - auc: 0.8445 - loss: 0.4968 - val_auc: 0.8712 - val_loss: 0.3747
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 412ms/step - auc: 0.8492 - loss: 0.4882 - val_auc: 0.8733 - val_loss: 0.3701
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 49s 414ms/step - auc: 0.8677 - loss: 0.4409 - val_auc: 0.8727 - val_loss: 0.4737
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 48s 409ms/step - auc: 0.8622 - loss: 0.4515 - val_auc: 0.8754 - val_loss: 0.4552


same as above again

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 186s 1s/step - auc: 0.6821 - loss: 0.6279 - val_auc: 0.8470 - val_loss: 0.5118
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 449ms/step - auc: 0.8004 - loss: 0.5345 - val_auc: 0.8575 - val_loss: 0.5165
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 436ms/step - auc: 0.8283 - loss: 0.4896 - val_auc: 0.8617 - val_loss: 0.4489
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 432ms/step - auc: 0.8150 - loss: 0.5188 - val_auc: 0.8624 - val_loss: 0.4903
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 443ms/step - auc: 0.8347 - loss: 0.4941 - val_auc: 0.8626 - val_loss: 0.4816
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 433ms/step - auc: 0.8286 - loss: 0.5123 - val_auc: 0.8641 - val_loss: 0.4307
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 440ms/step - auc: 0.8511 - loss: 0.4752 - val_auc: 0.8695 - val_loss: 0.4172
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 431ms/step - auc: 0.8450 - loss: 0.4786 - val_auc: 0.8716 - val_loss: 0.4642
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 438ms/step - auc: 0.8424 - loss: 0.4885 - val_auc: 0.8699 - val_loss: 0.4009
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 433ms/step - auc: 0.8431 - loss: 0.4747 - val_auc: 0.8689 - val_loss: 0.4369
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 431ms/step - auc: 0.8476 - loss: 0.4670 - val_auc: 0.8700 - val_loss: 0.4720
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 433ms/step - auc: 0.8532 - loss: 0.4767 - val_auc: 0.8697 - val_loss: 0.4905
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 429ms/step - auc: 0.8452 - loss: 0.4791 - val_auc: 0.8696 - val_loss: 0.4628
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 437ms/step - auc: 0.8521 - loss: 0.4676 - val_auc: 0.8698 - val_loss: 0.4401
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 449ms/step - auc: 0.8482 - loss: 0.4751 - val_auc: 0.8699 - val_loss: 0.4380
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 438ms/step - auc: 0.8590 - loss: 0.4557 - val_auc: 0.8667 - val_loss: 0.3854
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 441ms/step - auc: 0.8469 - loss: 0.4766 - val_auc: 0.8684 - val_loss: 0.4443
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 439ms/step - auc: 0.8559 - loss: 0.4690 - val_auc: 0.8716 - val_loss: 0.3888
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 432ms/step - auc: 0.8630 - loss: 0.4455 - val_auc: 0.8711 - val_loss: 0.4012
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 50s 429ms/step - auc: 0.8559 - loss: 0.4616 - val_auc: 0.8734 - val_loss: 0.4067




conditional hvflip, zoom, brightness on 128x128 images resized to 128x128 images with 256 batch size with 2e-3 learning rate

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 144s 851ms/step - auc: 0.6643 - loss: 0.6465 - val_auc: 0.8462 - val_loss: 0.5622
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 37s 318ms/step - auc: 0.7995 - loss: 0.5466 - val_auc: 0.8588 - val_loss: 0.4369
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 310ms/step - auc: 0.8197 - loss: 0.5049 - val_auc: 0.8633 - val_loss: 0.4813
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 311ms/step - auc: 0.8366 - loss: 0.4941 - val_auc: 0.8631 - val_loss: 0.4805
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 307ms/step - auc: 0.8382 - loss: 0.4934 - val_auc: 0.8633 - val_loss: 0.4846
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 312ms/step - auc: 0.8265 - loss: 0.4997 - val_auc: 0.8662 - val_loss: 0.4217
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 306ms/step - auc: 0.8263 - loss: 0.5030 - val_auc: 0.8716 - val_loss: 0.4543
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 317ms/step - auc: 0.8433 - loss: 0.4788 - val_auc: 0.8716 - val_loss: 0.4101
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 39s 340ms/step - auc: 0.8502 - loss: 0.4661 - val_auc: 0.8686 - val_loss: 0.4509
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 34s 302ms/step - auc: 0.8521 - loss: 0.4766 - val_auc: 0.8722 - val_loss: 0.4005
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 305ms/step - auc: 0.8556 - loss: 0.4655 - val_auc: 0.8709 - val_loss: 0.4194
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 305ms/step - auc: 0.8534 - loss: 0.4635 - val_auc: 0.8725 - val_loss: 0.4555
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 44s 395ms/step - auc: 0.8583 - loss: 0.4542 - val_auc: 0.8721 - val_loss: 0.4802
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 42s 367ms/step - auc: 0.8598 - loss: 0.4548 - val_auc: 0.8721 - val_loss: 0.4605
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 46s 395ms/step - auc: 0.8685 - loss: 0.4400 - val_auc: 0.8721 - val_loss: 0.4516
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 305ms/step - auc: 0.8578 - loss: 0.4628 - val_auc: 0.8670 - val_loss: 0.4252
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 301ms/step - auc: 0.8699 - loss: 0.4471 - val_auc: 0.8746 - val_loss: 0.5404
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 304ms/step - auc: 0.8519 - loss: 0.4707 - val_auc: 0.8713 - val_loss: 0.3192
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 316ms/step - auc: 0.8551 - loss: 0.4680 - val_auc: 0.8661 - val_loss: 0.3484
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 307ms/step - auc: 0.8577 - loss: 0.4616 - val_auc: 0.8649 - val_loss: 0.3657


same as above with parallel loading

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 133s 799ms/step - auc: 0.6720 - loss: 0.6229 - val_auc: 0.8245 - val_loss: 0.4761
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 311ms/step - auc: 0.8086 - loss: 0.5185 - val_auc: 0.8477 - val_loss: 0.5440
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 37s 316ms/step - auc: 0.8083 - loss: 0.5301 - val_auc: 0.8527 - val_loss: 0.4744
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 317ms/step - auc: 0.8415 - loss: 0.4888 - val_auc: 0.8530 - val_loss: 0.5154
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 315ms/step - auc: 0.8347 - loss: 0.4899 - val_auc: 0.8532 - val_loss: 0.4900
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 308ms/step - auc: 0.8353 - loss: 0.4947 - val_auc: 0.8580 - val_loss: 0.3815
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 311ms/step - auc: 0.8283 - loss: 0.5029 - val_auc: 0.8632 - val_loss: 0.5132
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 318ms/step - auc: 0.8392 - loss: 0.4853 - val_auc: 0.8633 - val_loss: 0.4844
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 313ms/step - auc: 0.8528 - loss: 0.4694 - val_auc: 0.8685 - val_loss: 0.4517
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 313ms/step - auc: 0.8432 - loss: 0.4875 - val_auc: 0.8671 - val_loss: 0.4791
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 313ms/step - auc: 0.8610 - loss: 0.4608 - val_auc: 0.8672 - val_loss: 0.4743
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 37s 320ms/step - auc: 0.8528 - loss: 0.4797 - val_auc: 0.8664 - val_loss: 0.4744
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 315ms/step - auc: 0.8590 - loss: 0.4507 - val_auc: 0.8677 - val_loss: 0.4825
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 304ms/step - auc: 0.8591 - loss: 0.4614 - val_auc: 0.8674 - val_loss: 0.4589
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 313ms/step - auc: 0.8529 - loss: 0.4673 - val_auc: 0.8675 - val_loss: 0.4573
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 318ms/step - auc: 0.8582 - loss: 0.4640 - val_auc: 0.8681 - val_loss: 0.3490
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 303ms/step - auc: 0.8483 - loss: 0.4714 - val_auc: 0.8672 - val_loss: 0.4965
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 35s 304ms/step - auc: 0.8638 - loss: 0.4551 - val_auc: 0.8664 - val_loss: 0.4498
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 36s 317ms/step - auc: 0.8528 - loss: 0.4628 - val_auc: 0.8683 - val_loss: 0.4186
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 37s 328ms/step - auc: 0.8484 - loss: 0.4674 - val_auc: 0.8712 - val_loss: 0.4515



256 im size -> 128 resize, with 1e-3 LR and 128 batch size

Epoch 1/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 171s 614ms/step - auc: 0.7071 - loss: 0.6086 - val_auc: 0.8394 - val_loss: 0.5356
Epoch 2/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 41s 187ms/step - auc: 0.7921 - loss: 0.5274 - val_auc: 0.8491 - val_loss: 0.4546
Epoch 3/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 181ms/step - auc: 0.8033 - loss: 0.5177 - val_auc: 0.8547 - val_loss: 0.4993
Epoch 4/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 181ms/step - auc: 0.8300 - loss: 0.5001 - val_auc: 0.8555 - val_loss: 0.5041
Epoch 5/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 182ms/step - auc: 0.8290 - loss: 0.4977 - val_auc: 0.8553 - val_loss: 0.4741
Epoch 6/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 184ms/step - auc: 0.8335 - loss: 0.4850 - val_auc: 0.8602 - val_loss: 0.4323
Epoch 7/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 181ms/step - auc: 0.8284 - loss: 0.4825 - val_auc: 0.8606 - val_loss: 0.4651
Epoch 8/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 182ms/step - auc: 0.8405 - loss: 0.4826 - val_auc: 0.8610 - val_loss: 0.4767
Epoch 9/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 41s 186ms/step - auc: 0.8453 - loss: 0.4807 - val_auc: 0.8616 - val_loss: 0.3702
Epoch 10/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 43s 185ms/step - auc: 0.8547 - loss: 0.4706 - val_auc: 0.8619 - val_loss: 0.4018
Epoch 11/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 181ms/step - auc: 0.8389 - loss: 0.4728 - val_auc: 0.8639 - val_loss: 0.4610
Epoch 12/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 181ms/step - auc: 0.8520 - loss: 0.4640 - val_auc: 0.8652 - val_loss: 0.4747
Epoch 13/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 41s 189ms/step - auc: 0.8496 - loss: 0.4697 - val_auc: 0.8655 - val_loss: 0.4654
Epoch 14/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 39s 179ms/step - auc: 0.8383 - loss: 0.4744 - val_auc: 0.8654 - val_loss: 0.4495
Epoch 15/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 183ms/step - auc: 0.8504 - loss: 0.4556 - val_auc: 0.8655 - val_loss: 0.4469
Epoch 16/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 39s 180ms/step - auc: 0.8534 - loss: 0.4656 - val_auc: 0.8644 - val_loss: 0.4235
Epoch 17/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 42s 193ms/step - auc: 0.8524 - loss: 0.4458 - val_auc: 0.8647 - val_loss: 0.4654
Epoch 18/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 58s 266ms/step - auc: 0.8529 - loss: 0.4494 - val_auc: 0.8673 - val_loss: 0.3859
Epoch 19/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 48s 210ms/step - auc: 0.8441 - loss: 0.4730 - val_auc: 0.8676 - val_loss: 0.3958
Epoch 20/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 40s 184ms/step - auc: 0.8460 - loss: 0.4684 - val_auc: 0.8680 - val_loss: 0.3813



256 im size -> 224 resize, with 1e-3 LR and 128 batch size


Epoch 1/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 187s 669ms/step - auc: 0.6229 - loss: 0.6494 - val_auc: 0.8291 - val_loss: 0.5450
Epoch 2/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 394ms/step - auc: 0.7942 - loss: 0.5478 - val_auc: 0.8447 - val_loss: 0.4660
Epoch 3/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 395ms/step - auc: 0.8162 - loss: 0.5078 - val_auc: 0.8492 - val_loss: 0.4767
Epoch 4/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 85s 394ms/step - auc: 0.8313 - loss: 0.4984 - val_auc: 0.8502 - val_loss: 0.5031
Epoch 5/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 85s 392ms/step - auc: 0.8377 - loss: 0.4916 - val_auc: 0.8502 - val_loss: 0.4753
Epoch 6/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 85s 393ms/step - auc: 0.8131 - loss: 0.5028 - val_auc: 0.8563 - val_loss: 0.4300
Epoch 7/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 398ms/step - auc: 0.8252 - loss: 0.5057 - val_auc: 0.8586 - val_loss: 0.4027
Epoch 8/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 398ms/step - auc: 0.8457 - loss: 0.4800 - val_auc: 0.8592 - val_loss: 0.4128
Epoch 9/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 398ms/step - auc: 0.8546 - loss: 0.4564 - val_auc: 0.8596 - val_loss: 0.4704
Epoch 10/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 401ms/step - auc: 0.8522 - loss: 0.4623 - val_auc: 0.8635 - val_loss: 0.4380
Epoch 11/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 87s 401ms/step - auc: 0.8552 - loss: 0.4640 - val_auc: 0.8640 - val_loss: 0.4486
Epoch 12/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 85s 395ms/step - auc: 0.8533 - loss: 0.4612 - val_auc: 0.8647 - val_loss: 0.4887
Epoch 13/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 399ms/step - auc: 0.8511 - loss: 0.4523 - val_auc: 0.8644 - val_loss: 0.4686
Epoch 14/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 399ms/step - auc: 0.8631 - loss: 0.4521 - val_auc: 0.8643 - val_loss: 0.4567
Epoch 15/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 85s 395ms/step - auc: 0.8637 - loss: 0.4538 - val_auc: 0.8643 - val_loss: 0.4527
Epoch 16/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 396ms/step - auc: 0.8512 - loss: 0.4628 - val_auc: 0.8612 - val_loss: 0.4275
Epoch 17/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 87s 403ms/step - auc: 0.8591 - loss: 0.4573 - val_auc: 0.8660 - val_loss: 0.4427
Epoch 18/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 397ms/step - auc: 0.8600 - loss: 0.4578 - val_auc: 0.8657 - val_loss: 0.3732
Epoch 19/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 87s 402ms/step - auc: 0.8551 - loss: 0.4513 - val_auc: 0.8675 - val_loss: 0.3805
Epoch 20/20
208/208 ━━━━━━━━━━━━━━━━━━━━ 86s 401ms/step - auc: 0.8694 - loss: 0.4458 - val_auc: 0.8647 - val_loss: 0.3911



BFCE with gamma=1.0, start LR = 2e-3

Epoch 1/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 203s 1s/step - auc: 0.6484 - loss: 0.3384 - recall: 0.5367 - val_auc: 0.8448 - val_loss: 0.1644 - val_recall: 0.5932
Epoch 2/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 53s 451ms/step - auc: 0.8024 - loss: 0.2652 - recall: 0.7113 - val_auc: 0.8561 - val_loss: 0.2535 - val_recall: 0.8220
Epoch 3/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 54s 460ms/step - auc: 0.8170 - loss: 0.2608 - recall: 0.7208 - val_auc: 0.8611 - val_loss: 0.2471 - val_recall: 0.8220
Epoch 4/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 441ms/step - auc: 0.8363 - loss: 0.2451 - recall: 0.7590 - val_auc: 0.8639 - val_loss: 0.2354 - val_recall: 0.7881
Epoch 5/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 442ms/step - auc: 0.8459 - loss: 0.2454 - recall: 0.8020 - val_auc: 0.8635 - val_loss: 0.2285 - val_recall: 0.7881
Epoch 6/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 439ms/step - auc: 0.8327 - loss: 0.2464 - recall: 0.7298 - val_auc: 0.8609 - val_loss: 0.1658 - val_recall: 0.6864
Epoch 7/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 54s 468ms/step - auc: 0.8495 - loss: 0.2350 - recall: 0.7678 - val_auc: 0.8619 - val_loss: 0.2424 - val_recall: 0.8051
Epoch 8/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 54s 467ms/step - auc: 0.8355 - loss: 0.2503 - recall: 0.7422 - val_auc: 0.8644 - val_loss: 0.2174 - val_recall: 0.7797
Epoch 9/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 53s 453ms/step - auc: 0.8556 - loss: 0.2357 - recall: 0.7881 - val_auc: 0.8632 - val_loss: 0.2505 - val_recall: 0.8305
Epoch 10/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 445ms/step - auc: 0.8555 - loss: 0.2330 - recall: 0.7990 - val_auc: 0.8741 - val_loss: 0.2359 - val_recall: 0.8305
Epoch 11/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 435ms/step - auc: 0.8597 - loss: 0.2342 - recall: 0.7680 - val_auc: 0.8746 - val_loss: 0.2316 - val_recall: 0.8051
Epoch 12/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 432ms/step - auc: 0.8667 - loss: 0.2268 - recall: 0.7780 - val_auc: 0.8764 - val_loss: 0.2068 - val_recall: 0.7881
Epoch 13/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 437ms/step - auc: 0.8770 - loss: 0.2196 - recall: 0.8184 - val_auc: 0.8765 - val_loss: 0.2152 - val_recall: 0.7966
Epoch 14/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 450ms/step - auc: 0.8599 - loss: 0.2323 - recall: 0.7870 - val_auc: 0.8765 - val_loss: 0.2209 - val_recall: 0.8136
Epoch 15/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 54s 464ms/step - auc: 0.8593 - loss: 0.2335 - recall: 0.7948 - val_auc: 0.8766 - val_loss: 0.2227 - val_recall: 0.8136
Epoch 16/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 439ms/step - auc: 0.8338 - loss: 0.2429 - recall: 0.7582 - val_auc: 0.8790 - val_loss: 0.1998 - val_recall: 0.7966
Epoch 17/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 447ms/step - auc: 0.8581 - loss: 0.2280 - recall: 0.7895 - val_auc: 0.8750 - val_loss: 0.2114 - val_recall: 0.7966
Epoch 18/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 52s 448ms/step - auc: 0.8541 - loss: 0.2397 - recall: 0.8130 - val_auc: 0.8662 - val_loss: 0.2371 - val_recall: 0.8136
Epoch 19/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 432ms/step - auc: 0.8481 - loss: 0.2391 - recall: 0.7710 - val_auc: 0.8686 - val_loss: 0.2239 - val_recall: 0.7966
Epoch 20/20
104/104 ━━━━━━━━━━━━━━━━━━━━ 51s 431ms/step - auc: 0.8616 - loss: 0.2253 - recall: 0.7732 - val_auc: 0.8757 - val_loss: 0.2216 - val_recall: 0.8220


Image + tabular model - concatenate before dropout

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.747239	0.301899	0.036603	0.663090	0.853451	0.265591	0.052067		0.822034
1	0.826349	0.258185	0.048969	0.748927	0.866693	0.175795	0.078125		0.677966
2	0.823671	0.260655	0.049678	0.744635	0.874862	0.257943	0.057571		0.847458
3	0.849761	0.243283	0.054363	0.766094	0.876901	0.231347	0.061671		0.788136
4	0.849320	0.242788	0.055853	0.761803	0.877371	0.219476	0.067003		0.788136
5	0.840105	0.250049	0.051100	0.757511	0.877494	0.335253	0.044758		0.915254
6	0.838506	0.251152	0.051670	0.770386	0.876938	0.178760	0.085189		0.745763
7	0.849765	0.241889	0.053933	0.748927	0.871399	0.230582	0.063732		0.822034
8	0.862795	0.232336	0.057766	0.778970	0.873680	0.160897	0.087973		0.669492
9	0.857649	0.237696	0.057908	0.798283	0.873545	0.256342	0.057292		0.838983


Image + tabular model - batchnorm after image, concatenate before dropout

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.710663	0.454379	0.031542	0.643777	0.814260	0.160241	0.091525		0.457627
1	0.787757	0.350359	0.041946	0.697425	0.847563	0.195894	0.065443		0.694915
2	0.813923	0.303099	0.047424	0.744635	0.864152	0.186974	0.071778		0.745763
3	0.826817	0.291715	0.051286	0.766094	0.871535	0.189923	0.072020		0.737288
4	0.840975	0.267355	0.054335	0.763949	0.872870	0.200781	0.072998		0.788136
5	0.824820	0.298856	0.050726	0.757511	0.845612	0.280786	0.053241		0.779661
6	0.829982	0.289678	0.050757	0.755365	0.840595	0.299545	0.050505		0.762712
7	0.845036	0.273628	0.054005	0.768240	0.859556	0.231588	0.066470		0.762712
8	0.838169	0.279787	0.053846	0.766094	0.857804	0.247839	0.060732		0.745763
9	0.843753	0.268427	0.051923	0.761803	0.878809	0.227556	0.069767		0.838983


Image + tabular model - batchnorm after image, concatenate after dropout

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.712089	0.444161	0.032651	0.678112	0.833374	0.140377	0.099593		0.415254
1	0.794279	0.337128	0.044705	0.721030	0.853547	0.197473	0.064360		0.677966
2	0.807739	0.314566	0.045796	0.744635	0.867802	0.198254	0.067315		0.703390
3	0.836564	0.275778	0.051494	0.761803	0.877215	0.192902	0.070957		0.728814
4	0.822433	0.292250	0.051324	0.748927	0.879566	0.209068	0.070518		0.796610
5	0.807789	0.314600	0.046305	0.727468	0.859715	0.315460	0.054933		0.872881
6	0.819325	0.300469	0.046512	0.738197	0.853305	0.237421	0.064828		0.796610
7	0.830601	0.289354	0.050658	0.759657	0.870972	0.244876	0.066576		0.830508
8	0.843237	0.267351	0.052632	0.766094	0.877986	0.232629	0.068843		0.822034
9	0.845955	0.267935	0.054253	0.759657	0.873457	0.208893	0.072568		0.771186



Image + tabular model with additional dense(32) layer
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.727580	0.429228	0.034028	0.703863	0.829059	0.202859	0.061685		0.601695
1	0.827311	0.275014	0.047414	0.781116	0.844682	0.190405	0.072529		0.677966
2	0.854223	0.239407	0.050969	0.796137	0.862649	0.228301	0.058357		0.872881
3	0.876469	0.225665	0.055618	0.843348	0.871493	0.179914	0.074853		0.754237
4	0.881354	0.217590	0.059126	0.839056	0.871459	0.176038	0.075207		0.771186
5	0.906044	0.194592	0.068403	0.873391	0.877113	0.192510	0.071975		0.796610
6	0.909833	0.190861	0.070604	0.879828	0.874408	0.182010	0.072968		0.745763
7	0.910858	0.188510	0.070492	0.864807	0.877534	0.185243	0.076480		0.788136
8	0.916857	0.183946	0.071316	0.875537	0.878160	0.178930	0.077645		0.771186
9	0.922464	0.177563	0.073015	0.881974	0.878020	0.177652	0.077187		0.762712


Image + tabular model with additional dense(64)+batchnorm layer
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.727318	0.411975	0.033211	0.740343	0.837663	0.174136	0.084615		0.559322
1	0.835796	0.255477	0.047486	0.802575	0.872012	0.195075	0.078901		0.754237
2	0.869116	0.225893	0.055429	0.834764	0.869100	0.204929	0.071799		0.822034
3	0.881753	0.216516	0.059817	0.843348	0.869578	0.187227	0.079435		0.762712
4	0.896247	0.204314	0.064809	0.858369	0.869212	0.175313	0.083029		0.771186
5	0.903415	0.198107	0.071104	0.839056	0.877330	0.187319	0.077430		0.796610
6	0.907038	0.194041	0.070239	0.832618	0.875775	0.185547	0.075758		0.762712
7	0.926051	0.176404	0.077677	0.884120	0.876845	0.177482	0.080717		0.762712
8	0.928241	0.173410	0.079384	0.873391	0.877134	0.177323	0.082353		0.771186
9	0.927760	0.174300	0.080305	0.881974	0.876743	0.177675	0.082652		0.771186


Image + tabular model using Convnexttiny with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.738875	0.328061	0.032748	0.768240	0.781421	0.185613	0.059524		0.508475
1	0.838089	0.250887	0.046549	0.800429	0.832211	0.211641	0.059413		0.703390
2	0.860552	0.232833	0.050950	0.793991	0.854418	0.222913	0.060664		0.805085
3	0.884240	0.214796	0.059274	0.847640	0.856867	0.204866	0.064677		0.771186
4	0.896566	0.203038	0.063223	0.856223	0.859649	0.197019	0.065918		0.745763
5	0.904613	0.196443	0.067645	0.860515	0.866402	0.194490	0.066982		0.720339
6	0.923590	0.178342	0.077349	0.899142	0.866197	0.186563	0.069275		0.720339
7	0.924516	0.177302	0.077790	0.888412	0.866506	0.181377	0.071181		0.694915
8	0.934593	0.166257	0.083011	0.920601	0.866508	0.179115	0.070609		0.677966
9	0.936880	0.164511	0.082755	0.920601	0.866531	0.181260	0.070496		0.686441


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.739537	0.335821	0.033422	0.755365	0.834421	0.177406	0.083655		0.550847
1	0.841872	0.247703	0.049165	0.789700	0.843131	0.183238	0.075543		0.677966
2	0.849015	0.244201	0.053929	0.798283	0.867039	0.198550	0.070669		0.788136
3	0.882410	0.218603	0.061529	0.841202	0.877820	0.165199	0.080203		0.669492
4	0.891029	0.208976	0.061924	0.821888	0.884643	0.191764	0.075918		0.788136
5	0.903079	0.198540	0.067069	0.858369	0.883058	0.178495	0.077739		0.745763
6	0.906770	0.194995	0.070410	0.858369	0.883562	0.166370	0.085127		0.737288
7	0.910018	0.192692	0.071801	0.856223	0.882422	0.169150	0.082772		0.728814
8	0.925678	0.177315	0.077440	0.890558	0.881585	0.172789	0.078269		0.720339
9	0.921616	0.180480	0.076706	0.873391	0.881383	0.176003	0.078876		0.737288


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm 20epochs
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.749452	0.328545	0.035155	0.751073	0.833574	0.198402	0.078231		0.584746
1	0.829623	0.257921	0.048470	0.768240	0.860957	0.178539	0.083805		0.627119
2	0.867337	0.230309	0.056760	0.809013	0.862802	0.185025	0.078846		0.694915
3	0.879351	0.220330	0.061748	0.826180	0.875404	0.203661	0.073810		0.788136
4	0.886426	0.214016	0.063310	0.817597	0.865528	0.189342	0.073975		0.703390
5	0.907283	0.195176	0.072566	0.879828	0.867222	0.163788	0.082912		0.694915
6	0.908510	0.192428	0.073928	0.843348	0.874330	0.186920	0.076790		0.754237
7	0.908373	0.192576	0.075608	0.854077	0.870529	0.223607	0.061892		0.754237
8	0.928257	0.171749	0.080560	0.888412	0.871014	0.162979	0.082661		0.694915
9	0.925429	0.174833	0.083662	0.864807	0.867999	0.158791	0.088486		0.703390
10	0.929189	0.170862	0.088254	0.875537	0.866107	0.166246	0.083008		0.720339
11	0.934294	0.165790	0.089716	0.894850	0.873791	0.146897	0.091116		0.677966
12	0.934795	0.165001	0.092917	0.875537	0.873487	0.151603	0.085805		0.686441
13	0.938579	0.159562	0.091735	0.888412	0.872016	0.140812	0.093863		0.661017
14	0.941164	0.155985	0.093842	0.892704	0.876088	0.143219	0.092857		0.661017
15	0.950369	0.145090	0.101094	0.912017	0.872728	0.143402	0.093381		0.669492
16	0.945732	0.151474	0.101897	0.899142	0.871373	0.145860	0.091224		0.669492
17	0.943966	0.152604	0.099880	0.890558	0.872101	0.144399	0.091549		0.661017
18	0.951547	0.143954	0.102817	0.916309	0.872128	0.144398	0.091228		0.661017
19	0.951739	0.143025	0.103883	0.918455	0.871892	0.145446	0.090803		0.661017


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm cosine_decay_restarts 20epochs
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.732306	0.347797	0.033037	0.733906	0.794159	0.169527	0.068256		0.533898
1	0.834590	0.251499	0.045710	0.785408	0.850143	0.218173	0.062054		0.737288
2	0.856309	0.237276	0.052393	0.817597	0.860196	0.199043	0.066038		0.711864
3	0.883159	0.218555	0.059909	0.849785	0.867793	0.197883	0.071770		0.762712
4	0.891039	0.210981	0.060709	0.845494	0.867879	0.207243	0.067159		0.771186
5	0.864645	0.233352	0.058714	0.813305	0.853949	0.250885	0.056213		0.805085
6	0.883568	0.215352	0.061120	0.843348	0.860864	0.222580	0.062971		0.779661
7	0.888218	0.210142	0.063062	0.828326	0.877603	0.213737	0.070545		0.822034
8	0.906694	0.193540	0.068574	0.869099	0.880582	0.189728	0.075243		0.788136
9	0.914019	0.186191	0.072433	0.873391	0.876490	0.174360	0.079929		0.762712
10	0.923001	0.178126	0.077668	0.877682	0.878537	0.178798	0.081365		0.788136
11	0.923587	0.177672	0.081070	0.871245	0.876521	0.169507	0.082863		0.745763
12	0.933647	0.165279	0.083084	0.892704	0.874991	0.168520	0.080229		0.711864
13	0.936928	0.162773	0.084821	0.907725	0.875398	0.166167	0.081633		0.711864
14	0.938553	0.161042	0.087651	0.918455	0.875474	0.167964	0.080769		0.711864
15	0.910782	0.191505	0.078137	0.849785	0.857080	0.226948	0.063933		0.779661
16	0.905964	0.194025	0.071734	0.862661	0.852186	0.217756	0.064906		0.728814
17	0.914418	0.185964	0.074981	0.860515	0.859287	0.179018	0.077261		0.745763
18	0.911767	0.188509	0.075540	0.847640	0.868647	0.199113	0.068095		0.754237
19	0.921855	0.177959	0.077713	0.860515	0.857842	0.180701	0.076923		0.711864



with 256x256 resized images
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.747419	0.326396	0.036463	0.701717	0.846849	0.231382	0.063444		0.711864
1	0.836752	0.253954	0.051162	0.798283	0.851210	0.230143	0.060797		0.737288
2	0.858665	0.236325	0.054564	0.787554	0.864220	0.240560	0.057416		0.813559
3	0.878099	0.220079	0.060239	0.821888	0.877492	0.191836	0.072848		0.745763
4	0.886156	0.212726	0.063174	0.817597	0.871360	0.204293	0.066912		0.771186
5	0.903848	0.197598	0.069055	0.851931	0.886850	0.187075	0.078008		0.796610
6	0.916598	0.185380	0.077681	0.879828	0.879883	0.179236	0.080762		0.754237
7	0.920823	0.181809	0.080428	0.871245	0.879457	0.173687	0.079737		0.720339
8	0.922475	0.178940	0.081314	0.871245	0.879336	0.176426	0.077899		0.728814
9	0.927139	0.174477	0.080572	0.881974	0.879021	0.178296	0.076649		0.728814



Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm finetune with 64 batch size

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.754251	0.312937	0.035091	0.751073	0.861417	0.232315	0.059343		0.796610
1	0.835903	0.251321	0.048806	0.789700	0.870475	0.225576	0.061224		0.788136
2	0.860274	0.234583	0.054171	0.778970	0.868440	0.219889	0.062026		0.762712
3	0.883218	0.215873	0.061419	0.815451	0.873688	0.218033	0.064120		0.796610
4	0.892777	0.207348	0.066311	0.854077	0.873645	0.225841	0.062992		0.813559
5	0.902849	0.198973	0.068194	0.843348	0.875085	0.198686	0.069859		0.754237
6	0.909809	0.190634	0.071633	0.860515	0.876950	0.198373	0.071047		0.788136
7	0.919819	0.182135	0.077331	0.875537	0.876998	0.191955	0.074196		0.762712
8	0.921769	0.179201	0.080039	0.877682	0.876562	0.187254	0.075745		0.754237
9	0.926708	0.174456	0.081097	0.888412	0.876131	0.187438	0.075939		0.754237

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.880855	0.217782	0.062122	0.793991	0.872893	0.203335	0.065265		0.762712
1	0.899489	0.200690	0.066689	0.864807	0.882338	0.217627	0.068056		0.830508
2	0.913671	0.187781	0.071797	0.866953	0.858795	0.211506	0.065598		0.762712
3	0.934696	0.165542	0.080651	0.892704	0.871614	0.174518	0.079777		0.728814
4	0.942817	0.156044	0.089651	0.903434	0.871159	0.155393	0.089958		0.728814


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm finetune with 256 batch size

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.732085	0.346150	0.033799	0.731760	0.834497	0.233410	0.054820		0.737288
1	0.833523	0.253622	0.048396	0.789700	0.848678	0.198135	0.061588		0.644068
2	0.862237	0.233260	0.054078	0.798283	0.846724	0.183116	0.069973		0.669492
3	0.880978	0.217222	0.060761	0.815451	0.854124	0.187928	0.072300		0.703390
4	0.891616	0.208489	0.066145	0.834764	0.857645	0.186636	0.071924		0.703390
5	0.899201	0.202050	0.066838	0.839056	0.863173	0.182506	0.072680		0.703390
6	0.909206	0.193386	0.073623	0.843348	0.861592	0.178357	0.071889		0.661017
7	0.921773	0.182079	0.076449	0.877682	0.862132	0.170947	0.076329		0.669492
8	0.924643	0.177677	0.077477	0.877682	0.860545	0.173568	0.075670		0.669492
9	0.926197	0.176813	0.079655	0.890558	0.860457	0.176769	0.076279		0.694915

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.886577	0.214948	0.068663	0.806867	0.857931	0.210755	0.064444		0.737288
1	0.920658	0.181429	0.074507	0.875537	0.861494	0.181110	0.068284		0.644068
2	0.927132	0.174850	0.080008	0.881974	0.857888	0.167349	0.076476		0.669492
3	0.943453	0.157784	0.087132	0.927039	0.856923	0.173372	0.077277		0.711864
4	0.944362	0.156203	0.092685	0.916309	0.862220	0.150915	0.084071		0.644068


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm finetune with 64 batch size with dropout 0.3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.768221	0.297619	0.038749	0.768240	0.843908	0.290486	0.046685		0.847458
1	0.830589	0.254151	0.047319	0.789700	0.853306	0.230978	0.058259		0.771186
2	0.844365	0.245696	0.051135	0.802575	0.860203	0.226324	0.058710		0.771186
3	0.862008	0.231321	0.052617	0.804721	0.867639	0.210254	0.062987		0.754237
4	0.874739	0.221868	0.058420	0.804721	0.858690	0.223080	0.059607		0.720339
5	0.886282	0.213078	0.060682	0.828326	0.871829	0.188204	0.073883		0.728814
6	0.902344	0.199867	0.069413	0.849785	0.876688	0.188944	0.073024		0.720339
7	0.907493	0.193647	0.071021	0.854077	0.875265	0.187505	0.073276		0.720339
8	0.914393	0.187423	0.074067	0.877682	0.876304	0.183942	0.076450		0.737288
9	0.911238	0.190077	0.072445	0.851931	0.876006	0.184316	0.076585		0.737288


	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.871327	0.225876	0.062554	0.781116	0.862063	0.231866	0.059711		0.771186
1	0.902840	0.198774	0.066510	0.847640	0.867239	0.182873	0.073386		0.703390
2	0.920328	0.181775	0.073050	0.884120	0.867399	0.165448	0.080392		0.694915
3	0.929873	0.171242	0.078773	0.892704	0.870862	0.176119	0.082375		0.728814
4	0.941879	0.158592	0.086680	0.907725	0.870224	0.170887	0.079102		0.686441


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm finetune with 64 batch size with label_smoothing 0.05

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.734993	0.331415	0.033142	0.706009	0.841938	0.268185	0.049139		0.822034
1	0.824071	0.262398	0.047148	0.787554	0.862111	0.223510	0.059063		0.737288
2	0.853037	0.241554	0.053116	0.791846	0.879942	0.222744	0.063136		0.788136
3	0.871683	0.229187	0.057854	0.813305	0.875741	0.226183	0.063898		0.847458
4	0.889228	0.215094	0.063809	0.856223	0.876638	0.205863	0.066874		0.728814
5	0.905231	0.199719	0.069307	0.841202	0.875483	0.183887	0.078040		0.728814
6	0.906411	0.198248	0.072062	0.854077	0.875849	0.183150	0.076854		0.728814
7	0.920093	0.185517	0.078185	0.869099	0.879916	0.187349	0.075983		0.737288
8	0.915695	0.188809	0.076276	0.856223	0.880368	0.188476	0.076790		0.754237
9	0.919311	0.186382	0.079969	0.892704	0.880678	0.185005	0.078691		0.754237

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.874632	0.230745	0.064607	0.785408	0.863148	0.201380	0.070079		0.754237
1	0.917538	0.187955	0.074040	0.864807	0.872235	0.188314	0.077899		0.728814
2	0.917891	0.187304	0.075279	0.854077	0.863677	0.183454	0.075092		0.694915
3	0.927593	0.178274	0.082574	0.875537	0.877644	0.204525	0.067939		0.754237
4	0.948123	0.155689	0.091796	0.929185	0.875704	0.165066	0.086189		0.703390


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm finetune with 64 batch size with label_smoothing 0.1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.744174	0.322000	0.035304	0.718884	0.851954	0.253010	0.059682		0.762712
1	0.828056	0.262073	0.047904	0.755365	0.861611	0.257688	0.056086		0.796610
2	0.852242	0.245549	0.052287	0.787554	0.868310	0.243313	0.060645		0.796610
3	0.861072	0.239434	0.055489	0.798283	0.870396	0.232436	0.064473		0.813559
4	0.885890	0.221357	0.061512	0.834764	0.865819	0.210470	0.069767		0.762712
5	0.896949	0.211436	0.067124	0.839056	0.876656	0.216766	0.069733		0.796610
6	0.911115	0.199342	0.072978	0.851931	0.879549	0.199484	0.078231		0.779661
7	0.916121	0.194414	0.076923	0.862661	0.878846	0.193523	0.080108		0.754237
8	0.924119	0.186484	0.079648	0.873391	0.879180	0.191890	0.079323		0.754237
9	0.927612	0.183744	0.082282	0.881974	0.878971	0.190701	0.079351		0.745763

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.880834	0.226587	0.067827	0.802575	0.859894	0.211873	0.067124		0.745763
1	0.908870	0.202704	0.069341	0.860515	0.872794	0.203752	0.071542		0.762712
2	0.926032	0.185845	0.080959	0.884120	0.868729	0.188647	0.081951		0.711864
3	0.935652	0.175529	0.084281	0.890558	0.865712	0.180507	0.083333		0.720339
4	0.948842	0.160110	0.096137	0.918455	0.864698	0.181437	0.082745		0.694915


Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm finetune with 64 batch size with label_smoothing 0.05, decay for finetuning corrected (based on finetune_epochs)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.735785	0.328375	0.033149	0.718884	0.838809	0.267181	0.044688		0.830508
1	0.833224	0.256345	0.047333	0.776824	0.866512	0.238930	0.063110		0.822034
2	0.852580	0.242573	0.051538	0.798283	0.866929	0.205139	0.067141		0.720339
3	0.871350	0.228743	0.058922	0.828326	0.873732	0.226126	0.066852		0.813559
4	0.887474	0.214761	0.063450	0.824034	0.872934	0.213059	0.067480		0.796610
5	0.892909	0.209408	0.063997	0.836910	0.877241	0.197371	0.077047		0.813559
6	0.902453	0.201652	0.068493	0.847640	0.881910	0.193819	0.079729		0.796610
7	0.919454	0.185716	0.076059	0.886266	0.878377	0.190646	0.080440		0.805085
8	0.925127	0.179882	0.078878	0.899142	0.879933	0.190758	0.080577		0.805085
9	0.927244	0.177527	0.079511	0.892704	0.879416	0.186088	0.083627		0.805085

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.879561	0.226800	0.066620	0.811159	0.868332	0.221308	0.061064		0.788136
1	0.910922	0.194757	0.070761	0.860515	0.865380	0.182536	0.076256		0.745763
2	0.934042	0.172401	0.081806	0.909871	0.877240	0.194414	0.076133		0.754237
3	0.944617	0.160913	0.089580	0.920601	0.879838	0.176126	0.083333		0.737288
4	0.949215	0.155123	0.094771	0.933476	0.879348	0.173568	0.084466		0.737288



Image + tabular model with additional dense(128)+batchnorm+dropout+dense(16)+batchnorm finetune with 64 batch size with label_smoothing 0.05, 
	decay for finetuning corrected (based on finetune_epochs), finetuning with same start lr as initial training

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.749789	0.315899	0.034980	0.723176	0.853420	0.247988	0.058680		0.813559
1	0.829496	0.256804	0.046929	0.757511	0.835633	0.241826	0.052894		0.720339
2	0.855147	0.240568	0.054240	0.811159	0.856045	0.249654	0.055357		0.788136
3	0.870939	0.227470	0.057664	0.834764	0.861538	0.236526	0.057392		0.779661
4	0.889153	0.212827	0.063495	0.830472	0.863100	0.226141	0.061947		0.771186
5	0.891764	0.210050	0.063851	0.836910	0.868012	0.198051	0.071605		0.737288
6	0.909960	0.195851	0.072810	0.856223	0.867942	0.185714	0.075288		0.720339
7	0.917039	0.187586	0.075556	0.866953	0.868402	0.189195	0.076450		0.737288
8	0.921236	0.184096	0.078636	0.871245	0.869398	0.186382	0.076923		0.728814
9	0.923284	0.181425	0.078917	0.869099	0.869790	0.183436	0.077555		0.720339

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.776253	0.312026	0.043940	0.693133	0.853880	0.265882	0.049630		0.796610
1	0.841921	0.252917	0.051927	0.766094	0.869191	0.226522	0.065333		0.830508
2	0.881893	0.221957	0.062351	0.839056	0.884838	0.239997	0.067327		0.864407
3	0.901098	0.205923	0.066542	0.841202	0.891689	0.205351	0.077105		0.830508
4	0.923039	0.186367	0.074986	0.881974	0.894985	0.203912	0.079545		0.830508


start_lr=1e-3 warmup_target=3e-3 alpha=1e-4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.757138	0.314190	0.036386	0.746781	0.805860	0.272871	0.044926		0.720339
1	0.802633	0.275155	0.042190	0.727468	0.854622	0.260967	0.051491		0.805085
2	0.839527	0.251757	0.049369	0.781116	0.860032	0.267591	0.053073		0.805085
3	0.863451	0.233697	0.055923	0.817597	0.871258	0.210192	0.067732		0.754237
4	0.871153	0.227757	0.057384	0.815451	0.876729	0.243336	0.059281		0.838983
5	0.887448	0.215304	0.064952	0.845494	0.877385	0.190793	0.085878		0.762712
6	0.894637	0.208269	0.066479	0.811159	0.880699	0.192661	0.075666		0.745763
7	0.905653	0.198692	0.070507	0.856223	0.883905	0.187201	0.079181		0.754237
8	0.914174	0.191057	0.075561	0.851931	0.882498	0.186658	0.075540		0.711864
9	0.916435	0.188868	0.076411	0.862661	0.884232	0.184398	0.077063		0.720339


start_lr=1e-3 warmup_target=5e-3 alpha=1e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.738895	0.322081	0.033400	0.751073	0.813865	0.292963	0.047212		0.796610
1	0.811658	0.269686	0.045335	0.776824	0.847714	0.247615	0.054176		0.813559
2	0.827163	0.259778	0.047348	0.768240	0.868446	0.232515	0.060399		0.822034
3	0.857876	0.238694	0.052881	0.789700	0.860808	0.256215	0.056265		0.822034
4	0.859661	0.238047	0.053434	0.774678	0.863945	0.214778	0.063875		0.762712
5	0.864262	0.231699	0.054743	0.811159	0.874675	0.223076	0.061975		0.813559
6	0.877242	0.222913	0.059203	0.806867	0.866805	0.223617	0.062249		0.788136
7	0.893745	0.210517	0.065260	0.849785	0.880175	0.203139	0.068098		0.779661
8	0.908760	0.195257	0.069454	0.862661	0.881670	0.187602	0.076588		0.745763
9	0.918217	0.186402	0.075000	0.869099	0.881986	0.190471	0.078809		0.762712



start_lr=1e-3 warmup_target=3e-3 alpha=(1e-4/3e-3) (final_target=1e-4)
1e-3 to 3e-3 to 1e-4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.746771	0.310892	0.032179	0.789700	0.832049	0.263441	0.046534		0.813559
1	0.821413	0.262724	0.043125	0.798283	0.850258	0.251965	0.051706		0.822034
2	0.842680	0.248604	0.047805	0.813305	0.858810	0.226705	0.057989		0.796610
3	0.845897	0.245679	0.049960	0.813305	0.855780	0.233566	0.054367		0.796610
4	0.856759	0.237153	0.051116	0.811159	0.868803	0.231968	0.058096		0.796610
5	0.881796	0.218602	0.058105	0.847640	0.859419	0.198284	0.068779		0.754237
6	0.889001	0.212425	0.062997	0.849785	0.861184	0.218800	0.060852		0.762712
7	0.904457	0.200322	0.068193	0.856223	0.869718	0.193125	0.071309		0.720339
8	0.906760	0.197930	0.073333	0.849785	0.872624	0.194329	0.075360		0.754237
9	0.915458	0.188811	0.074889	0.869099	0.873852	0.188106	0.074742		0.737288


start_lr=1e-3 warmup_target=5e-3 alpha=(1e-3/5e-3) (final_target=1e-3)
1e-3 to 5e-3 to 1e-3

	auc	loss	precision	recall	val_auc	val_loss	val_precision	val_recall
0	0.738117	0.328837	0.034926	0.746781	0.811803	0.293274	0.043191	0.830508
1	0.826458	0.261270	0.048014	0.770386	0.808222	0.301380	0.041961	0.754237
2	0.838109	0.253143	0.049600	0.772532	0.843677	0.248660	0.056483	0.745763
3	0.845838	0.245880	0.049668	0.770386	0.851595	0.262876	0.052784	0.771186
4	0.849386	0.242935	0.051004	0.796137	0.860560	0.210553	0.064151	0.720339
5	0.856322	0.238547	0.053878	0.800429	0.870070	0.219013	0.066527	0.805085
6	0.883574	0.217395	0.060784	0.821888	0.871457	0.180631	0.074800	0.711864
7	0.888423	0.212799	0.062400	0.836910	0.873423	0.193172	0.070132	0.720339
8	0.907350	0.196570	0.070823	0.860515	0.881130	0.179995	0.077059	0.737288
9	0.909969	0.193721	0.072324	0.866953	0.880777	0.182944	0.075993	0.745763


1e-3 to 1e-2 to 1e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.746518	0.318423	0.034802	0.725322	0.825794	0.273965	0.047856		0.813559
1	0.814541	0.267417	0.043206	0.740343	0.853145	0.253219	0.053215		0.813559
2	0.835772	0.254370	0.049775	0.783262	0.856761	0.228203	0.065809		0.737288
3	0.846527	0.246804	0.051387	0.783262	0.864460	0.238682	0.058599		0.779661
4	0.848887	0.245260	0.053485	0.785408	0.858849	0.169571	0.090113		0.610169
5	0.852699	0.242830	0.052952	0.766094	0.866194	0.187827	0.074202		0.728814
6	0.873036	0.226411	0.057958	0.811159	0.880037	0.204189	0.071594		0.788136
7	0.891920	0.209975	0.064976	0.830472	0.885495	0.214279	0.069920		0.813559
8	0.899031	0.204087	0.067621	0.841202	0.889060	0.203015	0.075337		0.805085
9	0.910976	0.192216	0.073024	0.834764	0.888726	0.195198	0.075331		0.771186


1e-3 to 5e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740014	0.322364	0.033848	0.706009	0.793083	0.309429	0.042174		0.762712
1	0.818121	0.266512	0.046852	0.761803	0.833800	0.280669	0.049552		0.796610
2	0.832644	0.257503	0.049902	0.761803	0.860678	0.253817	0.055931		0.771186
3	0.859019	0.239223	0.057358	0.800429	0.837817	0.310548	0.047328		0.788136
4	0.861220	0.237093	0.054778	0.798283	0.844372	0.212853	0.063150		0.720339
5	0.858053	0.239383	0.055623	0.781116	0.855210	0.280455	0.052839		0.796610
6	0.880657	0.220423	0.061179	0.819743	0.855152	0.250730	0.057070		0.762712
7	0.877348	0.222337	0.059906	0.817597	0.856144	0.246030	0.056527		0.822034
8	0.889607	0.212376	0.062796	0.826180	0.855736	0.217382	0.062412		0.754237
9	0.885263	0.215678	0.063791	0.834764	0.860739	0.193229	0.070234		0.711864


1e-3 to 1e-4 

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.719406	0.354666	0.031955	0.729614	0.830636	0.288253	0.047642		0.855932
1	0.825407	0.262515	0.046269	0.798283	0.855781	0.261168	0.053326		0.855932
2	0.850859	0.243583	0.051937	0.811159	0.857025	0.226845	0.065052		0.796610
3	0.859169	0.236194	0.053046	0.781116	0.857997	0.230656	0.059050		0.779661
4	0.875506	0.223713	0.057372	0.813305	0.868139	0.220000	0.067749		0.813559
5	0.894026	0.209368	0.063641	0.830472	0.872635	0.217393	0.065688		0.796610
6	0.899882	0.204586	0.068589	0.862661	0.866035	0.212790	0.066717		0.745763
7	0.912625	0.192564	0.071834	0.869099	0.871856	0.202202	0.069433		0.737288
8	0.916268	0.189373	0.073766	0.856223	0.873800	0.198245	0.070617		0.737288
9	0.919425	0.185512	0.075415	0.866953	0.874848	0.192278	0.074136		0.745763


1e-3 to 5e-3 to 5e-6

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.748701	0.326667	0.033576	0.731760	0.846645	0.260026	0.050706		0.822034
1	0.811196	0.270688	0.042932	0.776824	0.831995	0.275020	0.046237		0.796610
2	0.841064	0.249531	0.050422	0.793991	0.869728	0.246439	0.057333		0.805085
3	0.843499	0.248997	0.051617	0.787554	0.870262	0.259808	0.057613		0.830508
4	0.864799	0.233080	0.057156	0.817597	0.856282	0.213302	0.066327		0.771186
5	0.877548	0.223721	0.059669	0.828326	0.869329	0.223399	0.066761		0.796610
6	0.884437	0.217318	0.061132	0.839056	0.875247	0.217212	0.066230		0.771186
7	0.893957	0.208505	0.064976	0.841202	0.863330	0.197994	0.074074		0.728814
8	0.900903	0.202630	0.069538	0.845494	0.869271	0.217744	0.065539		0.788136
9	0.913998	0.191181	0.074277	0.871245	0.869657	0.199611	0.070258		0.762712


1e-3 to 1e-2 to 5e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.735769	0.335557	0.034610	0.746781	0.834074	0.287127	0.045695		0.805085
1	0.806559	0.273673	0.043393	0.755365	0.843609	0.269929	0.047809		0.813559
2	0.834610	0.254783	0.048573	0.770386	0.848905	0.246966	0.057070		0.762712
3	0.846817	0.245258	0.049207	0.778970	0.871215	0.215259	0.066713		0.805085
4	0.844721	0.249129	0.051742	0.761803	0.860595	0.264334	0.055724		0.779661
5	0.862678	0.235818	0.055387	0.785408	0.873749	0.295180	0.047170		0.847458
6	0.867079	0.233230	0.057203	0.815451	0.884119	0.174224	0.083930		0.694915
7	0.879699	0.219458	0.058572	0.821888	0.884769	0.262781	0.049342		0.889831
8	0.882483	0.217863	0.058929	0.847640	0.881007	0.210351	0.070853		0.745763
9	0.888632	0.211843	0.061655	0.841202	0.887141	0.192364	0.076222		0.779661


5e-3 to 1e-2 to 1e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.758691	0.306146	0.036249	0.701717	0.859744	0.270729	0.048654		0.872881
1	0.833777	0.255935	0.049056	0.763949	0.860588	0.244997	0.051917		0.745763
2	0.844002	0.247322	0.052549	0.791846	0.864390	0.211219	0.069656		0.737288
3	0.851886	0.241816	0.052593	0.778970	0.865782	0.212146	0.068356		0.754237
4	0.862812	0.233600	0.055388	0.787554	0.870931	0.194017	0.075306		0.728814
5	0.866405	0.231459	0.056719	0.796137	0.878194	0.207782	0.071317		0.771186
6	0.878703	0.224189	0.063259	0.804721	0.869234	0.271316	0.055743		0.838983
7	0.882464	0.218923	0.061953	0.834764	0.883803	0.201839	0.072093		0.788136
8	0.899418	0.203990	0.067001	0.830472	0.886252	0.209584	0.069424		0.796610
9	0.905183	0.198048	0.070038	0.841202	0.884184	0.205469	0.071266		0.796610
	

1e-3 to 1e-2 to 1e-4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.734459	0.319617	0.031993	0.733906	0.838710	0.270890	0.047572		0.813559
1	0.813055	0.267070	0.043011	0.763949	0.858574	0.260820	0.051808		0.813559
2	0.829591	0.257286	0.045217	0.759657	0.857219	0.265399	0.052822		0.864407
3	0.839887	0.250525	0.049850	0.783262	0.871225	0.226607	0.066914		0.762712
4	0.853167	0.243111	0.054070	0.776824	0.873881	0.184780	0.080913		0.661017
5	0.850691	0.241891	0.052389	0.804721	0.876501	0.237580	0.067350		0.779661
6	0.872945	0.226336	0.058233	0.821888	0.871409	0.178588	0.084608		0.703390
7	0.887103	0.215151	0.061631	0.817597	0.873771	0.214554	0.068182		0.762712
8	0.891085	0.213152	0.063833	0.841202	0.885969	0.206300	0.070527		0.805085
9	0.906495	0.197477	0.069264	0.858369	0.886675	0.191066	0.074790		0.754237

------------------------------------------------------------------------------------------------------------
------best------

1e-3 to 1e-2 to 1e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.746518	0.318423	0.034802	0.725322	0.825794	0.273965	0.047856		0.813559
1	0.814541	0.267417	0.043206	0.740343	0.853145	0.253219	0.053215		0.813559
2	0.835772	0.254370	0.049775	0.783262	0.856761	0.228203	0.065809		0.737288
3	0.846527	0.246804	0.051387	0.783262	0.864460	0.238682	0.058599		0.779661
4	0.848887	0.245260	0.053485	0.785408	0.858849	0.169571	0.090113		0.610169
5	0.852699	0.242830	0.052952	0.766094	0.866194	0.187827	0.074202		0.728814
6	0.873036	0.226411	0.057958	0.811159	0.880037	0.204189	0.071594		0.788136
7	0.891920	0.209975	0.064976	0.830472	0.885495	0.214279	0.069920		0.813559
8	0.899031	0.204087	0.067621	0.841202	0.889060	0.203015	0.075337		0.805085
9	0.910976	0.192216	0.073024	0.834764	0.888726	0.195198	0.075331		0.771186

------------------------------------------------------------------------------------------------------------

1e-3 to 1e-2 (5 epochs) to 1e-3 (15 epochs)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.729652	0.335525	0.034486	0.725322	0.829353	0.285550	0.046956		0.771186
1	0.812287	0.271022	0.046356	0.742489	0.816647	0.305724	0.047811		0.805085
2	0.843603	0.250568	0.053110	0.789700	0.842103	0.261252	0.058290		0.762712
3	0.836509	0.253108	0.050622	0.759657	0.842022	0.276122	0.052066		0.779661
4	0.853164	0.241842	0.053525	0.798283	0.859851	0.211696	0.066090		0.711864
5	0.857368	0.237740	0.053411	0.804721	0.863787	0.235112	0.060887		0.779661
6	0.869917	0.229577	0.056935	0.806867	0.874442	0.172602	0.085202		0.644068
7	0.862547	0.234965	0.054074	0.796137	0.864538	0.223019	0.061252		0.779661
8	0.874816	0.224172	0.056029	0.817597	0.863608	0.258534	0.057797		0.813559
9	0.883653	0.217760	0.060556	0.864807	0.865043	0.215108	0.063003		0.796610
10	0.883523	0.219065	0.061344	0.832618	0.861996	0.258427	0.055948		0.805085
11	0.892929	0.209301	0.062922	0.819743	0.875552	0.179618	0.083571		0.745763
12	0.898207	0.205509	0.065459	0.845494	0.877010	0.202735	0.071086		0.754237
13	0.901965	0.201644	0.068077	0.856223	0.870617	0.173607	0.087318		0.711864
14	0.907789	0.195951	0.071507	0.843348	0.883916	0.196417	0.070379		0.771186
15	0.914328	0.190759	0.074691	0.856223	0.880893	0.190156	0.072864		0.737288
16	0.920247	0.183186	0.074667	0.890558	0.880787	0.170856	0.079074		0.694915
17	0.924885	0.177630	0.081229	0.890558	0.884135	0.177267	0.078652		0.711864
18	0.931549	0.170542	0.084091	0.892704	0.884311	0.169864	0.083416		0.711864
19	0.922839	0.180049	0.080182	0.869099	0.881785	0.196242	0.074979		0.754237


1e-3 to 1e-2 (5 epochs) to 1e-4 (15 epochs)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.734726	0.330987	0.034602	0.729614	0.840796	0.277976	0.046500		0.805085
1	0.820833	0.264306	0.047001	0.761803	0.852400	0.254252	0.055524		0.822034
2	0.845280	0.248824	0.053201	0.802575	0.841389	0.236716	0.058543		0.728814
3	0.843678	0.247000	0.050466	0.766094	0.860122	0.252322	0.058421		0.796610
4	0.857188	0.241094	0.056960	0.802575	0.858158	0.276148	0.058191		0.779661
5	0.864578	0.233042	0.058730	0.789700	0.852716	0.223977	0.067946		0.720339
6	0.866648	0.234414	0.061340	0.787554	0.871233	0.226398	0.071429		0.745763
7	0.876345	0.226604	0.065622	0.809013	0.876815	0.275862	0.058217		0.813559
8	0.873229	0.228410	0.061944	0.806867	0.875082	0.179548	0.079397		0.669492
9	0.890367	0.212643	0.066826	0.839056	0.863488	0.200759	0.067599		0.737288
10	0.884673	0.216488	0.063589	0.830472	0.878160	0.244351	0.062865		0.822034
11	0.882091	0.221051	0.063484	0.821888	0.874571	0.171455	0.088071		0.669492
12	0.899775	0.203269	0.068892	0.828326	0.879505	0.209991	0.071210		0.788136
13	0.900992	0.204094	0.069644	0.843348	0.881479	0.179328	0.079926		0.728814
14	0.910199	0.193333	0.074150	0.856223	0.871341	0.181769	0.080492		0.720339
15	0.917382	0.185749	0.076484	0.862661	0.877463	0.216653	0.068095		0.754237
16	0.923488	0.180238	0.081690	0.879828	0.878751	0.179669	0.080229		0.711864
17	0.924162	0.178357	0.080484	0.884120	0.876511	0.189940	0.077063		0.720339
18	0.933842	0.168099	0.085536	0.881974	0.877231	0.181605	0.080374		0.728814
19	0.928320	0.174034	0.085093	0.881974	0.877281	0.178535	0.081262		0.720339


1e-3 to 1e-2 (5 epochs) to 1e-3 (5 epochs) + 10 epochs

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.742740	0.311941	0.032915	0.793991	0.843634	0.234350	0.054184		0.762712
1	0.825272	0.259651	0.046395	0.793991	0.861118	0.270730	0.055268		0.813559
2	0.815390	0.265238	0.044731	0.766094	0.876588	0.231769	0.063471		0.796610
3	0.843364	0.248089	0.051308	0.774678	0.867534	0.236806	0.061489		0.805085
4	0.858414	0.235987	0.051033	0.811159	0.863857	0.270136	0.044379		0.889831
5	0.857471	0.240962	0.056105	0.778970	0.861652	0.223065	0.060541		0.720339
6	0.869709	0.231240	0.057704	0.813305	0.837356	0.226755	0.058780		0.677966
7	0.876996	0.222180	0.058714	0.811159	0.872048	0.225457	0.062374		0.788136
8	0.893990	0.210335	0.065882	0.841202	0.881244	0.185600	0.076991		0.737288
9	0.909603	0.193808	0.072694	0.845494	0.880407	0.192641	0.075567		0.762712
10	0.911686	0.191805	0.073952	0.866953	0.881396	0.207119	0.070661		0.779661
11	0.914225	0.190322	0.075267	0.877682	0.882030	0.173442	0.083955		0.762712
12	0.918598	0.185197	0.077113	0.869099	0.878452	0.190210	0.075214		0.745763
13	0.918857	0.185868	0.079196	0.879828	0.878830	0.184991	0.077617		0.728814
14	0.914628	0.187997	0.075024	0.841202	0.879817	0.195004	0.071317		0.771186
15	0.920065	0.186159	0.080116	0.886266	0.871672	0.183317	0.076854		0.720339
16	0.920472	0.182644	0.078020	0.869099	0.877847	0.186996	0.074678		0.737288
17	0.922695	0.181373	0.079465	0.866953	0.876926	0.173912	0.083170		0.720339
18	0.919652	0.184293	0.078891	0.873391	0.875809	0.180174	0.075893		0.720339
19	0.928181	0.174038	0.082384	0.884120	0.875275	0.184312	0.077063		0.720339


1e-3 to 1e-2 (5 epochs) to 1e-4 (5 epochs) + 10 epochs

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.725520	0.358713	0.033584	0.699571	0.841828	0.265102	0.052632		0.796610
1	0.817195	0.265818	0.045118	0.757511	0.861706	0.282545	0.048622		0.822034
2	0.835881	0.254751	0.051122	0.791846	0.854791	0.248122	0.054054		0.796610
3	0.853363	0.241801	0.053556	0.785408	0.864856	0.197187	0.068684		0.711864
4	0.855120	0.238418	0.052418	0.774678	0.872861	0.245961	0.051715		0.855932
5	0.861790	0.234243	0.055581	0.789700	0.878955	0.230391	0.062338		0.813559
6	0.874311	0.225187	0.059728	0.809013	0.859722	0.232175	0.060407		0.779661
7	0.882545	0.218587	0.061010	0.824034	0.881041	0.217258	0.069834		0.822034
8	0.897604	0.205566	0.068663	0.856223	0.885154	0.206709	0.073783		0.796610
9	0.910194	0.196378	0.072711	0.869099	0.885159	0.198489	0.076222		0.779661
10	0.921376	0.184088	0.076671	0.903434	0.885427	0.195895	0.078680		0.788136
11	0.910077	0.194457	0.074759	0.866953	0.884553	0.194842	0.077447		0.771186
12	0.918300	0.186402	0.075024	0.854077	0.884509	0.191045	0.079407		0.771186
13	0.909276	0.196519	0.075167	0.847640	0.884074	0.191411	0.078584		0.771186
14	0.911302	0.193220	0.075465	0.854077	0.883446	0.190861	0.080490		0.779661
15	0.919713	0.185358	0.077010	0.873391	0.883144	0.189002	0.079505		0.762712
16	0.909490	0.193580	0.075095	0.847640	0.883545	0.191293	0.079654		0.779661
17	0.916660	0.188381	0.075051	0.860515	0.882489	0.190231	0.078856		0.771186
18	0.913974	0.190320	0.076894	0.862661	0.882468	0.188302	0.079646		0.762712
19	0.918606	0.186062	0.077547	0.881974	0.883199	0.188882	0.080035		0.771186


---------------------------------------------------------------------------------------------------------------------
trying out other optimizers

1e-3 to 1e-2 to 1e-3 with Nadam

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740903	0.327795	0.034630	0.729614	0.824755	0.287951	0.045410		0.779661
1	0.819902	0.263311	0.044428	0.759657	0.832896	0.273410	0.044455		0.771186
2	0.835227	0.256768	0.051444	0.787554	0.863440	0.267313	0.054484		0.813559
3	0.848138	0.247822	0.054134	0.785408	0.869429	0.245613	0.062384		0.855932
4	0.859326	0.239276	0.056223	0.813305	0.863900	0.197656	0.068935		0.669492
5	0.863321	0.234176	0.056688	0.793991	0.845671	0.208447	0.069172		0.686441
6	0.877479	0.222461	0.060277	0.821888	0.845275	0.338299	0.043146		0.813559
7	0.882628	0.218429	0.059944	0.830472	0.874043	0.194594	0.073269		0.771186
8	0.898071	0.204539	0.065587	0.843348	0.870836	0.197527	0.068598		0.762712
9	0.909499	0.194844	0.071216	0.873391	0.873853	0.199385	0.068759		0.779661


1e-3 to 1e-2 to 1e-3 with AdamW

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.742033	0.324246	0.033630	0.712446	0.860278	0.264708	0.053444		0.822034
1	0.826509	0.260175	0.047817	0.766094	0.852345	0.254041	0.051684		0.754237
2	0.841888	0.249599	0.049201	0.766094	0.837558	0.244821	0.055626		0.745763
3	0.854522	0.239694	0.054596	0.793991	0.848738	0.214869	0.062215		0.694915
4	0.856871	0.240568	0.056758	0.787554	0.856514	0.200931	0.074685		0.652542
5	0.856598	0.239698	0.054971	0.796137	0.878626	0.205114	0.078498		0.779661
6	0.875083	0.224620	0.060334	0.806867	0.862515	0.249326	0.061862		0.822034
7	0.887922	0.215192	0.064764	0.832618	0.875113	0.194648	0.080279		0.779661
8	0.896426	0.207321	0.069163	0.828326	0.881385	0.211199	0.072659		0.822034
9	0.907436	0.199436	0.071112	0.860515	0.881596	0.210204	0.078464		0.796610



1e-3 to 1e-2 to 1e-4 with Nadam

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.719043	0.350834	0.031574	0.718884	0.819153	0.250742	0.048622		0.762712
1	0.802500	0.276963	0.041920	0.740343	0.857719	0.243463	0.057533		0.822034
2	0.843275	0.248901	0.050067	0.798283	0.837940	0.258538	0.048253		0.737288
3	0.843584	0.246416	0.048923	0.809013	0.876616	0.209718	0.067472		0.805085
4	0.857815	0.236360	0.051927	0.806867	0.865021	0.265524	0.049371		0.864407
5	0.854098	0.241356	0.053102	0.815451	0.863864	0.214183	0.068726		0.754237
6	0.879743	0.222567	0.061686	0.843348	0.882520	0.174958	0.086139		0.737288
7	0.882130	0.218909	0.061146	0.824034	0.858698	0.229675	0.065903		0.779661
8	0.899143	0.204342	0.066508	0.841202	0.878495	0.194858	0.075517		0.805085
9	0.907254	0.196594	0.071214	0.864807	0.880113	0.195002	0.073930		0.805085



1e-3 to 1e-2 to 1e-4 with AdamW

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.745340	0.319788	0.035093	0.727468	0.845059	0.245020	0.054184		0.762712
1	0.809303	0.269096	0.044055	0.748927	0.857430	0.246424	0.056724		0.754237
2	0.841333	0.250106	0.050135	0.798283	0.846906	0.243345	0.055407		0.703390
3	0.853738	0.244247	0.056126	0.809013	0.845944	0.323681	0.047314		0.813559
4	0.850340	0.245519	0.055165	0.791846	0.861559	0.218091	0.069468		0.686441
5	0.856141	0.239376	0.054589	0.787554	0.877396	0.214125	0.072934		0.762712
6	0.874031	0.228138	0.059894	0.798283	0.875411	0.188818	0.076856		0.745763
7	0.880653	0.221447	0.061928	0.813305	0.872428	0.214687	0.067548		0.779661
8	0.896582	0.207927	0.066632	0.834764	0.878374	0.200222	0.071371		0.745763
9	0.908744	0.195882	0.071172	0.851931	0.878181	0.194687	0.073272		0.745763

---------------------------------------------------------------------------------------------------------------------


1e-3 to 1e-2 to 1e-3 train and finetune

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.718454	0.337264	0.031579	0.688841	0.839180	0.280609	0.046696		0.796610
1	0.811501	0.270514	0.044037	0.742489	0.848490	0.292658	0.045353		0.864407
2	0.841494	0.250041	0.050745	0.789700	0.864778	0.229142	0.061497		0.779661
3	0.850172	0.242880	0.052707	0.793991	0.869132	0.199704	0.071429		0.703390
4	0.862422	0.237303	0.057772	0.804721	0.849587	0.232009	0.062274		0.728814
5	0.872572	0.226980	0.058897	0.806867	0.862514	0.235907	0.066171		0.754237
6	0.878133	0.224822	0.062765	0.826180	0.881104	0.190530	0.081937		0.745763
7	0.881845	0.219478	0.062370	0.836910	0.888462	0.177880	0.077547		0.728814
8	0.900957	0.202544	0.066281	0.836910	0.888030	0.197761	0.073092		0.771186
9	0.907978	0.194343	0.071378	0.869099	0.890576	0.198280	0.073815		0.805085

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.804912	0.286837	0.049034	0.718884	0.852441	0.225733	0.067753		0.669492
1	0.829956	0.261734	0.051121	0.753219	0.847767	0.269721	0.048864		0.728814
2	0.844371	0.249536	0.051304	0.785408	0.876668	0.204330	0.076165		0.720339
3	0.867285	0.230263	0.055499	0.817597	0.889116	0.212104	0.072112		0.830508
4	0.902101	0.201753	0.064975	0.851931	0.897099	0.204490	0.074534		0.813559


1e-3 to 1e-2 to 1e-3 train and 0.1trainLR finetune

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.739812	0.334685	0.035517	0.716738	0.815062	0.278164	0.046282		0.754237
1	0.806421	0.275871	0.045714	0.740343	0.834495	0.274433	0.046811		0.771186
2	0.827593	0.261835	0.050710	0.751073	0.862166	0.233694	0.064945		0.745763
3	0.852768	0.242430	0.053184	0.783262	0.848317	0.212353	0.065600		0.694915
4	0.851952	0.242735	0.053469	0.778970	0.865780	0.186058	0.083857		0.677966
5	0.874440	0.227383	0.062427	0.804721	0.854708	0.173476	0.076459		0.644068
6	0.874743	0.227736	0.061815	0.798283	0.868951	0.177940	0.081430		0.694915
7	0.897835	0.207146	0.071285	0.834764	0.855166	0.262183	0.058787		0.805085
8	0.901022	0.202659	0.068737	0.845494	0.871435	0.203401	0.071901		0.737288
9	0.902605	0.200888	0.070362	0.839056	0.874118	0.194293	0.073213		0.720339

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.868210	0.236132	0.062103	0.776824	0.858650	0.223026	0.064964		0.754237
1	0.883604	0.219801	0.063981	0.828326	0.838687	0.208346	0.065128		0.669492
2	0.878839	0.223701	0.062661	0.836910	0.871623	0.224764	0.067274		0.813559
3	0.907072	0.198979	0.070500	0.849785	0.883099	0.203687	0.076987		0.788136
4	0.929133	0.177948	0.080877	0.894850	0.891392	0.193952	0.082888		0.788136


1e-3 to 1e-2 to 1e-4 train and finetune

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.730415	0.337912	0.032895	0.725322	0.841085	0.251203	0.051963		0.762712
1	0.815488	0.266187	0.044469	0.791846	0.846123	0.285583	0.046696		0.796610
2	0.832146	0.258544	0.048175	0.787554	0.871138	0.252216	0.054710		0.822034
3	0.849889	0.243508	0.050048	0.789700	0.853910	0.275830	0.048300		0.830508
4	0.846573	0.245537	0.050871	0.796137	0.865472	0.230697	0.061350		0.762712
5	0.863466	0.233855	0.053842	0.793991	0.869487	0.190617	0.072646		0.686441
6	0.871801	0.227632	0.057100	0.815451	0.889517	0.202399	0.066398		0.838983
7	0.884235	0.216939	0.060644	0.828326	0.881832	0.195318	0.072858		0.771186
8	0.902774	0.199660	0.067864	0.843348	0.886052	0.208874	0.069385		0.822034
9	0.911550	0.191672	0.069808	0.858369	0.886198	0.199585	0.074074		0.813559

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.805364	0.290706	0.046945	0.746781	0.825647	0.259418	0.049267		0.711864
1	0.825322	0.260748	0.048391	0.781116	0.852576	0.292701	0.052243		0.838983
2	0.838112	0.253092	0.051060	0.770386	0.860999	0.192215	0.073529		0.677966
3	0.876495	0.225973	0.057509	0.828326	0.891882	0.198935	0.080870		0.788136
4	0.900323	0.205382	0.067503	0.843348	0.900768	0.208581	0.074501		0.822034


1e-3 to 1e-2 to 1e-4 train and 0.1trainLR finetune

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.745499	0.328495	0.036160	0.721030	0.829900	0.279266	0.050830		0.830508
1	0.812901	0.272295	0.046729	0.751073	0.830546	0.293826	0.048212		0.788136
2	0.843402	0.250155	0.053312	0.787554	0.841022	0.245790	0.057355		0.720339
3	0.852624	0.243883	0.054642	0.776824	0.848432	0.223506	0.059406		0.711864
4	0.857926	0.240537	0.056830	0.791846	0.872536	0.206006	0.068342		0.737288
5	0.855250	0.240224	0.053666	0.778970	0.862333	0.253113	0.058967		0.822034
6	0.862625	0.235433	0.056846	0.787554	0.864310	0.209218	0.066667		0.737288
7	0.891040	0.211830	0.068135	0.843348	0.868920	0.224750	0.066139		0.779661
8	0.899616	0.204038	0.067198	0.832618	0.882721	0.190250	0.076278		0.771186
9	0.908107	0.197020	0.074288	0.856223	0.881838	0.193720	0.075063		0.762712

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.869180	0.236038	0.064575	0.763949	0.849604	0.220838	0.064112		0.737288
1	0.864470	0.238529	0.060742	0.783262	0.848990	0.194506	0.071086		0.754237
2	0.879872	0.223733	0.062660	0.839056	0.880689	0.190628	0.077601		0.745763
3	0.907775	0.200601	0.071630	0.871245	0.883625	0.205570	0.072498		0.779661
4	0.933554	0.174882	0.080376	0.918455	0.888026	0.194222	0.078856		0.771186


1e-3 to 1e-2 to 1e-4 train and 0.1trainLR finetune, label_smoothing=0.1 for finetune

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.742095	0.333696	0.035104	0.714592	0.857339	0.267139	0.053485		0.838983
1	0.814171	0.270055	0.046623	0.774678	0.836759	0.245849	0.053204		0.745763
2	0.833977	0.257365	0.049408	0.787554	0.871395	0.233761	0.057927		0.805085
3	0.848219	0.244680	0.051832	0.783262	0.863705	0.238865	0.059773		0.847458
4	0.861957	0.236628	0.055217	0.798283	0.864328	0.217335	0.061737		0.771186
5	0.863927	0.233332	0.056110	0.809013	0.869575	0.199615	0.072526		0.720339
6	0.872447	0.228943	0.059132	0.821888	0.874132	0.193186	0.076923		0.728814
7	0.882777	0.217749	0.059595	0.809013	0.881300	0.238208	0.061200		0.847458
8	0.893765	0.207300	0.063778	0.836910	0.887093	0.208665	0.069187		0.822034
9	0.904916	0.198301	0.067607	0.854077	0.885534	0.192873	0.071542		0.762712

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.855960	0.248621	0.057540	0.746781	0.859320	0.213482	0.062500		0.737288
1	0.873852	0.229225	0.058148	0.836910	0.856973	0.210026	0.059249		0.694915
2	0.880428	0.224871	0.057760	0.809013	0.868521	0.214916	0.065598		0.762712
3	0.898173	0.208392	0.062610	0.841202	0.886824	0.200470	0.074186		0.830508
4	0.939932	0.175075	0.078428	0.929185	0.889572	0.192049	0.080311		0.788136


1e-3 to 1e-2 to 1e-4 train and 0.1trainLR finetune, label_smoothing=0.2 for finetune

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.738165	0.319048	0.033409	0.725322	0.806508	0.306419	0.039224		0.805085
1	0.806699	0.271762	0.041662	0.727468	0.847105	0.260438	0.051368		0.779661
2	0.837418	0.254960	0.048950	0.770386	0.862809	0.216307	0.069767		0.762712
3	0.857571	0.240598	0.054078	0.781116	0.855795	0.236838	0.054118		0.779661
4	0.857981	0.241938	0.055144	0.798283	0.864322	0.218497	0.069157		0.737288
5	0.855397	0.239705	0.052438	0.793991	0.885615	0.188048	0.076923		0.728814
6	0.866548	0.232404	0.055951	0.791846	0.867038	0.199125	0.078329		0.762712
7	0.884552	0.217365	0.060985	0.826180	0.877207	0.246554	0.060024		0.847458
8	0.893482	0.210084	0.065792	0.839056	0.883752	0.201248	0.073896		0.779661
9	0.916669	0.188402	0.074061	0.892704	0.881417	0.197147	0.074713		0.771186

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.872493	0.244198	0.062899	0.783262	0.848185	0.294465	0.047667		0.805085
1	0.865828	0.248307	0.058089	0.798283	0.848199	0.242542	0.060309		0.728814
2	0.872959	0.246142	0.060528	0.802575	0.847727	0.277291	0.051627		0.779661
3	0.908462	0.215821	0.066490	0.862661	0.887929	0.220434	0.073446		0.771186
4	0.929856	0.198431	0.078004	0.905579	0.887530	0.209873	0.077586		0.762712




---------------------------------------------------------------------------------------------------------------------
age=0 when missing

1e-3 to 1e-2 to 1e-4 with age=0 when missing

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740964	0.326256	0.035305	0.746781	0.811396	0.282018	0.040602		0.754237
1	0.815965	0.265774	0.044808	0.757511	0.843544	0.268508	0.051762		0.796610
2	0.831979	0.256521	0.049685	0.761803	0.845033	0.263149	0.053549		0.728814
3	0.840581	0.250975	0.051736	0.783262	0.867543	0.241446	0.061580		0.779661
4	0.862036	0.232462	0.052838	0.811159	0.858444	0.168832	0.077164		0.627119
5	0.849651	0.245513	0.054640	0.768240	0.864908	0.222783	0.068285		0.762712
6	0.871761	0.227828	0.059046	0.802575	0.865649	0.286823	0.055104		0.805085
7	0.884065	0.216541	0.060446	0.809013	0.883234	0.196429	0.074513		0.745763
8	0.899660	0.204178	0.069368	0.841202	0.882627	0.192128	0.077911		0.771186
9	0.911583	0.193519	0.075539	0.864807	0.880973	0.186329	0.077408		0.728814


1e-3 to 1e-2 to 1e-3 with age=0 when missing

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.736504	0.343029	0.034031	0.718884	0.826223	0.252656	0.048683		0.720339
1	0.797988	0.279673	0.042316	0.733906	0.856698	0.249813	0.057214		0.779661
2	0.844059	0.249345	0.051546	0.804721	0.838810	0.205263	0.063281		0.686441
3	0.834562	0.255553	0.048633	0.774678	0.866685	0.228253	0.062246		0.779661
4	0.857307	0.240720	0.054000	0.806867	0.870858	0.241531	0.062083		0.788136
5	0.863449	0.234503	0.056372	0.798283	0.865977	0.231739	0.061704		0.779661
6	0.878349	0.225050	0.063053	0.826180	0.879097	0.220496	0.067608		0.771186
7	0.883544	0.218872	0.061389	0.815451	0.883520	0.209622	0.067529		0.796610
8	0.894216	0.208155	0.065963	0.845494	0.875141	0.196882	0.070644		0.762712
9	0.901240	0.203207	0.067938	0.841202	0.875800	0.199352	0.071828		0.762712


age=0 when missing seems to give comparable results to age=-10 when missing.
We'll keep age=-10 when missing since its different from existing values and easier to visualize.
---------------------------------------------------------------------------------------------------------------------


1e-3 to 1e-2 to 1e-4 train and finetune from L287

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.748443	0.320503	0.034577	0.763949	0.844963	0.254211	0.050463		0.830508
1	0.807744	0.274963	0.043400	0.774678	0.863454	0.224787	0.060056		0.728814
2	0.831407	0.257672	0.047877	0.778970	0.851789	0.209657	0.059286		0.703390
3	0.845252	0.246132	0.048057	0.796137	0.873208	0.207941	0.063814		0.720339
4	0.855642	0.238647	0.052428	0.785408	0.865987	0.245535	0.057486		0.771186
5	0.865196	0.233115	0.055418	0.815451	0.860939	0.285763	0.053723		0.855932
6	0.872806	0.230322	0.060481	0.815451	0.874009	0.250093	0.057298		0.805085
7	0.878641	0.223622	0.061502	0.834764	0.882306	0.223029	0.067391		0.788136
8	0.905941	0.199117	0.070336	0.858369	0.887317	0.177958	0.081671		0.728814
9	0.908721	0.195662	0.073596	0.843348	0.887808	0.190684	0.075214		0.745763

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.794508	0.295062	0.047783	0.714592	0.857485	0.218449	0.062500		0.728814
1	0.828577	0.259383	0.047167	0.778970	0.885783	0.276716	0.059675		0.838983
2	0.866599	0.232413	0.055360	0.811159	0.885422	0.253627	0.061327		0.830508
3	0.901753	0.201065	0.066938	0.845494	0.896531	0.182229	0.081953		0.796610
4	0.936179	0.166997	0.087145	0.907725	0.909204	0.179758	0.088342		0.822034


1e-3 to 1e-2 to 1e-4 train and finetune 0.1LR L287

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.718783	0.345454	0.030897	0.710300	0.825611	0.266222	0.047286		0.745763
1	0.820201	0.263550	0.044784	0.774678	0.849011	0.262098	0.048640		0.788136
2	0.833257	0.256762	0.047693	0.787554	0.855344	0.208722	0.062300		0.661017
3	0.846915	0.246396	0.050705	0.787554	0.867257	0.180903	0.083415		0.720339
4	0.857821	0.240078	0.053699	0.806867	0.858226	0.206310	0.072330		0.728814
5	0.860658	0.236730	0.054701	0.809013	0.875197	0.207784	0.072190		0.745763
6	0.868369	0.229326	0.056277	0.809013	0.857934	0.241490	0.060241		0.805085
7	0.884170	0.217603	0.060492	0.828326	0.881040	0.207771	0.069242		0.805085
8	0.896240	0.209172	0.069074	0.849785	0.880860	0.206452	0.070242		0.788136
9	0.914131	0.191087	0.073461	0.875537	0.881923	0.190147	0.075567		0.762712

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.839292	0.265845	0.051414	0.753219	0.845379	0.298211	0.043568		0.889831
1	0.851496	0.247672	0.051937	0.811159	0.865527	0.236529	0.053422		0.813559
2	0.874943	0.223973	0.052660	0.847640	0.865567	0.234665	0.056404		0.813559
3	0.924614	0.183455	0.074377	0.896996	0.892607	0.182378	0.079963		0.737288
4	0.953596	0.148815	0.098861	0.931330	0.902789	0.183625	0.087977		0.762712


1e-3 to 1e-2 to 1e-4 train and finetune full

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.739894	0.314360	0.033202	0.742489	0.820594	0.256801	0.043499		0.762712
1	0.809332	0.270157	0.042831	0.759657	0.854829	0.239060	0.056796		0.754237
2	0.837906	0.252396	0.048334	0.781116	0.857351	0.229262	0.061048		0.779661
3	0.845475	0.250066	0.053825	0.789700	0.857566	0.255967	0.059607		0.796610
4	0.855336	0.239343	0.053406	0.802575	0.874070	0.209418	0.062711		0.788136
5	0.847449	0.244790	0.051110	0.785408	0.857629	0.315715	0.044731		0.881356
6	0.864587	0.233752	0.054861	0.800429	0.872540	0.233756	0.061431		0.822034
7	0.880950	0.220393	0.060549	0.832618	0.871982	0.205277	0.068576		0.771186
8	0.900051	0.202895	0.068555	0.841202	0.878237	0.190792	0.075314		0.762712
9	0.904199	0.198149	0.068237	0.845494	0.878377	0.191097	0.074074		0.762712

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.772311	0.305892	0.039874	0.703863	0.790052	0.317542	0.038023		0.762712
1	0.780146	0.288063	0.036827	0.768240	0.835889	0.166915	0.091062		0.457627
2	0.787775	0.278158	0.038798	0.761803	0.822584	0.481434	0.062715		0.618644
3	0.821982	0.259026	0.042349	0.832618	0.853821	0.233378	0.051196		0.779661
4	0.835640	0.248764	0.045868	0.821888	0.849644	0.302463	0.045475		0.838983



1e-3 to 1e-2 to 1e-4 train and finetune 0.1LR full

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.733785	0.338203	0.033787	0.693133	0.794743	0.294229	0.041379		0.711864
1	0.809509	0.274224	0.045253	0.723176	0.852511	0.256651	0.057803		0.762712
2	0.841796	0.250850	0.050869	0.772532	0.829852	0.239935	0.051939		0.635593
3	0.858272	0.238146	0.054968	0.802575	0.852084	0.235217	0.062907		0.737288
4	0.854499	0.241383	0.053666	0.785408	0.859470	0.216430	0.064715		0.711864
5	0.867306	0.233780	0.059614	0.809013	0.869349	0.198698	0.075686		0.677966
6	0.881273	0.221793	0.062993	0.821888	0.857053	0.204251	0.068163		0.694915
7	0.892821	0.209694	0.065078	0.826180	0.864548	0.193075	0.071307		0.711864
8	0.902083	0.201107	0.066794	0.826180	0.876686	0.195870	0.067353		0.720339
9	0.913790	0.189861	0.073676	0.854077	0.875041	0.193761	0.069421		0.711864

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.801707	0.303207	0.045493	0.699571	0.846123	0.234237	0.055000		0.745763
1	0.835319	0.257986	0.047373	0.787554	0.820932	0.258792	0.050535		0.720339
2	0.832120	0.258584	0.048119	0.787554	0.827885	0.234859	0.051609		0.720339
3	0.864787	0.231089	0.051447	0.843348	0.879039	0.224539	0.062704		0.813559
4	0.903353	0.202797	0.059759	0.894850	0.896639	0.223916	0.064249		0.855932



1e-3 to 1e-2 to 1e-4 train & finetune 0.01LR full

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740048	0.328520	0.033433	0.723176	0.840180	0.286881	0.045760		0.864407
1	0.812138	0.269711	0.044536	0.766094	0.847234	0.297485	0.045065		0.847458
2	0.839186	0.248845	0.048215	0.793991	0.863489	0.206798	0.066717		0.754237
3	0.837409	0.255331	0.050352	0.783262	0.871274	0.249754	0.059364		0.822034
4	0.853750	0.243150	0.054821	0.800429	0.865451	0.274428	0.053496		0.855932
5	0.857502	0.239211	0.054097	0.800429	0.863732	0.178907	0.086560		0.644068
6	0.872836	0.226432	0.058069	0.806867	0.880919	0.208757	0.074313		0.779661
7	0.887524	0.215888	0.066964	0.836910	0.874572	0.204979	0.072555		0.779661
8	0.905004	0.197828	0.068846	0.854077	0.884347	0.198848	0.071931		0.779661
9	0.905466	0.199504	0.068845	0.845494	0.883832	0.190384	0.076475		0.779661

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.730181	0.405675	0.043464	0.557940	0.810104	0.250142	0.051866		0.694915
1	0.833914	0.265242	0.052679	0.746781	0.854097	0.254036	0.056683		0.805085
2	0.891315	0.210232	0.058643	0.860515	0.863016	0.221421	0.065852		0.822034
3	0.920199	0.188823	0.064395	0.920601	0.865101	0.224591	0.064043		0.813559
4	0.921354	0.185194	0.067883	0.909871	0.864271	0.227729	0.063428		0.796610


1e-4 to 1e-3 to 1e-5 train full model

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.691417	0.375337	0.026559	0.839056	0.784354	0.399513	0.029551		0.847458
1	0.810541	0.274754	0.035536	0.901288	0.837402	0.303819	0.037961		0.889831
2	0.848570	0.246153	0.044599	0.869099	0.856456	0.286496	0.047641		0.864407
3	0.850406	0.242918	0.046484	0.832618	0.825147	0.270553	0.043436		0.762712
4	0.842589	0.249679	0.049876	0.778970	0.790659	0.260236	0.048980		0.508475
5	0.817968	0.266787	0.044265	0.787554	0.852625	0.262708	0.054069		0.822034
6	0.841748	0.248568	0.046708	0.854077	0.868196	0.244140	0.049164		0.847458
7	0.879339	0.220789	0.052026	0.892704	0.874488	0.199858	0.065248		0.779661
8	0.902157	0.202840	0.059860	0.881974	0.877229	0.208223	0.064057		0.762712
9	0.919118	0.187534	0.064649	0.912017	0.882200	0.210509	0.062458		0.796610


1e-3 to 1e-2 to 1e-4 train full model

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.677873	0.345282	0.029222	0.766094	0.797638	0.355359	0.034138		0.898305
1	0.758583	0.296823	0.035700	0.746781	0.738865	0.333772	0.029758		0.771186
2	0.758709	0.296276	0.034986	0.740343	0.848127	0.295235	0.035026		0.923729
3	0.773053	0.286712	0.034427	0.774678	0.822238	0.259894	0.053395		0.686441
4	0.765210	0.299093	0.036573	0.751073	0.825434	0.260187	0.054274		0.677966
5	0.785330	0.283731	0.039418	0.738197	0.834068	1.201722	0.039066		0.822034
6	0.809022	0.267513	0.040207	0.785408	0.850398	0.558433	0.040361		0.872881
7	0.816290	0.263293	0.041857	0.804721	0.837940	0.241441	0.049135		0.745763
8	0.829959	0.253733	0.044622	0.804721	0.857589	0.320464	0.044797		0.813559
9	0.840297	0.247429	0.046525	0.839056	0.852458	0.259133	0.044580		0.805085



1e-3 to 1e-2 to 1e-4 train from L287-------------------------------------------15m 47s

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.735566	0.336837	0.035651	0.731760	0.847990	0.270732	0.051048		0.805085
1	0.826661	0.261202	0.047148	0.798283	0.866667	0.254198	0.051697		0.838983
2	0.844602	0.250923	0.053494	0.791846	0.878454	0.181828	0.067003		0.788136
3	0.856897	0.247854	0.056646	0.811159	0.860607	0.192731	0.069299		0.728814
4	0.879824	0.220303	0.055208	0.851931	0.881800	0.220910	0.059834		0.855932
5	0.888143	0.215267	0.060740	0.856223	0.869470	0.219312	0.057681		0.805085
6	0.898379	0.203825	0.063625	0.879828	0.875462	0.123165	0.111801		0.610169
7	0.926619	0.172188	0.078282	0.903434	0.892652	0.160784	0.081877		0.754237
8	0.944533	0.151207	0.089641	0.909871	0.892246	0.149670	0.099109		0.754237
9	0.954488	0.138643	0.105819	0.924893	0.898956	0.149488	0.097826		0.762712



1e-3 to 1e-2 to 1e-4 train from L287 with LS0.1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740364	0.315464	0.032878	0.800429	0.769971	0.235298	0.053772		0.567797
1	0.828411	0.262655	0.045734	0.813305	0.847348	0.221789	0.055419		0.762712
2	0.856306	0.244375	0.053759	0.813305	0.866774	0.266271	0.059701		0.813559
3	0.875047	0.230827	0.057315	0.839056	0.883615	0.226268	0.058291		0.872881
4	0.895061	0.212466	0.064235	0.869099	0.842634	0.177326	0.058039		0.737288
5	0.893952	0.212373	0.061840	0.879828	0.886708	0.140292	0.097651		0.669492
6	0.883346	0.221869	0.060122	0.847640	0.887131	0.280485	0.050306		0.906780
7	0.925521	0.182558	0.079204	0.905579	0.894723	0.154333	0.101466		0.762712
8	0.950936	0.151027	0.105599	0.922747	0.890166	0.153115	0.102771		0.754237
9	0.965637	0.127806	0.126970	0.933476	0.891704	0.153228	0.111663		0.762712


1e-3 to 1e-2 to 1e-4 train from L287 with LS0.2

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.733618	0.331816	0.033179	0.736051	0.845574	0.296985	0.047091		0.864407
1	0.814415	0.276160	0.043702	0.763949	0.844212	0.266248	0.054100		0.805085
2	0.833733	0.265841	0.048938	0.781116	0.853729	0.243177	0.058056		0.754237
3	0.852710	0.253460	0.052170	0.791846	0.851386	0.212968	0.072595		0.677966
4	0.849938	0.255711	0.052930	0.800429	0.863673	0.214354	0.072212		0.669492
5	0.858383	0.248485	0.052938	0.800429	0.871216	0.216924	0.071485		0.762712
6	0.870081	0.244337	0.059877	0.817597	0.878354	0.228062	0.070136		0.788136
7	0.884337	0.233545	0.064032	0.824034	0.883983	0.197887	0.084746		0.762712
8	0.904312	0.218472	0.071003	0.843348	0.884543	0.223719	0.074367		0.796610
9	0.914128	0.209234	0.075806	0.873391	0.887463	0.213742	0.075331		0.771186


---------------best-----------------------------------------------------------------------------
1e-3 to 1e-2 to 1e-3 train from L287

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.743814	0.312814	0.033256	0.766094	0.807275	0.230231	0.057912		0.601695
1	0.840404	0.248911	0.049712	0.815451	0.836363	0.298436	0.045176		0.813559
2	0.850338	0.246135	0.050459	0.789700	0.882379	0.207414	0.062016		0.813559
3	0.875496	0.223017	0.056674	0.845494	0.878740	0.248063	0.066423		0.771186
4	0.881220	0.228249	0.061521	0.834764	0.877370	0.319934	0.033463		0.949153
5	0.880714	0.221626	0.062197	0.854077	0.888946	0.171618	0.079597		0.737288
6	0.916245	0.186486	0.073431	0.871245	0.888444	0.172978	0.076724		0.754237
7	0.932088	0.168861	0.087453	0.903434	0.899933	0.180871	0.076733		0.788136
8	0.952218	0.138786	0.102041	0.933476	0.900829	0.160035	0.097236		0.805085
9	0.966133	0.119628	0.127089	0.946352	0.902654	0.150843	0.102247		0.771186
-------------------------------------------------------------------------------------------------


1e-3 to 1e-2 to 1e-3 train from L287 with LS0.1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.726467	0.348954	0.033900	0.723176	0.797385	0.755736	0.034722		0.932203
1	0.806112	0.277035	0.043376	0.751073	0.853132	0.257921	0.056638		0.788136
2	0.846651	0.252730	0.050520	0.813305	0.854101	0.279407	0.047437		0.838983
3	0.859800	0.243919	0.057191	0.796137	0.878958	0.270862	0.056786		0.847458
4	0.870558	0.233976	0.057461	0.819743	0.873270	0.353580	0.055747		0.822034
5	0.878648	0.226286	0.058477	0.830472	0.876694	0.157187	0.093051		0.669492
6	0.912585	0.197559	0.072980	0.879828	0.882136	0.177267	0.073920		0.754237
7	0.915493	0.193559	0.078065	0.879828	0.895376	0.139662	0.113260		0.694915
8	0.946742	0.155523	0.103639	0.922747	0.873864	0.152520	0.095181		0.669492
9	0.962627	0.132478	0.123357	0.946352	0.891131	0.150107	0.112033		0.686441


1e-3 to 1e-2 to 1e-3 train from L287 with LS0.2

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740510	0.321465	0.033808	0.763949	0.784524	0.369006	0.049229		0.703390
1	0.808010	0.282482	0.043579	0.766094	0.866563	0.261090	0.062918		0.796610
2	0.826625	0.272935	0.049225	0.783262	0.872396	0.253645	0.057661		0.889831
3	0.849854	0.259135	0.051063	0.819743	0.888991	0.236457	0.069820		0.788136
4	0.879995	0.236762	0.060691	0.841202	0.882860	0.228335	0.065619		0.813559
5	0.865806	0.248181	0.054670	0.824034	0.877026	0.223334	0.069804		0.754237
6	0.903845	0.216614	0.070104	0.851931	0.877420	0.233406	0.062301		0.830508
7	0.914971	0.205865	0.071615	0.881974	0.884870	0.215828	0.066396		0.830508
8	0.939221	0.181632	0.088199	0.914163	0.904132	0.175651	0.087536		0.779661
9	0.953184	0.164612	0.104919	0.929185	0.895459	0.194461	0.080799		0.788136


1e-3 to 1e-2 to 1e-3 train from L287 with 20 epoch - 5 epoch warmup, 5 epoch decay	
1e-3 to 1e-2 to 1e-3 train from L287 20ep (5W 5D)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.755949	0.308439	0.035585	0.761803	0.860272	0.305775	0.044244		0.830508
1	0.809167	0.270613	0.043063	0.763949	0.882891	0.269391	0.045668		0.906780
2	0.824497	0.260509	0.044555	0.787554	0.822330	0.202081	0.079818		0.593220
3	0.844720	0.245149	0.049468	0.817597	0.887350	0.163182	0.076068		0.754237
4	0.869417	0.228485	0.054518	0.845494	0.896386	0.229833	0.065445		0.847458
5	0.885519	0.214204	0.058261	0.875537	0.898394	0.123859	0.103399		0.618644
6	0.907116	0.194011	0.067730	0.892704	0.895794	0.169557	0.074544		0.796610
7	0.923828	0.185065	0.082189	0.886266	0.906956	0.185441	0.084071		0.805085
8	0.927331	0.170048	0.080891	0.912017	0.899814	0.193931	0.066937		0.838983
9	0.940402	0.159109	0.091298	0.916309	0.910334	0.151294	0.095528		0.796610
10	0.953620	0.140531	0.105962	0.942060	0.916006	0.142147	0.113350		0.762712
11	0.958817	0.131024	0.123941	0.942060	0.912757	0.152213	0.110840		0.771186
12	0.965034	0.121483	0.130487	0.937768	0.912130	0.156549	0.101604		0.805085
13	0.964436	0.121957	0.126659	0.942060	0.909946	0.155852	0.093814		0.771186
14	0.971566	0.105543	0.139079	0.959227	0.910339	0.114831	0.134400		0.711864
15	0.971574	0.109562	0.150956	0.948498	0.903838	0.126372	0.118881		0.720339
16	0.973067	0.104589	0.164641	0.959227	0.900173	0.115490	0.126853		0.652542
17	0.975347	0.101526	0.160422	0.946352	0.902471	0.095972	0.145038		0.644068
18	0.976984	0.098858	0.175088	0.959227	0.900227	0.134356	0.112299		0.711864
19	0.979771	0.090348	0.168539	0.965665	0.904066	0.115231	0.124019		0.669492


1e-3 to 1e-2 to 1e-3 train from L287 with 20 epoch - 10 epoch warmup, 10 epoch decay
1e-3 to 1e-2 to 1e-3 train from L287 20ep(10W10D)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.734801	0.335056	0.033970	0.710300	0.857148	0.270576	0.055682		0.830508
1	0.831132	0.257517	0.047669	0.781116	0.861451	0.208420	0.055051		0.771186
2	0.828876	0.268301	0.052217	0.768240	0.848216	0.293563	0.048072		0.813559
3	0.843658	0.250553	0.049479	0.804721	0.855564	0.217311	0.067390		0.737288
4	0.866586	0.231723	0.056311	0.826180	0.886789	0.223669	0.057803		0.847458
5	0.876198	0.224324	0.059508	0.845494	0.872335	0.264068	0.050025		0.864407
6	0.888292	0.213864	0.062372	0.849785	0.895573	0.184872	0.077295		0.813559
7	0.899648	0.203617	0.065357	0.849785	0.881107	0.222621	0.066618		0.771186
8	0.899505	0.208277	0.067864	0.843348	0.866536	0.157066	0.096324		0.644068
9	0.894163	0.210874	0.064324	0.856223	0.879909	0.193015	0.067321		0.813559
10	0.911622	0.190914	0.069197	0.884120	0.887773	0.224131	0.064473		0.813559
11	0.914366	0.191744	0.076624	0.888412	0.889865	0.186623	0.078097		0.737288
12	0.936871	0.163858	0.091520	0.905579	0.890644	0.132391	0.104946		0.737288
13	0.946177	0.149202	0.091600	0.905579	0.889043	0.131891	0.104575		0.677966
14	0.952620	0.139768	0.103918	0.933476	0.887653	0.143720	0.092841		0.703390
15	0.948930	0.150048	0.105393	0.914163	0.891059	0.137995	0.104922		0.686441
16	0.970472	0.112975	0.144177	0.937768	0.894791	0.114335	0.126168		0.686441
17	0.974846	0.103898	0.156648	0.950644	0.893521	0.135152	0.117560		0.669492
18	0.978131	0.096002	0.168118	0.952790	0.892422	0.116833	0.117834		0.627119
19	0.982332	0.086108	0.178304	0.969957	0.887709	0.118020	0.123457		0.593220


1e-3 to 1e-2 to 1e-3 train from L287 with 20 epoch - 5 epoch warmup, 15 epoch decay, finetune with full at 0.1LR of train
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.730537	0.323065	0.031432	0.751073	0.811395	0.234587	0.048808		0.728814
1	0.789600	0.289790	0.040630	0.781116	0.757167	0.232212	0.049324		0.618644
2	0.829375	0.259966	0.046173	0.796137	0.875740	0.226592	0.066860		0.779661
3	0.849422	0.243109	0.047560	0.828326	0.882965	0.247292	0.050693		0.898305
4	0.860530	0.236546	0.051964	0.826180	0.891005	0.185875	0.069435		0.822034
5	0.882045	0.217441	0.057340	0.839056	0.886561	0.200254	0.073409		0.762712
6	0.897226	0.203731	0.064017	0.860515	0.890751	0.188431	0.074736		0.779661
7	0.912890	0.188771	0.071516	0.879828	0.863957	0.198738	0.065401		0.788136
8	0.907097	0.193594	0.065141	0.873391	0.892975	0.215323	0.060750		0.864407
9	0.930750	0.170933	0.085619	0.884120	0.875448	0.190360	0.065186		0.771186
10	0.935047	0.164967	0.087705	0.907725	0.899024	0.170774	0.088645		0.754237
11	0.946708	0.150642	0.103565	0.916309	0.898981	0.138357	0.115922		0.703390
12	0.957424	0.133616	0.118817	0.922747	0.891427	0.161994	0.088547		0.779661
13	0.965283	0.122066	0.131949	0.927039	0.893057	0.113219	0.122835		0.661017
14	0.970296	0.111180	0.141142	0.933476	0.891201	0.093367	0.140206		0.576271
15	0.979376	0.092914	0.165182	0.954936	0.898011	0.135580	0.113128		0.686441
16	0.979114	0.095117	0.178202	0.961373	0.895498	0.113634	0.125673		0.593220
17	0.979905	0.093585	0.169739	0.948498	0.897831	0.108571	0.137218		0.618644
18	0.985265	0.079747	0.204016	0.959227	0.899124	0.119389	0.129032		0.644068
19	0.988762	0.069514	0.227638	0.972103	0.895567	0.108780	0.141684		0.584746

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.894091	0.320571	0.100134	0.643777	0.889237	0.132555	0.093509		0.720339
1	0.877915	0.262128	0.068966	0.781116	0.857150	0.107574	0.120735		0.389830
2	0.855325	0.240790	0.052515	0.815451	0.888931	0.207713	0.061608		0.805085
3	0.923353	0.179670	0.071623	0.903434	0.895788	0.154819	0.090909		0.737288
4	0.956239	0.142532	0.094928	0.939914	0.887950	0.143086	0.096894		0.661017



1e-3 to 1e-2 to 1e-4 train from L287 with 20 epoch - 5 epoch warmup, 15 epoch decay, finetune with full at 0.1LR of train
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.729598	0.325999	0.031719	0.761803	0.808930	0.407185	0.044444		0.830508
1	0.812385	0.273037	0.044969	0.740343	0.862749	0.283678	0.053664		0.788136
2	0.824902	0.262794	0.049907	0.748927	0.866434	0.265009	0.054696		0.898305
3	0.861943	0.237035	0.056392	0.819743	0.816280	0.498364	0.043228		0.889831
4	0.862569	0.235489	0.054843	0.834764	0.879365	0.286428	0.070073		0.813559
5	0.867949	0.231886	0.056917	0.817597	0.851993	0.271242	0.052721		0.788136
6	0.888181	0.212113	0.061361	0.866953	0.844547	0.176111	0.060890		0.661017
7	0.896203	0.208324	0.068829	0.856223	0.879225	0.204652	0.069909		0.779661
8	0.911944	0.188550	0.070279	0.871245	0.853924	0.207690	0.068635		0.728814
9	0.923657	0.175048	0.076441	0.890558	0.813722	0.130645	0.066102		0.330508
10	0.925506	0.176266	0.082041	0.896996	0.859847	0.180288	0.075278		0.745763
11	0.945207	0.150270	0.093348	0.918455	0.867664	0.136376	0.098750		0.669492
12	0.947576	0.148193	0.096076	0.914163	0.872546	0.124845	0.107285		0.686441
13	0.962359	0.122505	0.122958	0.952790	0.893071	0.129559	0.098131		0.711864
14	0.966521	0.119453	0.128693	0.944206	0.884608	0.129609	0.105195		0.686441
15	0.977092	0.098158	0.152877	0.963519	0.875403	0.113489	0.120521		0.627119
16	0.980351	0.090456	0.172883	0.954936	0.886228	0.122044	0.115556		0.661017
17	0.979904	0.092567	0.170937	0.959227	0.885055	0.105001	0.128250		0.627119
18	0.981633	0.088721	0.175246	0.957082	0.885880	0.111508	0.131667		0.669492
19	0.985851	0.078390	0.180328	0.967811	0.883051	0.107704	0.132246		0.618644

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.893177	0.305971	0.100955	0.680257	0.873018	0.135977	0.088435		0.661017
1	0.908245	0.205181	0.078399	0.824034	0.822266	0.182463	0.057949		0.661017
2	0.885673	0.226024	0.063833	0.828326	0.879559	0.231945	0.057790		0.864407
3	0.944309	0.154444	0.084400	0.933476	0.898626	0.175886	0.085960		0.762712
4	0.969721	0.120785	0.113241	0.950644	0.892497	0.151047	0.093787		0.677966


1e-3 to 1e-2 to 1e-3 train from L287 with 10 epoch, finetune with full model 1e-4 to 1e-3 to 1e-4
1e-3_1e-2_1e-3 train from L287 10E, finetune 0.1LR

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.737494	0.331926	0.035355	0.723176	0.784720	0.424848	0.047671		0.737288
1	0.812840	0.271972	0.044679	0.791846	0.824778	0.214061	0.062598		0.677966
2	0.841681	0.251816	0.050466	0.789700	0.861595	0.202123	0.075646		0.694915
3	0.839054	0.257387	0.051546	0.793991	0.809535	0.221978	0.050388		0.771186
4	0.853064	0.246766	0.054340	0.804721	0.857187	0.240207	0.053523		0.830508
5	0.872084	0.231155	0.057354	0.866953	0.879698	0.248555	0.056027		0.838983
6	0.894034	0.207503	0.060415	0.843348	0.868991	0.194069	0.085106		0.745763
7	0.914662	0.187505	0.073179	0.884120	0.870296	0.167680	0.076480		0.788136
8	0.935569	0.162707	0.084131	0.905579	0.886426	0.154206	0.089268		0.754237
9	0.947940	0.149556	0.098825	0.920601	0.896300	0.151993	0.103865		0.728814

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.880901	0.245700	0.075728	0.753219	0.787773	0.201622	0.053371		0.644068
1	0.865291	0.245669	0.060379	0.793991	0.864072	0.185421	0.063187		0.779661
2	0.839009	0.249997	0.046747	0.811159	0.870062	0.210488	0.055770		0.855932
3	0.867349	0.225828	0.049047	0.877682	0.886770	0.245303	0.049812		0.898305
4	0.893168	0.205179	0.054099	0.907725	0.888286	0.225497	0.060552		0.855932


1e-3 to 1e-2 to 1e-4 train from L287 with 10 epoch, finetune with full model 1e-4 to 1e-3 to 1e-5
1e-3_1e-2_1e-4 train from L287 10E, finetune 0.1LR

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.751535	0.320048	0.035332	0.729614	0.691542	0.648981	0.033699		0.754237
1	0.842122	0.248067	0.048264	0.811159	0.819613	0.328051	0.041704		0.779661
2	0.854329	0.240431	0.051651	0.815451	0.866244	0.164864	0.084287		0.686441
3	0.856920	0.239293	0.053393	0.824034	0.882941	0.245052	0.064327		0.838983
4	0.887719	0.215242	0.062975	0.854077	0.880437	0.171186	0.072816		0.762712
5	0.893626	0.208858	0.063537	0.862661	0.884175	0.203629	0.072347		0.762712
6	0.907959	0.197785	0.074353	0.869099	0.899109	0.167782	0.085377		0.796610
7	0.935691	0.164983	0.088507	0.905579	0.895356	0.177763	0.082465		0.805085
8	0.958872	0.133183	0.110246	0.942060	0.900808	0.152037	0.094668		0.737288
9	0.969033	0.114012	0.127383	0.946352	0.911145	0.147143	0.107399		0.762712

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.877674	0.252662	0.073020	0.759657	0.858155	0.157568	0.081165		0.661017
1	0.904877	0.198838	0.072070	0.877682	0.825490	0.160261	0.066800		0.567797
2	0.869804	0.231437	0.057049	0.824034	0.881237	0.218014	0.056845		0.830508
3	0.920804	0.182125	0.070014	0.879828	0.898035	0.184521	0.083730		0.745763
4	0.959444	0.140562	0.099908	0.935622	0.904356	0.168804	0.092324		0.754237


------------------------------------------------best--------------------------------------------------------

1e-3 to 1e-2 to 1e-4 train from L287 with 10 epoch, finetune with L66 with 1e-4 to 1e-3 to 1e-5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.739364	0.325165	0.035199	0.723176	0.839993	0.310122	0.049596		0.779661
1	0.814731	0.269792	0.044705	0.768240	0.803840	0.243165	0.048618		0.805085
2	0.828077	0.258947	0.046776	0.804721	0.873619	0.247589	0.047752		0.872881
3	0.844181	0.251530	0.051932	0.787554	0.888869	0.299582	0.053668		0.923729
4	0.885165	0.217105	0.062321	0.841202	0.882497	0.195677	0.075421		0.720339
5	0.889662	0.216268	0.063213	0.856223	0.877085	0.184769	0.077132		0.720339
6	0.904283	0.199754	0.069377	0.860515	0.886727	0.146325	0.091013		0.677966
7	0.926851	0.174764	0.082038	0.894850	0.902844	0.181839	0.079038		0.779661
8	0.949366	0.147589	0.099672	0.914163	0.906462	0.139802	0.104167		0.720339
9	0.968168	0.118449	0.131618	0.944206	0.904937	0.147554	0.107053		0.720339

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.894538	0.231390	0.085372	0.763949	0.877392	0.181795	0.065280		0.771186
1	0.921218	0.181502	0.078155	0.879828	0.892342	0.135416	0.114493		0.669492
2	0.888609	0.218733	0.069073	0.804721	0.882141	0.308441	0.056977		0.830508
3	0.949474	0.151680	0.097275	0.927039	0.879192	0.137838	0.118980		0.711864
4	0.977532	0.109725	0.131357	0.969957	0.915032	0.131077	0.133433		0.754237
------------------------------------------------------------------------------------------------------------


1e-3 to 1e-2 to 1e-4 train from L287 with 10 epoch, finetune with L154 with 1e-4 to 1e-3 to 1e-5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.743805	0.329260	0.034894	0.733906	0.840568	0.291819	0.059607		0.720339
1	0.824830	0.269452	0.049702	0.768240	0.849983	1.811106	0.061238		0.796610
2	0.834502	0.257390	0.048173	0.772532	0.864165	0.212038	0.068114		0.771186
3	0.840066	0.261458	0.048644	0.800429	0.880668	0.243974	0.054126		0.855932
4	0.861851	0.239207	0.053581	0.815451	0.882677	0.221790	0.054889		0.813559
5	0.883650	0.220344	0.060797	0.847640	0.875372	0.169561	0.088174		0.720339
6	0.899584	0.200179	0.064883	0.869099	0.892010	0.146166	0.089583		0.728814
7	0.926300	0.172510	0.077599	0.890558	0.886445	0.163434	0.078415		0.788136
8	0.952502	0.140089	0.104935	0.939914	0.889482	0.146501	0.099299		0.720339
9	0.965743	0.120071	0.123703	0.946352	0.892722	0.148326	0.101449		0.711864

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.925244	0.186283	0.091959	0.854077	0.825963	0.156568	0.064035		0.618644
1	0.915532	0.188931	0.071960	0.871245	0.876402	0.136100	0.102921		0.627119
2	0.936531	0.162073	0.086207	0.912017	0.875072	0.127895	0.101626		0.635593
3	0.967557	0.117226	0.122040	0.939914	0.882759	0.144476	0.095006		0.661017
4	0.983804	0.087897	0.173262	0.967811	0.871850	0.121147	0.116013		0.601695



1e-3 to 1e-2 to 1e-4 train from L66 with 10 epoch

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.737427	0.320168	0.033838	0.776824	0.548078	0.292705	0.023788		0.220339
1	0.791721	0.276260	0.039515	0.789700	0.874904	0.240717	0.057692		0.864407
2	0.808511	0.271490	0.041789	0.796137	0.864605	0.252228	0.069498		0.762712
3	0.832592	0.261945	0.046684	0.811159	0.866442	0.252088	0.058714		0.805085
4	0.842247	0.245219	0.046584	0.832618	0.790061	3.171585	0.041258		0.855932
5	0.814177	0.266464	0.042505	0.811159	0.860725	0.294283	0.042728		0.881356
6	0.858690	0.237783	0.050536	0.819743	0.882533	0.207681	0.065407		0.762712
7	0.867990	0.225621	0.052958	0.860515	0.893166	0.205085	0.059333		0.813559
8	0.890663	0.206081	0.056503	0.881974	0.893854	0.190606	0.066955		0.788136
9	0.915282	0.185336	0.067749	0.922747	0.891923	0.180557	0.072803		0.737288


smaller architecture (d16->d8) 1e-3 to 1e-2 to 1e-4 train from L287 with 10 epoch, finetune with L66 with 1e-4 to 1e-3 to 1e-5  

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.754469	0.303373	0.035298	0.727468	0.795827	0.446328	0.055280		0.661017
1	0.816132	0.268286	0.047415	0.759657	0.878478	0.288580	0.052632		0.889831
2	0.836836	0.252576	0.048182	0.778970	0.826275	0.353294	0.036631		0.822034
3	0.846659	0.245782	0.052344	0.800429	0.883289	0.207343	0.085655		0.703390
4	0.867066	0.230979	0.056946	0.815451	0.872195	0.230067	0.060013		0.754237
5	0.887950	0.214489	0.061593	0.819743	0.883597	0.172122	0.080348		0.703390
6	0.899130	0.202608	0.064121	0.843348	0.896432	0.216199	0.065906		0.881356
7	0.932008	0.169919	0.083064	0.881974	0.900968	0.144515	0.104116		0.728814
8	0.955856	0.139782	0.113361	0.924893	0.900706	0.153942	0.100711		0.720339
9	0.968223	0.119563	0.126711	0.933476	0.903094	0.150005	0.107143		0.737288

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.901152	0.221848	0.087715	0.776824	0.843950	0.215323	0.057029		0.728814
1	0.914111	0.194502	0.081984	0.858369	0.839476	0.182841	0.087010		0.601695
2	0.907813	0.194149	0.068555	0.851931	0.890656	0.184276	0.072831		0.754237
3	0.944146	0.153323	0.081250	0.920601	0.896921	0.173644	0.089041		0.771186
4	0.975733	0.111730	0.123518	0.961373	0.898216	0.152593	0.098837		0.720339


l2(1e-4) reg in D128 and D16

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.730535	0.350634	0.031763	0.746781	0.831277	0.301273	0.033045		0.872881
1	0.827946	0.286850	0.045257	0.759657	0.742398	0.303824	0.044807		0.559322
2	0.855926	0.265681	0.053305	0.804721	0.874232	0.279806	0.048372		0.881356
3	0.877530	0.242444	0.057466	0.828326	0.895377	0.151978	0.100883		0.677966
4	0.890601	0.227675	0.062353	0.854077	0.871275	0.258837	0.059381		0.796610
5	0.887196	0.230236	0.062802	0.836910	0.882134	0.188951	0.085127		0.737288
6	0.913965	0.201406	0.073158	0.881974	0.894080	0.121060	0.138434		0.644068
7	0.927426	0.184156	0.079063	0.890558	0.892313	0.169097	0.098039		0.720339
8	0.952961	0.150827	0.104000	0.920601	0.905430	0.145000	0.115033		0.745763
9	0.969254	0.123687	0.131406	0.946352	0.904801	0.140500	0.117729		0.720339

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.906912	0.206886	0.076512	0.830472	0.880593	0.212048	0.059291		0.822034
1	0.925404	0.183088	0.079821	0.881974	0.862967	0.153072	0.098413		0.525424
2	0.920864	0.187649	0.076147	0.890558	0.862313	0.150383	0.081292		0.618644
3	0.960901	0.138016	0.106680	0.935622	0.890074	0.142116	0.113821		0.711864
4	0.979710	0.104529	0.166227	0.946352	0.889962	0.124165	0.127807		0.627119


l2(1e-4) reg in D4 and D128 and D16

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.746965	0.356597	0.036272	0.753219	0.810736	0.303005	0.050932		0.694915
1	0.831982	0.286150	0.046834	0.774678	0.849343	0.224123	0.063927		0.711864
2	0.847965	0.273152	0.051659	0.798283	0.820456	0.968324	0.069307		0.652542
3	0.865440	0.258061	0.054847	0.821888	0.883524	0.232579	0.074017		0.813559
4	0.860476	0.261970	0.054844	0.817597	0.878106	0.210284	0.078899		0.728814
5	0.860571	0.260140	0.055206	0.828326	0.882398	0.159896	0.100765		0.669492
6	0.887376	0.231851	0.062301	0.841202	0.887097	0.241007	0.068454		0.822034
7	0.912331	0.201524	0.071707	0.866953	0.899669	0.225381	0.077778		0.771186
8	0.941079	0.166906	0.094688	0.914163	0.904864	0.193010	0.095689		0.771186
9	0.951658	0.149166	0.098723	0.929185	0.905784	0.176656	0.097208		0.796610

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.884460	0.237790	0.070355	0.783262	0.819592	0.130104	0.101727		0.449153
1	0.895997	0.220846	0.070204	0.834764	0.828861	0.641764	0.051514		0.822034
2	0.908313	0.201173	0.069624	0.869099	0.865477	0.174427	0.083166		0.703390
3	0.934551	0.176364	0.083992	0.912017	0.903181	0.204670	0.087703		0.822034
4	0.975185	0.122723	0.124585	0.965665	0.892810	0.171773	0.100599		0.711864


l2(1e-3) reg in D4 and D128 and D16

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.768350	0.535866	0.035984	0.787554	0.848251	0.414959	0.054845		0.762712
1	0.823810	0.379791	0.046230	0.785408	0.848017	0.275625	0.082237		0.635593
2	0.840942	0.317069	0.050761	0.793991	0.808606	1.605693	0.046154		0.813559
3	0.844704	0.314828	0.048976	0.811159	0.861335	0.301888	0.066109		0.737288
4	0.850777	0.308519	0.054305	0.802575	0.862505	0.279652	0.053799		0.822034
5	0.867222	0.306498	0.058046	0.819743	0.880374	0.247203	0.080366		0.669492
6	0.899978	0.250968	0.068213	0.858369	0.897740	0.156046	0.206226		0.449153
7	0.923268	0.220457	0.082151	0.894850	0.882687	0.237899	0.077750		0.796610
8	0.949316	0.169497	0.099459	0.907725	0.898616	0.145913	0.107098		0.728814
9	0.965392	0.134026	0.124894	0.950644	0.899365	0.153614	0.106117		0.720339


l2(1e-5) reg in D4 and D128 and D16

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740786	0.324734	0.036013	0.710300	0.809791	0.318580	0.048342		0.728814
1	0.809016	0.277082	0.043344	0.751073	0.847855	0.255866	0.051165		0.762712
2	0.836957	0.256007	0.047119	0.789700	0.847709	0.258130	0.062004		0.728814
3	0.862059	0.240522	0.055342	0.804721	0.884902	0.152567	0.092031		0.694915
4	0.879009	0.224370	0.057715	0.841202	0.884857	0.181763	0.068130		0.762712
5	0.875771	0.229120	0.057068	0.845494	0.882850	0.215027	0.064891		0.805085
6	0.910773	0.198564	0.071703	0.881974	0.881034	0.158817	0.095758		0.669492
7	0.911591	0.197019	0.075783	0.877682	0.905390	0.196135	0.084897		0.805085
8	0.946697	0.154894	0.097794	0.922747	0.900092	0.157873	0.096296		0.771186
9	0.955693	0.142178	0.108215	0.935622	0.902404	0.156852	0.098291		0.779661


----------------------------------------------------------

decided that l2(1e-4) reg in D4 and D128 and D16 is best in terms of stability and least diff b/w trainauc and validauc

----------------------------------------------------------

finetune 1e-5_1e-4_1e-5 with 4 warmup EP

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.741455	0.349729	0.034185	0.738197	0.778768	0.501286	0.034688		0.838983
1	0.819537	0.297423	0.046889	0.781116	0.873173	0.329927	0.047354		0.864407
2	0.860246	0.264538	0.056376	0.802575	0.869503	0.282142	0.063028		0.822034
3	0.867804	0.258968	0.057786	0.815451	0.844160	0.226630	0.077001		0.644068
4	0.880211	0.247274	0.060957	0.847640	0.828608	0.165753	0.077691		0.593220
5	0.858726	0.257108	0.050360	0.811159	0.895192	0.187372	0.085151		0.694915
6	0.893468	0.226540	0.062375	0.866953	0.893831	0.198997	0.080205		0.796610
7	0.925889	0.190220	0.077954	0.873391	0.889626	0.190832	0.083948		0.771186
8	0.951033	0.155816	0.106965	0.922747	0.894901	0.157421	0.105598		0.703390
9	0.962726	0.133987	0.125000	0.933476	0.899222	0.154066	0.105521		0.728814

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.908948	0.229372	0.099187	0.759657	0.889188	0.147075	0.099744		0.661017
1	0.942015	0.166006	0.105169	0.881974	0.900773	0.164585	0.092593		0.720339
2	0.959356	0.141000	0.110825	0.924893	0.895622	0.158780	0.093182		0.694915
3	0.962285	0.136552	0.120565	0.933476	0.896913	0.154055	0.111258		0.711864
4	0.977431	0.110324	0.149188	0.946352	0.902730	0.132582	0.124242		0.694915


finetune 1e-5_1e-4_1e-5 with 3 warmup EP

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.729050	0.359102	0.033747	0.688841	0.813578	0.383584	0.064774		0.618644
1	0.822809	0.299660	0.049538	0.770386	0.800844	0.662856	0.041685		0.805085
2	0.841474	0.278316	0.052277	0.798283	0.884413	0.332997	0.052277		0.855932
3	0.865895	0.254384	0.058383	0.819743	0.886091	0.205203	0.066294		0.805085
4	0.868258	0.250999	0.056515	0.828326	0.855556	0.212343	0.059157		0.796610
5	0.878624	0.237349	0.058060	0.834764	0.878928	0.160576	0.094007		0.677966
6	0.908006	0.209258	0.073393	0.864807	0.891884	0.160362	0.097041		0.694915
7	0.924189	0.190921	0.080935	0.869099	0.878201	0.140408	0.120661		0.618644
8	0.946050	0.165817	0.099718	0.912017	0.899995	0.153231	0.109415		0.728814
9	0.965625	0.134122	0.123972	0.937768	0.900160	0.142629	0.111111		0.677966

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.907003	0.231734	0.098618	0.781116	0.878867	0.166057	0.084052		0.661017
1	0.943555	0.165962	0.103919	0.899142	0.882703	0.161562	0.091647		0.669492
2	0.960624	0.141314	0.113908	0.942060	0.873628	0.141729	0.099573		0.593220
3	0.970692	0.125637	0.129222	0.944206	0.880198	0.149339	0.104735		0.618644
4	0.982137	0.103727	0.154449	0.972103	0.885711	0.148432	0.110783		0.635593


finetune 1e-6_1e-5_1e-6 with 4 warmup EP

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.746420	0.348676	0.034469	0.740343	0.842961	0.348009	0.040075		0.906780
1	0.832939	0.282055	0.046689	0.789700	0.772819	0.239719	0.055804		0.635593
2	0.846466	0.272855	0.052551	0.806867	0.875908	0.198031	0.078292		0.745763
3	0.856799	0.262437	0.054373	0.800429	0.777704	0.474640	0.043043		0.728814
4	0.863323	0.258229	0.054990	0.834764	0.875311	0.178575	0.085170		0.720339
5	0.877159	0.240509	0.057995	0.813305	0.865409	0.178015	0.090036		0.635593
6	0.893321	0.222202	0.062122	0.860515	0.866901	0.200497	0.071778		0.745763
7	0.923274	0.192813	0.079738	0.888412	0.886913	0.129658	0.113176		0.567797
8	0.951843	0.153728	0.105341	0.918455	0.903205	0.132593	0.110807		0.686441
9	0.968952	0.121413	0.131703	0.959227	0.897907	0.149122	0.109141		0.677966

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.900641	0.254026	0.098156	0.708154	0.840187	0.155313	0.080311		0.525424
1	0.916238	0.222408	0.104332	0.759657	0.848936	0.167256	0.079439		0.576271
2	0.928728	0.194744	0.107335	0.813305	0.858075	0.167817	0.081655		0.618644
3	0.934789	0.181240	0.107094	0.839056	0.865938	0.144466	0.093750		0.610169
4	0.941398	0.169399	0.108876	0.871245	0.867595	0.141254	0.093176		0.601695



------------------------------------------------------------------------------------------------------
After dataset creation update, LR 1e-3 to 1e-2 to 1e-4 with l2reg (1e-4) on 3 dense layers, and stage2 with 3 epoch warmup and with 0.1LR

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.756457	0.339716	0.036079	0.751073	0.837646	0.369851	0.044360		0.889831
1	0.815656	0.299101	0.046314	0.746781	0.877965	0.281927	0.061818		0.864407
2	0.829180	0.282704	0.045749	0.802575	0.873585	0.393937	0.049289		0.881356
3	0.855569	0.262863	0.055238	0.809013	0.863158	0.229114	0.061276		0.822034
4	0.860268	0.259582	0.056920	0.815451	0.819356	0.589080	0.050619		0.796610
5	0.845125	0.271364	0.051750	0.802575	0.864839	0.188560	0.082136		0.677966
6	0.882757	0.233310	0.058911	0.845494	0.893319	0.213089	0.065942		0.822034
7	0.911003	0.205237	0.071168	0.877682	0.900759	0.187078	0.079610		0.830508
8	0.939607	0.170336	0.092560	0.907725	0.903689	0.157035	0.101925		0.762712
9	0.956450	0.144458	0.115815	0.933476	0.905307	0.152620	0.111111		0.745763

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.901349	0.221164	0.082648	0.819743	0.879825	0.142076	0.089758		0.661017
1	0.916411	0.194154	0.078619	0.864807	0.879287	0.217032	0.066291		0.796610
2	0.919889	0.188968	0.076880	0.884120	0.870874	0.157881	0.081612		0.669492
3	0.905096	0.209565	0.073646	0.860515	0.888957	0.186669	0.071055		0.805085
4	0.930475	0.181998	0.075589	0.916309	0.895929	0.194852	0.080205		0.796610


After dataset creation update, LR 1e-3 to 1e-2 to 1e-4 with l2reg (1e-4) on 3 dense layers, and stage2 with 4 epoch warmup and with 0.1LR

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.736958	0.347074	0.032921	0.759657	0.826679	0.384315	0.055420		0.771186
1	0.821425	0.288400	0.045944	0.787554	0.876574	0.219492	0.071651		0.779661
2	0.828512	0.287536	0.047916	0.757511	0.812667	0.282552	0.044265		0.788136
3	0.847886	0.274727	0.057984	0.802575	0.874343	0.256371	0.057669		0.822034
4	0.875693	0.245464	0.061496	0.821888	0.868499	0.193409	0.068362		0.728814
5	0.859189	0.259879	0.052133	0.815451	0.888523	0.184482	0.082936		0.737288
6	0.886375	0.233317	0.060741	0.847640	0.877455	0.177013	0.089186		0.677966
7	0.911182	0.214199	0.072103	0.869099	0.900042	0.198540	0.081010		0.788136
8	0.943012	0.167946	0.093612	0.912017	0.901251	0.120252	0.120000		0.635593
9	0.956582	0.147104	0.107826	0.931330	0.898420	0.165791	0.095815		0.737288

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.901500	0.222523	0.081327	0.804721	0.894760	0.158437	0.087125		0.762712
1	0.934782	0.173607	0.084680	0.888412	0.846350	0.160999	0.074866		0.593220
2	0.912751	0.198164	0.075076	0.851931	0.881709	0.167090	0.091109		0.703390
3	0.912476	0.201938	0.074763	0.862661	0.867212	0.173976	0.075000		0.686441
4	0.957071	0.149751	0.100207	0.933476	0.899557	0.159762	0.107239		0.677966



Without Tabular input -> dense(4) with LR 1e-3 to 1e-2 to 1e-4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.723489	0.360661	0.031414	0.744635	0.808905	0.213877	0.071281		0.584746
1	0.832341	0.284629	0.045272	0.796137	0.765216	0.330448	0.033026		0.728814
2	0.849334	0.265948	0.050392	0.800429	0.872577	0.232846	0.055071		0.855932
3	0.864904	0.250259	0.052168	0.839056	0.886312	0.291660	0.045788		0.898305
4	0.873742	0.244839	0.056193	0.830472	0.869124	0.187790	0.077578		0.694915
5	0.894217	0.225359	0.064459	0.851931	0.894053	0.205718	0.083015		0.737288
6	0.914067	0.200984	0.072479	0.877682	0.863474	0.335152	0.042783		0.906780
7	0.921786	0.193894	0.076173	0.888412	0.871740	0.167185	0.086279		0.703390
8	0.951032	0.152121	0.099377	0.924893	0.907042	0.138179	0.110553		0.745763
9	0.962785	0.132817	0.110999	0.942060	0.903766	0.150441	0.100111		0.762712


again same

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.757950	0.337392	0.033201	0.789700	0.832507	0.181971	0.078086		0.525424
1	0.802824	0.307537	0.041094	0.796137	0.839632	0.275978	0.044364		0.830508
2	0.839441	0.276423	0.045868	0.832618	0.824848	0.273236	0.043843		0.796610
3	0.843235	0.281065	0.051055	0.804721	0.882697	0.251507	0.079096		0.711864
4	0.852845	0.273827	0.049782	0.834764	0.879847	0.185350	0.093708		0.593220
5	0.874912	0.253485	0.057521	0.847640	0.861517	0.214297	0.061634		0.754237
6	0.888011	0.231423	0.060507	0.875537	0.886519	0.193278	0.076068		0.754237
7	0.921689	0.194242	0.075779	0.907725	0.899439	0.186240	0.081308		0.822034
8	0.941200	0.165795	0.087560	0.933476	0.899531	0.161650	0.092386		0.771186
9	0.960097	0.140598	0.109531	0.942060	0.901901	0.162279	0.099435		0.745763


full train - settings same as above

	auc			loss		precision	recall
0	0.764989	0.328454	0.036045	0.760274
1	0.842815	0.278580	0.050506	0.785959
2	0.864511	0.256204	0.057291	0.813356
3	0.876023	0.241506	0.059846	0.825342
4	0.886776	0.229450	0.062630	0.821918
5	0.869418	0.260301	0.055518	0.840753
6	0.888638	0.244179	0.060539	0.849315
7	0.921239	0.201241	0.078204	0.876712
8	0.947892	0.161428	0.097054	0.919521
9	0.960507	0.137463	0.110324	0.933219

private score : 0.8723, public score : 0.8840



dropout 0.4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.717414	0.367418	0.032555	0.727468	0.805925	0.548483	0.046334		0.771186
1	0.827725	0.294745	0.049130	0.793991	0.881750	0.257544	0.058270		0.838983
2	0.822080	0.292981	0.046668	0.802575	0.844809	0.219894	0.073582		0.703390
3	0.855379	0.271578	0.052924	0.817597	0.881397	0.230498	0.074894		0.745763
4	0.876923	0.247156	0.057973	0.809013	0.866961	0.231307	0.064214		0.754237
5	0.879122	0.239361	0.060303	0.828326	0.904570	0.154568	0.098940		0.711864
6	0.902374	0.217467	0.067164	0.869099	0.894410	0.180408	0.072363		0.796610
7	0.922515	0.195979	0.075673	0.899142	0.894853	0.148486	0.100599		0.711864
8	0.944173	0.166782	0.098745	0.912017	0.897873	0.175767	0.094512		0.788136
9	0.964930	0.133938	0.123249	0.944206	0.894587	0.162505	0.109069		0.754237


dropout 0.4 LR 1e-3_1e-2_1e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.730829	0.362229	0.033383	0.727468	0.794151	0.388695	0.050790		0.762712
1	0.803364	0.313434	0.045081	0.776824	0.844788	0.255693	0.067315		0.703390
2	0.840150	0.282203	0.050131	0.781116	0.828657	0.228656	0.070577		0.652542
3	0.856149	0.269929	0.053640	0.804721	0.846377	0.234019	0.062404		0.686441
4	0.869142	0.251184	0.055798	0.824034	0.895652	0.263457	0.057860		0.898305
5	0.889644	0.229598	0.060496	0.854077	0.890273	0.232650	0.070896		0.805085
6	0.907405	0.207825	0.067650	0.875537	0.885410	0.209572	0.060373		0.796610
7	0.915571	0.202458	0.074270	0.873391	0.901326	0.201592	0.079299		0.805085
8	0.950316	0.156951	0.103374	0.927039	0.901616	0.144688	0.106061		0.711864
9	0.961091	0.139647	0.123404	0.933476	0.903143	0.152729	0.100578		0.737288


saturation 0.7, 1.3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.720578	0.368829	0.033547	0.708154	0.825300	0.458595	0.042010		0.864407
1	0.827681	0.291079	0.047407	0.800429	0.868708	0.286952	0.053163		0.847458
2	0.805303	0.306287	0.041443	0.746781	0.822179	0.280814	0.045966		0.796610
3	0.850029	0.276542	0.051268	0.811159	0.877247	0.235589	0.062500		0.805085
4	0.859247	0.262494	0.051188	0.836910	0.884262	0.215430	0.077181		0.779661
5	0.882178	0.247306	0.058485	0.849785	0.881273	0.178401	0.096894		0.661017
6	0.896601	0.226369	0.065854	0.869099	0.893001	0.156712	0.097156		0.694915
7	0.926361	0.191404	0.077150	0.899142	0.879003	0.167877	0.082700		0.737288
8	0.941187	0.172454	0.090655	0.903434	0.895975	0.145266	0.104798		0.703390
9	0.960387	0.142958	0.115415	0.946352	0.895760	0.147931	0.100350		0.728814


saturation 0.5, 1.5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.731714	0.355658	0.033826	0.727468	0.850447	0.314518	0.051651		0.822034
1	0.821075	0.299228	0.046855	0.776824	0.861001	0.276412	0.065434		0.779661
2	0.847427	0.281558	0.053372	0.821888	0.874196	0.425085	0.057946		0.855932
3	0.833186	0.286384	0.045449	0.796137	0.866728	0.244459	0.059673		0.805085
4	0.864911	0.257300	0.055701	0.819743	0.861618	0.207419	0.082377		0.669492
5	0.867736	0.250378	0.053088	0.828326	0.889851	0.198524	0.084259		0.771186
6	0.885694	0.230601	0.060076	0.877682	0.895645	0.209646	0.071480		0.838983
7	0.909009	0.210933	0.069536	0.884120	0.889558	0.178076	0.090000		0.686441
8	0.938914	0.173379	0.090948	0.920601	0.897181	0.177109	0.084762		0.754237
9	0.961278	0.136908	0.117033	0.937768	0.897687	0.142015	0.097254		0.720339

saturation 0.5, 1.5 again

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.758782	0.340858	0.039038	0.721030	0.768939	0.655839	0.053302		0.567797
1	0.810987	0.302237	0.045495	0.759657	0.834554	0.316779	0.044719		0.796610
2	0.835125	0.282383	0.049133	0.802575	0.844150	0.220351	0.097983		0.576271
3	0.851409	0.270222	0.052508	0.813305	0.873208	0.296326	0.058824		0.898305
4	0.857543	0.271293	0.053383	0.802575	0.889223	0.205342	0.091981		0.661017
5	0.875996	0.253570	0.058788	0.834764	0.889952	0.253790	0.059941		0.855932
6	0.900055	0.224258	0.066802	0.847640	0.874695	0.205394	0.060361		0.822034
7	0.922493	0.195646	0.078703	0.890558	0.900109	0.190593	0.078577		0.805085
8	0.938608	0.175215	0.093996	0.896996	0.897429	0.140701	0.106061		0.652542
9	0.959005	0.142326	0.116329	0.935622	0.897986	0.126809	0.118644		0.652542


saturation 0.8, 1.2

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.752029	0.344879	0.034136	0.763949	0.845523	0.278615	0.048711		0.864407
1	0.811184	0.303537	0.044174	0.770386	0.832950	0.276426	0.053959		0.779661
2	0.843711	0.277615	0.052460	0.793991	0.873121	0.262603	0.063087		0.796610
3	0.826296	0.297309	0.046897	0.783262	0.814991	0.344471	0.032781		0.838983
4	0.856568	0.263329	0.054325	0.789700	0.855304	0.286436	0.043199		0.855932
5	0.866327	0.251824	0.052833	0.830472	0.866274	0.214978	0.098972		0.652542
6	0.896532	0.227629	0.068488	0.849785	0.883796	0.185756	0.082759		0.711864
7	0.924160	0.195253	0.079626	0.894850	0.883848	0.180273	0.084586		0.762712
8	0.941556	0.172360	0.094667	0.914163	0.891169	0.173646	0.092998		0.720339
9	0.960093	0.144439	0.112819	0.929185	0.896352	0.146129	0.109756		0.686441


saturation 0.9, 1.1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.736090	0.355532	0.033941	0.718884	0.857991	0.299859	0.054264		0.830508
1	0.806230	0.303618	0.043126	0.766094	0.728274	0.490316	0.034374		0.830508
2	0.846783	0.274765	0.051001	0.798283	0.885806	0.261138	0.060160		0.830508
3	0.861958	0.259480	0.052916	0.839056	0.853166	0.227359	0.064715		0.711864
4	0.828971	0.283517	0.045960	0.806867	0.848986	0.300100	0.066453		0.703390
5	0.871988	0.251713	0.055894	0.826180	0.880335	0.173306	0.099476		0.644068
6	0.880603	0.243848	0.059236	0.834764	0.887967	0.244431	0.066051		0.788136
7	0.893978	0.223547	0.060363	0.871245	0.875262	0.236524	0.060936		0.805085
8	0.920294	0.195591	0.074870	0.896996	0.887899	0.262811	0.082272		0.711864
9	0.945107	0.163048	0.093492	0.924893	0.896734	0.209423	0.083184		0.788136


saturation 0.8, 1.5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.749916	0.348657	0.035855	0.740343	0.816876	0.348513	0.039778		0.788136
1	0.820456	0.293670	0.047039	0.761803	0.830227	0.342822	0.054499		0.728814
2	0.851339	0.272886	0.051415	0.806867	0.857628	0.276737	0.048000		0.864407
3	0.864691	0.255965	0.053401	0.839056	0.868353	0.332276	0.085682		0.644068
4	0.853006	0.271139	0.051050	0.824034	0.876033	0.235487	0.058407		0.838983
5	0.884853	0.240630	0.061228	0.845494	0.884054	0.181591	0.078112		0.771186
6	0.903681	0.215227	0.065881	0.875537	0.903023	0.142934	0.111255		0.728814
7	0.930805	0.183649	0.083267	0.901288	0.888540	0.168181	0.090367		0.771186
8	0.959946	0.141586	0.115293	0.939914	0.875966	0.146062	0.101430		0.661017
9	0.967374	0.125922	0.130949	0.950644	0.897662	0.119096	0.124417		0.677966

saturation 0.8, 1.5 again

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.731267	0.358535	0.032688	0.789700	0.800093	0.232680	0.076223		0.567797
1	0.807914	0.309327	0.040271	0.804721	0.851315	0.391931	0.037196		0.881356
2	0.845709	0.278523	0.049107	0.826180	0.849941	0.236775	0.066347		0.703390
3	0.857370	0.265294	0.050800	0.824034	0.850215	0.227415	0.052836		0.805085
4	0.857746	0.263670	0.050566	0.824034	0.866814	0.184531	0.069914		0.754237
5	0.875066	0.252516	0.054796	0.860515	0.845564	0.240566	0.054886		0.694915
6	0.893011	0.232038	0.063022	0.858369	0.880425	0.216356	0.063492		0.779661
7	0.920770	0.198335	0.074126	0.909871	0.886765	0.204833	0.072917		0.771186
8	0.940700	0.170601	0.083445	0.931330	0.896492	0.151376	0.092677		0.686441
9	0.956509	0.145076	0.098569	0.931330	0.890398	0.164863	0.091097		0.745763



saturation 1.0, 1.5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.730615	0.352826	0.033330	0.733906	0.836259	0.247875	0.054391		0.703390
1	0.827069	0.290761	0.045449	0.789700	0.852627	0.332434	0.047222		0.864407
2	0.838553	0.287436	0.049451	0.802575	0.849702	0.303669	0.056364		0.788136
3	0.866013	0.257791	0.055395	0.824034	0.865094	0.256997	0.059615		0.788136
4	0.864795	0.265220	0.055865	0.817597	0.875953	0.215703	0.075928		0.745763
5	0.882561	0.240434	0.057302	0.860515	0.890355	0.223341	0.061736		0.855932
6	0.903135	0.222811	0.071127	0.866953	0.878106	0.248838	0.064238		0.822034
7	0.919183	0.206174	0.079655	0.871245	0.898347	0.220274	0.075358		0.847458
8	0.951561	0.155676	0.104922	0.937768	0.898915	0.155427	0.103666		0.694915
9	0.960437	0.142699	0.123861	0.933476	0.899754	0.152904	0.105528		0.711864




Using saturation 1.0, 1.5 for now 

CLAHE with clip_limit=0.01

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.724970	0.363374	0.032985	0.703863	0.799358	0.334594	0.038945		0.788136
1	0.810915	0.304576	0.045365	0.738197	0.828407	0.325470	0.059923		0.788136
2	0.839564	0.285904	0.052893	0.800429	0.858207	0.276413	0.058278		0.745763
3	0.870863	0.254418	0.057486	0.841202	0.879607	0.179671	0.095293		0.703390
4	0.886741	0.236250	0.061026	0.849785	0.863494	0.231222	0.070579		0.754237
5	0.890688	0.230242	0.060463	0.851931	0.875322	0.235262	0.058426		0.805085
6	0.902510	0.219383	0.068489	0.871245	0.892422	0.152589	0.110193		0.677966
7	0.933079	0.182403	0.087117	0.914163	0.877438	0.157290	0.087196		0.669492
8	0.955103	0.149553	0.107872	0.929185	0.886631	0.155034	0.097814		0.720339
9	0.969551	0.122337	0.140640	0.952790	0.876358	0.130703	0.117355		0.601695


CLAHE with clip_limit=5e-3 with clip before&after

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.729654	0.357176	0.034176	0.742489	0.810559	0.522135	0.036808		0.855932
1	0.823706	0.296573	0.046114	0.804721	0.838967	0.216062	0.073149		0.694915
2	0.834977	0.285374	0.049840	0.802575	0.833182	0.245277	0.058591		0.754237
3	0.861111	0.260372	0.053659	0.819743	0.838356	0.252136	0.070845		0.661017
4	0.844389	0.272891	0.051003	0.813305	0.873153	0.310725	0.043174		0.889831
5	0.879402	0.238310	0.057930	0.843348	0.886929	0.252308	0.054802		0.889831
6	0.899343	0.220687	0.068127	0.847640	0.870702	0.204802	0.081614		0.737288
7	0.918546	0.199897	0.079246	0.884120	0.878117	0.178702	0.083749		0.711864
8	0.946529	0.163807	0.101505	0.912017	0.890128	0.131810	0.124615		0.686441
9	0.963057	0.136381	0.129295	0.920601	0.897851	0.116703	0.134948		0.661017


CLAHE with clip_limit=5e-3 with clip before&after LR 3e-3_1e-2_3e-3
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.721571	0.357083	0.031361	0.723176	0.745573	1.131984	0.037154		0.796610
1	0.793362	0.317454	0.040953	0.748927	0.858339	0.348348	0.041128		0.889831
2	0.818144	0.299114	0.048383	0.783262	0.867615	0.236910	0.077922		0.711864
3	0.855108	0.265158	0.052155	0.828326	0.868633	0.224851	0.069145		0.788136
4	0.866537	0.259088	0.052675	0.824034	0.856635	0.235790	0.062187		0.737288
5	0.879227	0.248359	0.059477	0.849785	0.855620	0.193276	0.070819		0.703390
6	0.883739	0.252195	0.060292	0.849785	0.874589	0.166903	0.093525		0.661017
7	0.906432	0.228448	0.070923	0.881974	0.884476	0.269789	0.077261		0.745763
8	0.929700	0.200906	0.083267	0.899142	0.892164	0.198419	0.093288		0.694915
9	0.947154	0.170776	0.101646	0.914163	0.891207	0.178136	0.089662		0.720339


saturation 1.2, 1.5 without CLAHE

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.761167	0.330207	0.035504	0.748927	0.833611	0.376093	0.052876		0.771186
1	0.824209	0.290358	0.048499	0.776824	0.832896	0.340212	0.048604		0.796610
2	0.848001	0.273639	0.049921	0.809013	0.874732	0.243612	0.069510		0.745763
3	0.867230	0.253643	0.054210	0.826180	0.868250	0.243705	0.061370		0.805085
4	0.876290	0.252631	0.058639	0.843348	0.875315	0.255738	0.048685		0.847458
5	0.875227	0.247545	0.056797	0.845494	0.876988	0.297413	0.049953		0.898305
6	0.895026	0.238501	0.066622	0.856223	0.890502	0.238737	0.070433		0.813559
7	0.918436	0.209292	0.073916	0.892704	0.894147	0.187591	0.088975		0.779661
8	0.945910	0.169546	0.101891	0.924893	0.901382	0.143788	0.113869		0.661017
9	0.959698	0.143833	0.121305	0.933476	0.902772	0.150533	0.105660		0.711864


saturation 1.2, 1.5 modified aug method

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.769961	0.328909	0.035866	0.813305	0.844904	0.278510	0.066879		0.711864
1	0.824560	0.289156	0.045664	0.763949	0.882429	0.276925	0.065052		0.796610
2	0.852625	0.268983	0.053210	0.800429	0.843500	0.250106	0.058978		0.762712
3	0.863336	0.264088	0.055821	0.826180	0.888102	0.294469	0.049097		0.898305
4	0.882331	0.242481	0.059955	0.856223	0.878323	0.204749	0.080897		0.703390
5	0.888721	0.242111	0.064142	0.843348	0.847518	0.262045	0.053664		0.788136
6	0.901244	0.232945	0.070798	0.860515	0.884567	0.186943	0.082652		0.771186
7	0.919530	0.202431	0.074982	0.873391	0.884260	0.178586	0.085437		0.745763
8	0.954127	0.154948	0.103083	0.918455	0.899681	0.168299	0.092119		0.762712
9	0.969631	0.123857	0.133434	0.946352	0.907166	0.139209	0.113402		0.745763


CLAHE with clip_limit=5e-3 with clip before&after with replace_zeros with saturation 1.2, 1.5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.744127	0.353201	0.037177	0.738197	0.748092	0.579202	0.040549		0.576271
1	0.820680	0.300298	0.046939	0.761803	0.869075	0.310587	0.042410		0.906780
2	0.858064	0.269492	0.054191	0.828326	0.872284	0.254851	0.055620		0.813559
3	0.860118	0.261934	0.050319	0.830472	0.862088	0.211651	0.085366		0.652542
4	0.886576	0.247654	0.064277	0.839056	0.863567	0.170746	0.085620		0.661017
5	0.876149	0.245229	0.054960	0.847640	0.853417	0.194839	0.069420		0.669492
6	0.890478	0.243325	0.063559	0.869099	0.886990	0.189073	0.091319		0.686441
7	0.924635	0.194364	0.076866	0.890558	0.862909	0.137957	0.095989		0.567797
8	0.947112	0.161653	0.098833	0.927039	0.891574	0.135874	0.109459		0.686441
9	0.964075	0.133822	0.119440	0.933476	0.892737	0.118397	0.119874		0.644068


CLAHE with clip_limit=5e-3 with clip before&after with add_small_random with saturation 1.2, 1.5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.727981	0.353666	0.033225	0.701717	0.792266	0.759229	0.054396		0.618644
1	0.807759	0.300767	0.047218	0.746781	0.843568	0.222484	0.070812		0.694915
2	0.862609	0.262814	0.055776	0.813305	0.859354	0.176195	0.094148		0.627119
3	0.841561	0.290616	0.053623	0.800429	0.844377	0.262105	0.061422		0.754237
4	0.864460	0.268154	0.058948	0.834764	0.872530	0.223874	0.070804		0.754237
5	0.877487	0.249837	0.062185	0.821888	0.860874	0.223230	0.060339		0.754237
6	0.910937	0.212219	0.074121	0.873391	0.880904	0.155423	0.100132		0.644068
7	0.930811	0.187278	0.084173	0.914163	0.864627	0.223717	0.072156		0.720339
8	0.947815	0.163802	0.098706	0.916309	0.888477	0.112000	0.131387		0.610169
9	0.969840	0.123993	0.141587	0.957082	0.883879	0.134398	0.107776		0.669492


age_approx scaled

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.741112	0.349615	0.033529	0.697425	0.835267	0.400554	0.041058		0.881356
1	0.816029	0.298595	0.047172	0.776824	0.867185	0.295865	0.057681		0.830508
2	0.853935	0.272725	0.055095	0.813305	0.862185	0.283106	0.044975		0.830508
3	0.869088	0.256805	0.056911	0.826180	0.891159	0.228982	0.066291		0.796610
4	0.873323	0.249562	0.056658	0.845494	0.892062	0.211199	0.078553		0.754237
5	0.884100	0.234614	0.057560	0.828326	0.887741	0.180138	0.084008		0.703390
6	0.906534	0.210873	0.071186	0.899142	0.866991	0.129810	0.118859		0.635593
7	0.925482	0.212698	0.082571	0.890558	0.886749	0.169407	0.104922		0.686441
8	0.942855	0.183384	0.094828	0.920601	0.898834	0.144096	0.108497		0.703390
9	0.966563	0.141319	0.120906	0.950644	0.898545	0.144442	0.113703		0.661017


age_approx scaled again

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.730848	0.354868	0.032305	0.712446	0.797364	0.302987	0.045309		0.720339
1	0.794240	0.315173	0.045905	0.725322	0.849540	0.317800	0.044796		0.838983
2	0.841833	0.279565	0.052843	0.791846	0.864632	0.347955	0.048918		0.881356
3	0.849937	0.274895	0.051378	0.796137	0.866400	0.288399	0.046305		0.838983
4	0.871898	0.252616	0.059718	0.826180	0.845686	0.209341	0.063345		0.754237
5	0.884360	0.239128	0.059857	0.826180	0.861736	0.247497	0.054991		0.779661
6	0.903890	0.218059	0.069627	0.864807	0.888552	0.148320	0.100765		0.669492
7	0.928705	0.188229	0.080832	0.884120	0.893570	0.157514	0.098765		0.745763
8	0.951966	0.155581	0.105238	0.931330	0.897470	0.152580	0.095402		0.703390
9	0.969009	0.123977	0.132092	0.959227	0.901668	0.141805	0.112861		0.728814


age_approx scaled full train

	auc			loss		precision	recall
0	0.736511	0.349758	0.032967	0.765411
1	0.826873	0.294608	0.046343	0.787671
2	0.842908	0.284267	0.050998	0.813356
3	0.850645	0.272813	0.050614	0.804795
4	0.858615	0.265913	0.054630	0.803082
5	0.860524	0.261993	0.055457	0.801370
6	0.886200	0.238755	0.063056	0.825342
7	0.916255	0.207239	0.073900	0.888699
8	0.934388	0.180894	0.085374	0.919521
9	0.950248	0.155676	0.102795	0.938356

private score : 0.8709, public score : 0.8824


no batch norm for only tabular

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.731345	0.359111	0.034105	0.714592	0.833089	0.339540	0.041919		0.822034
1	0.799645	0.307721	0.042199	0.729614	0.864380	0.250930	0.062889		0.771186
2	0.836303	0.280011	0.046141	0.819743	0.852180	0.273761	0.055381		0.745763
3	0.825219	0.292496	0.050515	0.757511	0.867318	0.254901	0.064808		0.788136
4	0.863805	0.253697	0.055670	0.811159	0.866394	0.251490	0.071906		0.728814
5	0.884843	0.235224	0.065631	0.843348	0.853943	0.281174	0.064140		0.745763
6	0.881933	0.235963	0.058426	0.854077	0.837585	0.468121	0.065287		0.694915
7	0.914948	0.207629	0.073954	0.884120	0.901471	0.176728	0.082569		0.762712
8	0.943049	0.168220	0.093640	0.909871	0.903719	0.155379	0.102959		0.737288
9	0.965492	0.132667	0.123464	0.948498	0.903031	0.160798	0.107275		0.737288


no batch norm for only tabular dropout 0.3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.728619	0.352764	0.032227	0.727468	0.819953	0.334584	0.042124		0.813559
1	0.817863	0.295612	0.048945	0.746781	0.847851	0.315311	0.046392		0.838983
2	0.845079	0.279645	0.055683	0.783262	0.860801	0.303660	0.040971		0.915254
3	0.853119	0.266803	0.050564	0.798283	0.758893	0.408448	0.038132		0.754237
4	0.877612	0.244609	0.061436	0.828326	0.876317	0.221494	0.077484		0.720339
5	0.896062	0.222984	0.067341	0.843348	0.881796	0.221754	0.075707		0.771186
6	0.902974	0.213497	0.064671	0.839056	0.886201	0.221985	0.066261		0.830508
7	0.928635	0.184866	0.081134	0.896996	0.888332	0.149687	0.102564		0.711864
8	0.955220	0.148226	0.114719	0.909871	0.891157	0.137719	0.111262		0.694915
9	0.969341	0.122844	0.134515	0.944206	0.899370	0.113753	0.123377		0.644068


saturation up and down  1.1-1.4/0.6-0.9

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.716813	0.386593	0.033435	0.684549	0.781153	0.417677	0.042417		0.618644
1	0.815138	0.299937	0.045577	0.761803	0.871378	0.336064	0.044758		0.940678
2	0.843949	0.279944	0.054493	0.806867	0.869619	0.320657	0.054236		0.889831
3	0.862862	0.263932	0.056717	0.809013	0.851755	0.229209	0.071867		0.694915
4	0.847990	0.276434	0.053702	0.787554	0.865676	0.332772	0.074718		0.728814
5	0.876534	0.244936	0.057842	0.847640	0.876549	0.247487	0.058570		0.805085
6	0.890800	0.229680	0.063340	0.839056	0.895711	0.148933	0.115108		0.677966
7	0.921860	0.194277	0.077109	0.890558	0.876269	0.200748	0.065440		0.813559
8	0.942761	0.165771	0.087818	0.903434	0.893360	0.150740	0.093946		0.762712
9	0.956910	0.143237	0.106020	0.933476	0.904143	0.155878	0.100228		0.745763


zoomin and zoomout (only height. width will be changed equally to match aspect ratio) zoomin : (-0.2, -0.1) zoomout : (0.1, 0.2)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.733860	0.356273	0.034272	0.733906	0.810435	0.369088	0.039847		0.796610
1	0.815112	0.296992	0.043458	0.783262	0.855938	0.284752	0.053514		0.838983
2	0.843777	0.276195	0.050294	0.806867	0.872768	0.205108	0.091629		0.686441
3	0.857786	0.260283	0.051204	0.834764	0.888480	0.231480	0.072486		0.788136
4	0.861072	0.259145	0.052487	0.819743	0.875721	0.316125	0.044418		0.957627
5	0.874493	0.258566	0.058035	0.845494	0.884607	0.254389	0.072044		0.779661
6	0.903857	0.218995	0.065529	0.873391	0.887146	0.204311	0.071094		0.771186
7	0.913742	0.204252	0.071218	0.881974	0.883448	0.195756	0.075972		0.728814
8	0.943625	0.165083	0.095228	0.929185	0.900525	0.134835	0.102464		0.669492
9	0.959592	0.142287	0.116989	0.920601	0.889846	0.157445	0.089109		0.686441

above again

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.739166	0.343861	0.035795	0.744635	0.712028	0.231644	0.054415		0.449153
1	0.811069	0.301289	0.049644	0.733906	0.815368	0.364156	0.039901		0.822034
2	0.823521	0.291474	0.048662	0.772532	0.769013	0.334129	0.067844		0.618644
3	0.842231	0.274961	0.052183	0.787554	0.876937	0.426076	0.069351		0.788136
4	0.852526	0.262399	0.049846	0.798283	0.864425	0.233105	0.080722		0.720339
5	0.863403	0.250349	0.054107	0.832618	0.874129	0.307471	0.043863		0.923729
6	0.887480	0.229546	0.058930	0.841202	0.876068	0.208495	0.073442		0.728814
7	0.921421	0.193942	0.076125	0.881974	0.870774	0.169475	0.089701		0.686441
8	0.942925	0.165473	0.090482	0.909871	0.890669	0.176938	0.087262		0.737288
9	0.960433	0.138843	0.113061	0.939914	0.882952	0.149008	0.114493		0.669492

above again

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.731547	0.360448	0.033251	0.725322	0.841832	0.290545	0.062180		0.720339
1	0.829952	0.288305	0.048083	0.783262	0.811060	0.280958	0.049703		0.779661
2	0.819868	0.300976	0.046450	0.796137	0.862506	0.372736	0.057243		0.830508
3	0.839701	0.276658	0.047395	0.800429	0.872341	0.288935	0.045953		0.923729
4	0.863911	0.254624	0.052503	0.830472	0.856525	0.226331	0.060758		0.720339
5	0.849050	0.270497	0.052033	0.815451	0.835795	0.255155	0.051057		0.838983
6	0.882687	0.234645	0.059505	0.871245	0.877095	0.185964	0.075770		0.771186
7	0.906240	0.208883	0.066038	0.886266	0.895693	0.154192	0.088799		0.745763
8	0.935658	0.175737	0.085028	0.896996	0.901071	0.149618	0.107143		0.711864
9	0.956786	0.144937	0.113271	0.937768	0.901556	0.151673	0.105327		0.737288


zoomin and zoomout with constant fill

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.751979	0.334241	0.032868	0.787554	0.805982	0.286980	0.050761		0.593220
1	0.824958	0.287261	0.042722	0.796137	0.877233	0.304060	0.051307		0.898305
2	0.838554	0.276280	0.050228	0.804721	0.861152	0.332295	0.043423		0.872881
3	0.854489	0.261862	0.049908	0.817597	0.866592	0.282918	0.044293		0.881356
4	0.842175	0.271983	0.047451	0.809013	0.867554	0.313628	0.050339		0.881356
5	0.875863	0.241291	0.056296	0.851931	0.844088	0.289691	0.041298		0.830508
6	0.892266	0.228724	0.063273	0.856223	0.886902	0.184712	0.087379		0.762712
7	0.918323	0.199749	0.073313	0.888412	0.886604	0.182944	0.073380		0.796610
8	0.939386	0.172764	0.087762	0.924893	0.878191	0.180979	0.082342		0.762712
9	0.955538	0.148088	0.107821	0.937768	0.890996	0.137621	0.112601		0.711864


zoom - (-0.2, 0.2) nearest fill

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.746254	0.344220	0.035785	0.740343	0.817415	0.471989	0.039034		0.822034
1	0.822740	0.290689	0.044896	0.768240	0.838244	0.461492	0.063427		0.652542
2	0.835401	0.282625	0.050793	0.776824	0.870864	0.291079	0.051619		0.864407
3	0.859964	0.259393	0.053091	0.834764	0.886321	0.210221	0.072173		0.762712
4	0.869792	0.251456	0.057746	0.845494	0.847706	0.252407	0.046081		0.822034
5	0.873163	0.256393	0.059348	0.828326	0.886799	0.251973	0.065217		0.864407
6	0.908525	0.210945	0.066444	0.854077	0.884346	0.192091	0.077855		0.762712
7	0.932968	0.181179	0.083801	0.896996	0.889277	0.149175	0.107570		0.686441
8	0.948312	0.160324	0.099838	0.924893	0.892496	0.140798	0.111111		0.686441
9	0.969372	0.122890	0.135609	0.946352	0.894097	0.137838	0.112376		0.669492


zoom - (-0.2, 0.2) constant fill

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.756179	0.337376	0.036484	0.766094	0.803666	0.373684	0.033126		0.813559
1	0.824361	0.290359	0.046542	0.772532	0.853387	0.282200	0.054958		0.822034
2	0.848636	0.270231	0.052061	0.793991	0.843682	0.285210	0.047523		0.796610
3	0.842557	0.269629	0.050581	0.793991	0.854161	0.225315	0.070312		0.762712
4	0.854272	0.264932	0.052459	0.789700	0.885735	0.259666	0.062243		0.898305
5	0.889508	0.227462	0.061221	0.854077	0.868858	0.238462	0.055426		0.805085
6	0.903254	0.213354	0.069879	0.877682	0.872358	0.191676	0.087379		0.686441
7	0.921038	0.194650	0.078535	0.892704	0.873936	0.198362	0.066520		0.771186
8	0.951477	0.153474	0.095449	0.927039	0.883221	0.162165	0.086100		0.703390
9	0.968643	0.123377	0.128205	0.944206	0.886588	0.142863	0.093552		0.627119


original zoom - (-0.2, 0.2), (-0.2, 0.2) fill : nearest

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.743700	0.356474	0.034110	0.778970	0.778370	0.351975	0.045660		0.771186
1	0.819061	0.300775	0.044613	0.796137	0.834720	0.302890	0.054945		0.720339
2	0.855972	0.269772	0.057240	0.830472	0.843367	0.203362	0.065801		0.644068
3	0.847664	0.277786	0.053698	0.813305	0.847404	0.484610	0.034731		0.940678
4	0.845990	0.272389	0.051932	0.830472	0.848550	0.174325	0.083221		0.525424
5	0.878290	0.254045	0.061982	0.849785	0.812211	0.360952	0.038052		0.847458
6	0.868906	0.248319	0.054545	0.843348	0.858171	0.204852	0.066935		0.703390
7	0.900426	0.218446	0.068300	0.873391	0.875914	0.188218	0.080247		0.771186
8	0.932571	0.181804	0.084064	0.884120	0.890629	0.167999	0.085687		0.771186
9	0.956292	0.146164	0.105834	0.942060	0.889677	0.158619	0.097643		0.737288


RandomBrightness - currently 0.15 - change to (0.05, 0.15) and (-0.15, -0.05)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.720499	0.366087	0.030434	0.772532	0.836620	0.323746	0.040748		0.830508
1	0.812320	0.302495	0.042899	0.787554	0.874230	0.283914	0.053403		0.864407
2	0.851864	0.276559	0.053127	0.811159	0.883668	0.274855	0.062500		0.805085
3	0.853823	0.267908	0.052759	0.839056	0.883896	0.293547	0.070588		0.762712
4	0.866225	0.259146	0.053248	0.819743	0.882562	0.172132	0.103967		0.644068
5	0.886495	0.234742	0.059478	0.871245	0.875672	0.138247	0.114428		0.584746
6	0.896230	0.222235	0.067256	0.864807	0.881111	0.217062	0.067358		0.771186
7	0.920239	0.196841	0.077235	0.899142	0.878061	0.213884	0.069016		0.796610
8	0.939206	0.171367	0.085165	0.931330	0.879828	0.166130	0.072639		0.762712
9	0.954177	0.151954	0.103760	0.935622	0.888610	0.150146	0.095074		0.703390


max 2 augmentations probabilistic with numpy random int    - 16m 28s

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.735200	0.363934	0.031699	0.757511	0.800185	0.378692	0.048638		0.635593
1	0.816162	0.294328	0.043632	0.796137	0.836959	0.270215	0.049073		0.762712
2	0.839867	0.280529	0.052515	0.813305	0.867319	0.248597	0.068038		0.728814
3	0.868335	0.254589	0.055320	0.841202	0.871989	0.193243	0.075401		0.677966
4	0.874163	0.243411	0.055288	0.862661	0.882232	0.204328	0.074232		0.737288
5	0.885035	0.233913	0.060649	0.830472	0.878570	0.214589	0.076990		0.754237
6	0.894108	0.223724	0.064322	0.847640	0.894454	0.198637	0.079796		0.796610
7	0.916355	0.198902	0.075275	0.881974	0.894132	0.210224	0.070392		0.822034
8	0.937528	0.171677	0.085446	0.918455	0.896441	0.153374	0.098737		0.728814
9	0.957318	0.141325	0.108981	0.927039	0.897405	0.130926	0.110482		0.661017


same again													- 16m 36s

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.727126	0.346814	0.030971	0.774678	0.793022	0.364456	0.045023		0.762712
1	0.794897	0.307335	0.037394	0.783262	0.825552	0.338700	0.040316		0.822034
2	0.839812	0.278619	0.049889	0.817597	0.862472	0.266545	0.061790		0.737288
3	0.848906	0.274054	0.054241	0.806867	0.679363	0.988891	0.031488		0.771186
4	0.841750	0.278501	0.048511	0.828326	0.867771	0.214560	0.070132		0.720339
5	0.875328	0.244953	0.057424	0.839056	0.889671	0.170652	0.097893		0.669492
6	0.903928	0.213595	0.066362	0.871245	0.895696	0.189720	0.086957		0.762712
7	0.917452	0.194222	0.070069	0.892704	0.880582	0.198459	0.080110		0.737288
8	0.937592	0.172435	0.089207	0.909871	0.882366	0.158986	0.095710		0.737288
9	0.952559	0.147209	0.105695	0.939914	0.888190	0.159828	0.092715		0.711864


above but with tf random int 								- 16m 41s
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.716924	0.361909	0.030097	0.768240	0.824069	0.296030	0.045862		0.779661
1	0.809128	0.304750	0.041592	0.800429	0.854643	0.252639	0.058376		0.779661
2	0.840764	0.280423	0.047232	0.824034	0.873083	0.314766	0.054671		0.872881
3	0.852612	0.264699	0.050467	0.800429	0.869142	0.245072	0.060218		0.796610
4	0.865669	0.254593	0.055437	0.839056	0.875271	0.274350	0.068254		0.728814
5	0.852983	0.272054	0.052348	0.813305	0.886522	0.217177	0.071257		0.754237
6	0.879792	0.238771	0.057630	0.871245	0.876305	0.196265	0.076190		0.745763
7	0.901185	0.214820	0.068227	0.881974	0.881081	0.171465	0.095074		0.703390
8	0.931285	0.183645	0.082835	0.920601	0.892183	0.170646	0.091274		0.771186
9	0.946920	0.160731	0.098646	0.922747	0.898831	0.152914	0.108886		0.737288


with contrast and hue

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.766528	0.330630	0.034846	0.802575	0.829666	0.284609	0.054417		0.720339
1	0.820363	0.298760	0.045230	0.787554	0.805785	0.323075	0.048634		0.754237
2	0.859879	0.262784	0.053633	0.798283	0.817184	0.245062	0.061983		0.635593
3	0.867030	0.250448	0.054714	0.821888	0.827309	0.405392	0.039394		0.881356
4	0.890229	0.231630	0.064066	0.828326	0.821638	0.161396	0.075055		0.576271
5	0.899813	0.218765	0.065256	0.873391	0.876172	0.173145	0.085595		0.694915
6	0.924867	0.193374	0.077567	0.894850	0.878642	0.165696	0.108497		0.703390
7	0.947511	0.168155	0.098222	0.924893	0.873119	0.158814	0.097442		0.677966
8	0.968396	0.129371	0.132953	0.950644	0.876090	0.134838	0.114809		0.584746
9	0.984902	0.088055	0.189223	0.972103	0.868970	0.104254	0.137168		0.525424


with contrast and hue, with 2 apply_flip

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.741136	0.346532	0.034540	0.763949	0.809189	0.223152	0.070664		0.559322
1	0.810536	0.300457	0.044730	0.776824	0.766823	0.501952	0.030954		0.830508
2	0.828887	0.284627	0.048319	0.811159	0.856102	0.568878	0.063069		0.762712
3	0.840537	0.289804	0.052032	0.793991	0.845889	0.224510	0.062926		0.703390
4	0.870367	0.250251	0.056882	0.843348	0.845292	0.279677	0.049322		0.771186
5	0.904901	0.213857	0.067252	0.871245	0.851227	0.162071	0.075526		0.669492
6	0.914943	0.203582	0.072209	0.879828	0.859924	0.178359	0.078923		0.720339
7	0.944855	0.164836	0.093094	0.914163	0.845475	0.101767	0.108511		0.432203
8	0.966616	0.130647	0.119978	0.942060	0.853978	0.109109	0.125000		0.550847
9	0.982206	0.092966	0.170778	0.965665	0.820309	0.082080	0.151815		0.389830


with rot click, rot anticlock

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.725954	0.353895	0.030488	0.793991	0.803489	0.184569	0.137363		0.211864
1	0.798932	0.314803	0.043044	0.785408	0.862374	0.235977	0.062000		0.788136
2	0.840335	0.278060	0.047121	0.841202	0.869416	0.242258	0.061069		0.813559
3	0.846600	0.269179	0.050163	0.824034	0.865563	0.260827	0.046284		0.881356
4	0.857061	0.261409	0.051910	0.828326	0.852553	0.266407	0.057210		0.830508
5	0.857793	0.257631	0.049402	0.832618	0.844254	0.146200	0.088060		0.500000
6	0.878510	0.236430	0.056492	0.856223	0.863979	0.261254	0.066952		0.796610
7	0.906622	0.203934	0.066340	0.901288	0.878957	0.127638	0.104825		0.533898
8	0.924440	0.184127	0.073356	0.909871	0.878899	0.163727	0.078995		0.745763
9	0.940908	0.167430	0.086231	0.909871	0.880352	0.142707	0.089224		0.652542


with rot click, rot anticlock again

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.721990	0.355768	0.032473	0.721030	0.788791	0.435990	0.043518		0.805085
1	0.813101	0.299288	0.044119	0.802575	0.836360	0.270837	0.047595		0.813559
2	0.828372	0.288457	0.046359	0.804721	0.859541	0.307676	0.060102		0.796610
3	0.854220	0.261635	0.051938	0.845494	0.860308	0.305926	0.054666		0.838983
4	0.859516	0.251347	0.049798	0.847640	0.868336	0.279643	0.052522		0.855932
5	0.863400	0.253229	0.051335	0.849785	0.862399	0.286614	0.052466		0.847458
6	0.881095	0.249447	0.059253	0.871245	0.857396	0.234044	0.069346		0.745763
7	0.906743	0.214587	0.066860	0.888412	0.870860	0.219009	0.071954		0.745763
8	0.924072	0.191907	0.077502	0.884120	0.871660	0.193127	0.079663		0.720339
9	0.946901	0.158993	0.096099	0.935622	0.879931	0.170089	0.084168		0.711864


with rot click, rot anticlock - 20 epoch

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.741445	0.345246	0.034416	0.800429	0.852149	0.298251	0.045433		0.805085
1	0.814660	0.294408	0.042400	0.821888	0.835374	0.307300	0.046501		0.838983
2	0.858534	0.264560	0.051592	0.834764	0.851391	0.318345	0.039878		0.889831
3	0.875053	0.249701	0.060157	0.836910	0.867573	0.144044	0.118492		0.559322
4	0.873295	0.248372	0.055422	0.843348	0.876920	0.201606	0.069391		0.762712
5	0.892479	0.230758	0.064772	0.841202	0.874408	0.280804	0.067883		0.788136
6	0.908639	0.210333	0.068816	0.884120	0.858422	0.215671	0.067783		0.720339
7	0.912405	0.204431	0.070151	0.875537	0.832602	0.155015	0.091473		0.500000
8	0.931512	0.187367	0.084991	0.907725	0.867190	0.118471	0.137592		0.474576
9	0.943191	0.175528	0.092507	0.903434	0.856451	0.099240	0.155556		0.474576
10	0.939807	0.180754	0.090052	0.922747	0.869355	0.154768	0.096401		0.635593
11	0.959862	0.155841	0.115282	0.929185	0.852960	0.131063	0.101010		0.508475
12	0.964110	0.143715	0.131595	0.942060	0.871481	0.127029	0.114901		0.542373
13	0.977344	0.116190	0.164333	0.957082	0.775709	0.117497	0.144044		0.440678
14	0.982101	0.104823	0.186985	0.974249	0.860221	0.095493	0.166667		0.381356
15	0.988948	0.080431	0.228168	0.969957	0.866440	0.085027	0.146341		0.355932
16	0.991667	0.067712	0.268065	0.987124	0.861769	0.088387	0.167286		0.381356
17	0.992854	0.061203	0.283323	0.980687	0.842838	0.102184	0.146853		0.533898
18	0.995915	0.047361	0.332852	0.989270	0.854537	0.086045	0.158784		0.398305
19	0.997066	0.041366	0.373484	0.991416	0.856989	0.075779	0.153846		0.288136


with rot click, rot anticlock , no hue

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.754240	0.333604	0.035506	0.731760	0.862761	0.358760	0.038923		0.906780
1	0.794053	0.305810	0.041950	0.740343	0.813128	0.301170	0.054677		0.703390
2	0.824255	0.283935	0.044189	0.783262	0.856534	0.315034	0.056626		0.822034
3	0.845563	0.267794	0.052143	0.819743	0.856238	0.250862	0.056638		0.788136
4	0.862466	0.254490	0.053889	0.813305	0.867468	0.224272	0.070411		0.754237
5	0.859913	0.262115	0.055710	0.817597	0.874413	0.253783	0.069486		0.779661
6	0.899469	0.228838	0.068830	0.864807	0.868016	0.172142	0.112977		0.627119
7	0.930234	0.191918	0.080576	0.888412	0.884351	0.157332	0.107843		0.652542
8	0.957706	0.151685	0.109765	0.933476	0.851688	0.147192	0.098918		0.542373
9	0.971757	0.120073	0.148760	0.965665	0.869337	0.088672	0.144262		0.372881



identity removed from aug_func_1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.731872	0.359389	0.034038	0.759657	0.833328	0.489225	0.038419		0.872881
1	0.814570	0.298850	0.045637	0.778970	0.781956	0.306610	0.043606		0.754237
2	0.825138	0.290320	0.045941	0.811159	0.867765	0.300580	0.046592		0.915254
3	0.846572	0.275934	0.049501	0.798283	0.864316	0.236011	0.069841		0.745763
4	0.872128	0.251111	0.056740	0.845494	0.857686	0.240514	0.075075		0.635593
5	0.883039	0.239507	0.057286	0.832618	0.873387	0.208755	0.083799		0.635593
6	0.916898	0.202902	0.075210	0.862661	0.865069	0.215915	0.066958		0.779661
7	0.936314	0.181897	0.083530	0.912017	0.878783	0.123946	0.122776		0.584746
8	0.955935	0.151787	0.102180	0.935622	0.875343	0.105548	0.119374		0.516949
9	0.979192	0.104590	0.154422	0.974249	0.832848	0.093797	0.139651		0.474576


zoomout removed

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.748947	0.339558	0.035918	0.744635	0.782655	0.779536	0.043416		0.771186
1	0.835451	0.282061	0.046749	0.806867	0.875033	0.270418	0.074722		0.796610
2	0.846465	0.278039	0.051174	0.785408	0.853504	0.294884	0.044350		0.881356
3	0.860497	0.260841	0.053115	0.843348	0.882109	0.210741	0.055556		0.847458
4	0.870832	0.247155	0.055807	0.847640	0.869154	0.318057	0.050098		0.864407
5	0.875050	0.250429	0.057411	0.826180	0.876452	0.163254	0.091435		0.669492
6	0.894669	0.219780	0.060462	0.875537	0.889307	0.148663	0.097324		0.677966
7	0.910004	0.203651	0.068428	0.894850	0.871157	0.145480	0.089820		0.635593
8	0.931225	0.176099	0.080779	0.916309	0.884803	0.126382	0.110791		0.652542
9	0.951686	0.146754	0.101254	0.935622	0.886956	0.157866	0.086509		0.711864


no batch norm at the end (with augmentations rot click and rot anticlock and hue and zoomout and identity and others not previously removed)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.711530	0.393073	0.030084	0.785408	0.831880	0.343787	0.037828		0.855932
1	0.809284	0.327607	0.040624	0.787554	0.873617	0.278654	0.045435		0.906780
2	0.866480	0.263724	0.053145	0.841202	0.867277	0.225037	0.063481		0.754237
3	0.869655	0.254165	0.052695	0.843348	0.890881	0.244218	0.056345		0.906780
4	0.859710	0.285251	0.051623	0.832618	0.864065	0.197515	0.094682		0.618644
5	0.889861	0.245977	0.065022	0.836910	0.857829	0.191134	0.069725		0.644068
6	0.912961	0.212296	0.071764	0.884120	0.872708	0.183196	0.093897		0.677966
7	0.940950	0.174775	0.090811	0.903434	0.884410	0.196860	0.072289		0.762712
8	0.970808	0.122594	0.127526	0.961373	0.885391	0.128866	0.089286		0.635593
9	0.981537	0.093726	0.160651	0.974249	0.876400	0.110158	0.119454		0.593220


no zoomout, brightness up and down 0.05 to 0.1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.704412	0.390268	0.033323	0.697425	0.801450	0.388055	0.033757		0.855932
1	0.805646	0.307697	0.044533	0.740343	0.852141	0.328396	0.059110		0.822034
2	0.828023	0.289610	0.047554	0.821888	0.857519	0.226249	0.078362		0.745763
3	0.842189	0.276395	0.050798	0.826180	0.821187	0.258752	0.054760		0.745763
4	0.855367	0.260190	0.051272	0.839056	0.870662	0.205564	0.084103		0.694915
5	0.861298	0.251931	0.051430	0.841202	0.791467	0.299302	0.041851		0.720339
6	0.887488	0.227375	0.059391	0.858369	0.862821	0.184503	0.079523		0.677966
7	0.900737	0.216557	0.066460	0.873391	0.852861	0.194547	0.071313		0.745763
8	0.922459	0.189353	0.073060	0.899142	0.863358	0.182880	0.078295		0.669492
9	0.943686	0.163192	0.091852	0.931330	0.870397	0.166435	0.086903		0.601695


contrast 1.1_1.25 0.75_0.9

		auc		loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.723798	0.347117	0.029806	0.763949	0.818782	0.284694	0.064838		0.661017
1	0.804919	0.307337	0.043147	0.789700	0.835722	0.287651	0.057602		0.728814
2	0.835903	0.284464	0.051593	0.785408	0.838800	0.259002	0.062789		0.576271
3	0.853641	0.266195	0.056779	0.813305	0.845944	0.278992	0.058272		0.737288
4	0.862276	0.257402	0.055308	0.826180	0.843882	0.339730	0.049489		0.779661
5	0.839133	0.276583	0.047799	0.843348	0.871089	0.227653	0.052493		0.847458
6	0.873880	0.245587	0.059145	0.860515	0.863413	0.170606	0.104167		0.508475
7	0.891629	0.231989	0.065609	0.860515	0.876113	0.150587	0.103208		0.627119
8	0.925738	0.193269	0.075881	0.901288	0.884876	0.146796	0.091319		0.686441
9	0.936890	0.175264	0.084007	0.914163	0.885353	0.148576	0.096154		0.720339


contrast 0.75_0.9 - no contrast up

		auc		loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.725501	0.362851	0.033575	0.766094	0.804793	0.219749	0.065748		0.525424
1	0.831761	0.285510	0.045929	0.830472	0.864021	0.240441	0.069424		0.796610
2	0.845445	0.272862	0.047756	0.817597	0.862994	0.210807	0.069401		0.745763
3	0.851541	0.270915	0.051494	0.828326	0.866595	0.199437	0.078244		0.694915
4	0.860651	0.254684	0.051876	0.821888	0.845418	0.211358	0.064257		0.677966
5	0.865544	0.244836	0.054007	0.864807	0.862640	0.296514	0.048709		0.847458
6	0.877473	0.239996	0.058153	0.843348	0.873305	0.183244	0.086595		0.618644
7	0.907919	0.206132	0.068017	0.886266	0.864208	0.150788	0.104377		0.525424
8	0.922878	0.189995	0.077514	0.888412	0.865882	0.117789	0.108007		0.491525
9	0.945081	0.161330	0.090392	0.920601	0.866290	0.117493	0.114603		0.525424


contrast 1.1_1.25 0.75_0.9, rotation 0.2 to 0.3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.720110	0.364034	0.031466	0.751073	0.671632	0.469343	0.028065		0.805085
1	0.788448	0.317780	0.038437	0.785408	0.856434	0.293149	0.050710		0.847458
2	0.841362	0.280337	0.047679	0.809013	0.866338	0.302522	0.051088		0.915254
3	0.864750	0.261077	0.054070	0.806867	0.865269	0.236305	0.073111		0.762712
4	0.867952	0.253755	0.053145	0.830472	0.886825	0.263863	0.076588		0.745763
5	0.883308	0.237635	0.062835	0.856223	0.862888	0.227566	0.082247		0.694915
6	0.912186	0.209186	0.070870	0.875537	0.863649	0.189028	0.083953		0.669492
7	0.935328	0.178452	0.084759	0.903434	0.872207	0.166996	0.088816		0.686441
8	0.961489	0.136959	0.116322	0.942060	0.854142	0.177046	0.086406		0.635593
9	0.978329	0.100588	0.168581	0.976395	0.842691	0.143743	0.113006		0.449153


above with brightness 0.05 to 0.1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.747149	0.343326	0.035470	0.744635	0.829605	0.382040	0.046310		0.813559
1	0.828023	0.289374	0.048666	0.798283	0.838746	0.326351	0.051665		0.762712
2	0.855367	0.264042	0.054084	0.828326	0.772749	0.336731	0.041053		0.661017
3	0.860573	0.263536	0.054382	0.845494	0.843688	0.242710	0.058524		0.779661
4	0.871767	0.256108	0.057506	0.864807	0.830641	0.213743	0.069324		0.677966
5	0.888735	0.250318	0.062461	0.860515	0.849564	0.167062	0.075279		0.686441
6	0.926021	0.194804	0.075191	0.909871	0.851543	0.150180	0.091824		0.618644
7	0.934880	0.185959	0.089415	0.899142	0.852620	0.143095	0.102524		0.550847
8	0.967897	0.130519	0.133742	0.935622	0.843188	0.088014	0.143345		0.355932
9	0.982309	0.092065	0.181090	0.969957	0.824632	0.102301	0.125628		0.423729


contrast 1.1_1.25 0.75_0.9, rotation 0.1 to 0.3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.741687	0.353995	0.034510	0.738197	0.748803	0.336471	0.036580		0.677966
1	0.824945	0.292066	0.048011	0.800429	0.832592	0.296136	0.059305		0.737288
2	0.838085	0.288116	0.052258	0.789700	0.799948	0.232651	0.066738		0.525424
3	0.870207	0.257543	0.059370	0.836910	0.859445	0.277909	0.055288		0.779661
4	0.879213	0.241188	0.060397	0.862661	0.845826	0.173301	0.071125		0.567797
5	0.889975	0.230050	0.063386	0.873391	0.842696	0.164303	0.084605		0.516949
6	0.914719	0.198835	0.071267	0.879828	0.854202	0.132568	0.093883		0.559322
7	0.948681	0.159625	0.102157	0.924893	0.856128	0.147325	0.092757		0.618644
8	0.969772	0.121977	0.135378	0.937768	0.847377	0.114143	0.105263		0.474576
9	0.984116	0.090501	0.183842	0.957082	0.824246	0.096416	0.121884		0.372881

above with aug1, aug2 swapped

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740298	0.361466	0.035390	0.736051	0.812120	0.344743	0.040567		0.872881
1	0.821084	0.296233	0.048133	0.766094	0.826227	0.289721	0.045345		0.796610
2	0.846291	0.276133	0.050868	0.817597	0.729497	0.863515	0.036334		0.745763
3	0.861964	0.263169	0.057165	0.804721	0.844716	0.381393	0.041259		0.822034
4	0.868858	0.249961	0.058104	0.826180	0.842503	0.292650	0.049947		0.796610
5	0.879188	0.249585	0.062229	0.830472	0.866632	0.327709	0.039657		0.940678
6	0.895678	0.225984	0.064440	0.873391	0.863096	0.206640	0.076217		0.703390
7	0.917990	0.197763	0.073797	0.869099	0.878054	0.180286	0.079024		0.686441
8	0.944423	0.164617	0.095356	0.907725	0.879750	0.197688	0.075993		0.745763
9	0.961746	0.134948	0.118585	0.942060	0.878324	0.154533	0.090323		0.711864


saturation 1.1_1.25 0.75_0.9

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.742304	0.357169	0.032522	0.800429	0.825763	0.259926	0.053968		0.720339
1	0.821304	0.298507	0.044276	0.776824	0.841551	0.299330	0.052572		0.788136
2	0.861554	0.263335	0.050557	0.847640	0.866259	0.204134	0.079646		0.686441
3	0.870911	0.257180	0.055470	0.854077	0.835446	0.364041	0.053258		0.720339
4	0.881194	0.244603	0.060090	0.862661	0.863055	0.243041	0.062624		0.805085
5	0.888095	0.235235	0.062228	0.860515	0.853602	0.240715	0.053369		0.838983
6	0.912342	0.206501	0.068977	0.866953	0.837766	0.242200	0.055143		0.754237
7	0.940089	0.171175	0.091425	0.933476	0.846380	0.157618	0.080574		0.618644
8	0.968343	0.125114	0.127623	0.952790	0.856055	0.126765	0.102421		0.466102
9	0.979206	0.098791	0.166975	0.967811	0.854983	0.120181	0.103512		0.474576


above with no_identity in aug2

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.723488	0.370871	0.031881	0.690987	0.761733	0.363378	0.048724		0.711864
1	0.802436	0.304575	0.042732	0.753219	0.824929	0.334303	0.032937		0.915254
2	0.821421	0.290286	0.045488	0.785408	0.844205	0.335506	0.048246		0.838983
3	0.852832	0.269300	0.055090	0.817597	0.864607	0.232140	0.068855		0.728814
4	0.869077	0.254314	0.058986	0.826180	0.870982	0.227356	0.061362		0.771186
5	0.862641	0.253502	0.052949	0.811159	0.866957	0.226835	0.072961		0.720339
6	0.888808	0.231458	0.063518	0.836910	0.885807	0.204378	0.071158		0.796610
7	0.917572	0.201323	0.074226	0.894850	0.869166	0.173267	0.084325		0.720339
8	0.931363	0.183541	0.082672	0.894850	0.888435	0.184383	0.084762		0.754237
9	0.961171	0.139628	0.111736	0.937768	0.891698	0.160225	0.099886		0.745763


saturation 1.1_1.4 0.6_0.9

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.739875	0.355753	0.033662	0.761803	0.778433	0.268701	0.061883		0.584746
1	0.805074	0.307710	0.043714	0.778970	0.826923	0.219613	0.069102		0.593220
2	0.832434	0.288595	0.045760	0.804721	0.869402	0.233590	0.069914		0.754237
3	0.857355	0.263534	0.051955	0.832618	0.861891	0.181509	0.074811		0.669492
4	0.859668	0.259197	0.051818	0.834764	0.857883	0.238729	0.075437		0.694915
5	0.859750	0.262089	0.054035	0.817597	0.868649	0.259558	0.058824		0.771186
6	0.888544	0.233612	0.059569	0.847640	0.858405	0.227854	0.063194		0.771186
7	0.907281	0.216367	0.067330	0.886266	0.856447	0.124803	0.104712		0.508475
8	0.927494	0.188090	0.076579	0.881974	0.854648	0.141498	0.090909		0.601695
9	0.945650	0.160973	0.090130	0.924893	0.859428	0.132862	0.086902		0.584746


saturation 1.1_1.3 0.7_0.9

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.728767	0.359494	0.032872	0.733906	0.709946	0.576183	0.029851		0.322034
1	0.808117	0.300693	0.043406	0.783262	0.860989	0.254019	0.069558		0.720339
2	0.832456	0.282432	0.048780	0.824034	0.861152	0.336155	0.040746		0.906780
3	0.857160	0.258705	0.051453	0.843348	0.825851	0.321672	0.047170		0.805085
4	0.854375	0.256710	0.050125	0.858369	0.857339	0.191141	0.079511		0.661017
5	0.867495	0.247259	0.053804	0.849785	0.852850	0.267436	0.057194		0.805085
6	0.879882	0.235361	0.057364	0.873391	0.855966	0.243277	0.058596		0.771186
7	0.909079	0.206812	0.067506	0.896996	0.878268	0.201378	0.072588		0.720339
8	0.924438	0.189375	0.076537	0.884120	0.883844	0.173779	0.090431		0.728814
9	0.948449	0.156024	0.099407	0.935622	0.878676	0.232647	0.076547		0.796610


saturation 1.1_1.25 0.75_0.9 again

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.718869	0.370466	0.031079	0.757511	0.803946	0.246823	0.052524		0.652542
1	0.812183	0.298079	0.042602	0.789700	0.843627	0.263314	0.052316		0.813559
2	0.815888	0.299032	0.045732	0.802575	0.829014	0.454051	0.041449		0.872881
3	0.851789	0.271039	0.053099	0.809013	0.854200	0.312486	0.043239		0.864407
4	0.842806	0.270592	0.049892	0.841202	0.867787	0.253622	0.062952		0.737288
5	0.870317	0.243871	0.055422	0.843348	0.873908	0.316547	0.045852		0.889831
6	0.880737	0.233678	0.056684	0.856223	0.853416	0.248260	0.060298		0.754237
7	0.907490	0.210377	0.071279	0.877682	0.868117	0.174553	0.074043		0.737288
8	0.919483	0.192805	0.075981	0.905579	0.880235	0.184256	0.079380		0.737288
9	0.942066	0.164450	0.092430	0.924893	0.877967	0.162248	0.084938		0.635593


above with brightness 0.05 to 0.1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.764301	0.340789	0.037085	0.740343	0.785189	0.316723	0.047514		0.728814
1	0.824126	0.294998	0.046416	0.772532	0.854990	0.274449	0.058041		0.813559
2	0.842726	0.281425	0.049919	0.796137	0.839984	0.246331	0.061123		0.728814
3	0.866135	0.259113	0.058407	0.832618	0.824711	0.215972	0.067837		0.720339
4	0.829497	0.284726	0.048158	0.793991	0.882260	0.233036	0.073021		0.805085
5	0.869733	0.246625	0.057646	0.815451	0.869883	0.276315	0.051010		0.855932
6	0.894020	0.219082	0.061340	0.858369	0.872289	0.202442	0.080448		0.669492
7	0.919603	0.195513	0.075339	0.871245	0.884324	0.174421	0.084713		0.737288
8	0.939413	0.171663	0.090596	0.903434	0.882041	0.160969	0.090811		0.711864
9	0.948288	0.155071	0.104882	0.931330	0.889914	0.180210	0.080979		0.728814


age without scaling with batchnorm

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.726311	0.364533	0.032433	0.718884	0.813142	0.324336	0.049696		0.762712
1	0.816837	0.301569	0.049005	0.766094	0.823164	0.314467	0.052595		0.635593
2	0.839980	0.284574	0.051462	0.785408	0.871074	0.243266	0.080252		0.754237
3	0.847993	0.274429	0.050361	0.793991	0.874769	0.246497	0.066904		0.796610
4	0.852402	0.269978	0.052624	0.789700	0.856850	0.223582	0.073191		0.728814
5	0.874684	0.246473	0.060356	0.830472	0.879464	0.189225	0.092945		0.703390
6	0.902857	0.221371	0.069211	0.856223	0.893076	0.184422	0.082714		0.754237
7	0.919032	0.199530	0.076252	0.881974	0.869615	0.142601	0.117005		0.635593
8	0.938612	0.173521	0.088514	0.901288	0.895456	0.174005	0.086576		0.754237
9	0.958814	0.143337	0.112720	0.935622	0.889040	0.154440	0.092379		0.677966


age with scaling with batchnorm

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.737473	0.355595	0.033846	0.755365	0.807326	0.373859	0.045703		0.779661
1	0.800342	0.309598	0.040375	0.766094	0.849704	0.257400	0.079051		0.677966
2	0.834806	0.290772	0.051362	0.781116	0.856319	0.260062	0.062411		0.745763
3	0.861625	0.262128	0.057217	0.800429	0.881714	0.196224	0.081951		0.711864
4	0.870156	0.253441	0.059697	0.819743	0.860295	0.300210	0.052284		0.805085
5	0.882671	0.239748	0.061720	0.828326	0.853139	0.132159	0.110915		0.533898
6	0.893726	0.229019	0.065113	0.845494	0.893348	0.188710	0.085520		0.745763
7	0.913765	0.202658	0.071669	0.869099	0.883363	0.159020	0.093363		0.703390
8	0.940210	0.170040	0.088584	0.914163	0.898463	0.161407	0.097802		0.754237
9	0.950342	0.154389	0.101402	0.931330	0.894506	0.187309	0.086538		0.762712


train on full data and submit (submission name : age with scaling with batchnorm, with new augment)

	auc			loss		precision	recall
0	0.767540	0.325126	0.035084	0.803082
1	0.828766	0.289804	0.048037	0.806507
2	0.843162	0.276921	0.050708	0.791096
3	0.852041	0.266434	0.050161	0.827055
4	0.870566	0.249834	0.054633	0.811644
5	0.871521	0.254235	0.059200	0.823630
6	0.883113	0.242054	0.060048	0.849315
7	0.904956	0.216047	0.064338	0.876712
8	0.919136	0.196960	0.071008	0.886986
9	0.939530	0.169501	0.084722	0.914384

private score : 0.8625, public score : 0.8824



without training=False in base model

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.690737	0.371104	0.028999	0.723176	0.793618	0.279152	0.045748		0.720339
1	0.807714	0.302857	0.044026	0.787554	0.836185	0.350967	0.040520		0.898305
2	0.838081	0.281999	0.049557	0.791846	0.878904	0.351913	0.044948		0.923729
3	0.858660	0.261152	0.052580	0.802575	0.814460	0.338681	0.033546		0.889831
4	0.865757	0.250705	0.054020	0.839056	0.842018	0.187276	0.064488		0.618644
5	0.878448	0.242929	0.061851	0.843348	0.867122	0.176798	0.073029		0.745763
6	0.861830	0.265959	0.054604	0.834764	0.868590	0.189179	0.080847		0.711864
7	0.895940	0.224208	0.064821	0.854077	0.876257	0.183355	0.072051		0.771186
8	0.919649	0.193944	0.079191	0.899142	0.885927	0.164289	0.085174		0.686441
9	0.934384	0.176218	0.088131	0.909871	0.890831	0.176011	0.079792		0.779661


age with scaling with batchnorm again save model

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.744768	0.347072	0.033270	0.785408	0.835005	0.294503	0.053955		0.745763
1	0.800280	0.314425	0.042392	0.778970	0.826939	0.318022	0.045898		0.796610
2	0.842873	0.282279	0.052792	0.781116	0.874597	0.284792	0.055876		0.822034
3	0.852002	0.276726	0.052531	0.824034	0.849372	0.198457	0.078629		0.661017
4	0.870306	0.258231	0.057369	0.841202	0.859804	0.260659	0.062176		0.813559
5	0.884967	0.242874	0.058688	0.873391	0.862214	0.165818	0.074600		0.711864
6	0.912035	0.210617	0.067960	0.879828	0.860290	0.129269	0.118217		0.516949
7	0.933358	0.184778	0.083812	0.907725	0.883263	0.112771	0.114082		0.542373
8	0.964318	0.135806	0.124718	0.948498	0.858013	0.094496	0.130081		0.406780
9	0.979509	0.097925	0.170093	0.976395	0.790024	0.087004	0.154079		0.432203


train-valid save model with TTA valid

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.723307	0.366187	0.033004	0.695279	0.843502	0.322216	0.046682		0.864407
1	0.839612	0.286896	0.052901	0.776824	0.836062	0.290946	0.069136		0.711864
2	0.847320	0.275658	0.050572	0.815451	0.854076	0.233109	0.069340		0.703390
3	0.837887	0.282056	0.048120	0.804721	0.854927	0.233284	0.066090		0.711864
4	0.888536	0.238169	0.063098	0.849785	0.873112	0.158434	0.090261		0.644068
5	0.902997	0.218986	0.065944	0.901288	0.850691	0.159199	0.085057		0.627119
6	0.921879	0.198287	0.077870	0.909871	0.871108	0.178159	0.101892		0.593220
7	0.953500	0.151700	0.105339	0.935622	0.863069	0.114581	0.123950		0.500000
8	0.970317	0.122954	0.136420	0.948498	0.880830	0.114647	0.125761		0.525424
9	0.984137	0.090528	0.188020	0.969957	0.875125	0.108112	0.132584		0.500000


	Id						AUC			Recall		Precision
0	Non-augmented			0.874800	0.500000	0.132584
1	Augmented_1				0.871423	0.533898	0.118199
2	Augmented_2				0.864620	0.457627	0.108871
3	Augmented_3				0.891106	0.584746	0.123435
4	Augmented_4				0.865278	0.440678	0.114790
5	Augmented_5				0.832942	0.228814	0.114894
6	Augmented_6				0.824432	0.296610	0.125448
7	Augmented_7				0.892226	0.627119	0.130973
8	Augmented_8				0.867572	0.474576	0.113590
9	Augmented_9				0.832109	0.262712	0.121569
10	Augmented_10			0.825646	0.271186	0.124514
11	Augmented_11			0.838864	0.254237	0.120968
12	Augmented_12			0.841108	0.245763	0.144279
13	Augmented_13			0.834326	0.228814	0.133663
14	Augmented_14			0.833731	0.245763	0.122363
15	Augmented_15			0.893188	0.610169	0.127208
16	Augmented_16			0.851382	0.330508	0.148289
17	Augmented_17			0.879956	0.466102	0.120350
18	Augmented_18			0.840308	0.228814	0.137056
19	Augmented_19			0.845431	0.288136	0.143460
20	Augmented_20			0.892524	0.661017	0.128501
21	Augmented_21			0.841186	0.305085	0.132841
22	Augmented_22			0.829447	0.228814	0.108000
23	Augmented_23			0.892466	0.644068	0.129252
24	Augmented_24			0.836193	0.305085	0.157205
25	Augmented_25			0.845937	0.279661	0.125954
26	Augmented_26			0.878217	0.525424	0.139955
27	Augmented_27			0.875842	0.542373	0.116788
28	Augmented_28			0.836090	0.228814	0.113924
29	Augmented_29			0.874867	0.516949	0.111927
30	Augmented_30			0.844332	0.203390	0.104348
31	Augmented_31			0.831455	0.347458	0.133550
32	Augmented_32			0.830683	0.254237	0.123967
33	Combined_with_mean		0.900614	0.372881	0.260355
34	Combined_with_median	0.891395	0.406780	0.237624


age with scale with BN, with new augment-test TTA

private score : 0.5344, public score : 0.5976




TTA with all possible augmentations specified with prev same train-valid model

	Id						AUC			Recall		Precision
0	Non-augmented			0.874800	0.500000	0.132584
1	Augmented_1				0.881048	0.533898	0.135193
2	Augmented_2				0.887644	0.567797	0.119005
3	Augmented_3				0.840526	0.271186	0.134454
4	Augmented_4				0.821557	0.271186	0.116364
5	Augmented_5				0.880305	0.440678	0.114035
6	Augmented_6				0.890951	0.593220	0.130841
7	Augmented_7				0.834583	0.313559	0.142308
8	Augmented_8				0.834839	0.228814	0.112500
9	Augmented_9				0.872956	0.491525	0.119588
10	Augmented_10			0.886904	0.610169	0.130909
11	Augmented_11			0.835450	0.220339	0.103586
12	Augmented_12			0.834801	0.245763	0.107807
13	Augmented_13			0.877242	0.432203	0.108051
14	Augmented_14			0.886638	0.576271	0.120141
15	Augmented_15			0.832396	0.228814	0.118421
16	Augmented_16			0.845507	0.279661	0.134146
17	Augmented_17			0.864084	0.516949	0.118447
18	Augmented_18			0.893201	0.593220	0.119658
19	Augmented_19			0.834879	0.262712	0.116105
20	Augmented_20			0.823054	0.237288	0.116183
21	Augmented_21			0.874284	0.516949	0.116412
22	Augmented_22			0.888624	0.627119	0.113846
23	Augmented_23			0.827650	0.271186	0.116364
24	Augmented_24			0.803498	0.245763	0.092652
25	Augmented_25			0.868174	0.423729	0.128866
26	Augmented_26			0.889876	0.491525	0.135198
27	Augmented_27			0.835866	0.220339	0.132653
28	Augmented_28			0.848547	0.220339	0.128713
29	Augmented_29			0.876326	0.466102	0.115546
30	Augmented_30			0.888727	0.618644	0.125862
31	Augmented_31			0.824809	0.245763	0.114625
32	Augmented_32			0.842047	0.271186	0.122137
33	Combined_with_mean		0.903131	0.389831	0.223301
34	Combined_with_median	0.893524	0.423729	0.196850


all possible augmentations - test TTA

private score : 0.5331, public score : 0.4979


extract only orig from TTA 

private score : 0.8624, public score : 0.8825


extract only aug0 from TTA 

0.5038, 0.5074


extract only aug1 from TTA 

0.4998, 0.5317


extract only aug2 from TTA 

0.5185, 0.4603


extract only aug8 from TTA

0.5433, 0.4719


extract only aug16 from TTA

0.5189, 0.4728


extract only aug24 from TTA

0.5528, 0.5324



all possible augmentations - test TTA - predictions on images sorted by names

private score : 0.8947, public score : 0.9043


random augmentations - test TTA - predictions on images sorted by names

private score : 0.8966, public score : 0.9066


default BFCE params g2 ls0

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.716588	0.199665	0.030402	0.738197	0.825644	0.162406	0.058867		0.677966
1	0.817693	0.160242	0.042825	0.783262	0.869979	0.152313	0.051170		0.889831
2	0.840973	0.151283	0.048291	0.778970	0.875497	0.150063	0.058859		0.830508
3	0.852991	0.145268	0.052543	0.802575	0.859976	0.132234	0.069299		0.728814
4	0.868812	0.134124	0.054553	0.830472	0.514490	41.153183	0.019105		0.991525
5	0.857905	0.143890	0.052093	0.830472	0.891962	0.127401	0.072543		0.788136
6	0.879842	0.129213	0.059490	0.845494	0.877123	0.108789	0.075938		0.703390
7	0.911582	0.109116	0.072183	0.864807	0.893002	0.112984	0.076507		0.838983
8	0.929825	0.096251	0.078061	0.905579	0.894958	0.090477	0.083108		0.779661
9	0.942560	0.084286	0.088485	0.918455	0.905520	0.095813	0.081531		0.830508

	Id						AUC			Recall		Precision
0	Non-augmented			0.905698	0.830508	0.081531
1	Augmented_1				0.900119	0.822034	0.081788
2	Augmented_2				0.891976	0.881356	0.059429
3	Augmented_3				0.860007	0.779661	0.058265
4	Augmented_4				0.876323	0.822034	0.061902
5	Augmented_5				0.895825	0.830508	0.085515
6	Augmented_6				0.892377	0.881356	0.063183
7	Augmented_7				0.843802	0.762712	0.056320
8	Augmented_8				0.861340	0.822034	0.061276
9	Augmented_9				0.899246	0.822034	0.080165
10	Augmented_10			0.894101	0.881356	0.057714
11	Augmented_11			0.859749	0.788136	0.057550
12	Augmented_12			0.876697	0.847458	0.059952
13	Augmented_13			0.891502	0.788136	0.078614
14	Augmented_14			0.892630	0.864407	0.058186
15	Augmented_15			0.852644	0.779661	0.056790
16	Augmented_16			0.877950	0.872881	0.062538
17	Augmented_17			0.905246	0.830508	0.079805
18	Augmented_18			0.891136	0.898305	0.059887
19	Augmented_19			0.868069	0.847458	0.059988
20	Augmented_20			0.861133	0.796610	0.057811
21	Augmented_21			0.890677	0.805085	0.073701
22	Augmented_22			0.887353	0.906780	0.055846
23	Augmented_23			0.857530	0.788136	0.048691
24	Augmented_24			0.851412	0.830508	0.052660
25	Augmented_25			0.897115	0.805085	0.094716
26	Augmented_26			0.886633	0.830508	0.066940
27	Augmented_27			0.871197	0.745763	0.066415
28	Augmented_28			0.874804	0.788136	0.072600
29	Augmented_29			0.905109	0.838983	0.081214
30	Augmented_30			0.893150	0.872881	0.059572
31	Augmented_31			0.862316	0.779661	0.055355
32	Augmented_32			0.869311	0.830508	0.059829
33	Combined_with_mean		0.907995	0.864407	0.079501
34	Combined_with_median	0.908255	0.881356	0.073811


BFCE alpha=0.5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.684952	0.220760	0.029383	0.718884	0.719645	0.232161	0.042757		0.415254
1	0.781921	0.181011	0.039180	0.774678	0.816215	0.162329	0.042292		0.788136
2	0.818612	0.164676	0.045104	0.789700	0.877305	0.144630	0.057681		0.830508
3	0.827681	0.163392	0.047999	0.813305	0.872522	0.143423	0.049020		0.847458
4	0.834402	0.148637	0.046689	0.826180	0.874851	0.138791	0.056128		0.830508
5	0.851933	0.139453	0.052531	0.826180	0.744923	0.144327	0.059659		0.711864
6	0.852750	0.154833	0.054831	0.830472	0.863360	0.120080	0.076087		0.652542
7	0.887864	0.125732	0.057543	0.845494	0.875197	0.079911	0.100917		0.652542
8	0.904848	0.110772	0.068786	0.877682	0.888880	0.067328	0.116987		0.618644
9	0.926428	0.097142	0.077929	0.907725	0.876339	0.060896	0.110400		0.584746

	Id						AUC			Recall		Precision
0	Non-augmented			0.876259	0.584746	0.110400
1	Augmented_1				0.875432	0.584746	0.107981
2	Augmented_2				0.878661	0.661017	0.093637
3	Augmented_3				0.890372	0.703390	0.106410
4	Augmented_4				0.890499	0.669492	0.102999
5	Augmented_5				0.881349	0.559322	0.105263
6	Augmented_6				0.878010	0.635593	0.095541
7	Augmented_7				0.900355	0.703390	0.108781
8	Augmented_8				0.891542	0.661017	0.110169
9	Augmented_9				0.868665	0.584746	0.105505
10	Augmented_10			0.878399	0.669492	0.089977
11	Augmented_11			0.896942	0.728814	0.107769
12	Augmented_12			0.893682	0.711864	0.102314
13	Augmented_13			0.880168	0.593220	0.108192
14	Augmented_14			0.873326	0.644068	0.091566
15	Augmented_15			0.903456	0.728814	0.109694
16	Augmented_16			0.900018	0.720339	0.107731
17	Augmented_17			0.873269	0.610169	0.106352
18	Augmented_18			0.879277	0.669492	0.089671
19	Augmented_19			0.900680	0.703390	0.099760
20	Augmented_20			0.897823	0.703390	0.103234
21	Augmented_21			0.880119	0.644068	0.106592
22	Augmented_22			0.873530	0.694915	0.086864
23	Augmented_23			0.890655	0.720339	0.091892
24	Augmented_24			0.893084	0.754237	0.098560
25	Augmented_25			0.868708	0.516949	0.117534
26	Augmented_26			0.868867	0.601695	0.100709
27	Augmented_27			0.885868	0.661017	0.116942
28	Augmented_28			0.895983	0.627119	0.120718
29	Augmented_29			0.873745	0.550847	0.104502
30	Augmented_30			0.882805	0.644068	0.091898
31	Augmented_31			0.898946	0.686441	0.103185
32	Augmented_32			0.897478	0.686441	0.102144
33	Combined_with_mean		0.903198	0.669492	0.131886
34	Combined_with_median	0.902507	0.661017	0.120743


BFCE alpha=0.75

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.724253	0.225591	0.033439	0.721030	0.742734	0.189798	0.038562		0.627119
1	0.764860	0.191439	0.037673	0.729614	0.823308	0.181352	0.042297		0.805085
2	0.827115	0.159720	0.046205	0.751073	0.856365	0.131549	0.070648		0.711864
3	0.841982	0.155551	0.051304	0.785408	0.846473	0.135767	0.054039		0.805085
4	0.839485	0.151587	0.051239	0.793991	0.878051	0.114633	0.085129		0.669492
5	0.858733	0.142165	0.057470	0.809013	0.897630	0.150641	0.077098		0.864407
6	0.887996	0.124125	0.062801	0.811159	0.888177	0.103423	0.074012		0.745763
7	0.904522	0.109144	0.067556	0.847640	0.899379	0.096664	0.078947		0.788136
8	0.927269	0.095853	0.082224	0.888412	0.899208	0.093313	0.081882		0.796610
9	0.945077	0.082841	0.093442	0.914163	0.900830	0.083322	0.088933		0.762712

	Id						AUC			Recall		Precision
0	Non-augmented			0.901013	0.762712	0.088933
1	Augmented_1				0.902728	0.771186	0.089567
2	Augmented_2				0.893350	0.813559	0.073620
3	Augmented_3				0.859028	0.677966	0.075614
4	Augmented_4				0.870428	0.703390	0.077138
5	Augmented_5				0.900617	0.771186	0.090818
6	Augmented_6				0.903076	0.855932	0.075542
7	Augmented_7				0.873005	0.754237	0.080836
8	Augmented_8				0.878016	0.728814	0.075109
9	Augmented_9				0.893150	0.754237	0.085495
10	Augmented_10			0.896681	0.822034	0.071958
11	Augmented_11			0.867058	0.737288	0.078027
12	Augmented_12			0.881558	0.745763	0.076655
13	Augmented_13			0.896282	0.754237	0.093291
14	Augmented_14			0.891669	0.830508	0.075096
15	Augmented_15			0.863036	0.694915	0.075996
16	Augmented_16			0.873543	0.737288	0.076923
17	Augmented_17			0.898829	0.745763	0.081633
18	Augmented_18			0.900810	0.855932	0.070877
19	Augmented_19			0.880296	0.720339	0.073785
20	Augmented_20			0.886332	0.754237	0.075616
21	Augmented_21			0.895143	0.813559	0.084581
22	Augmented_22			0.892972	0.847458	0.065876
23	Augmented_23			0.855361	0.754237	0.066368
24	Augmented_24			0.880269	0.822034	0.071324
25	Augmented_25			0.898916	0.754237	0.105952
26	Augmented_26			0.894891	0.754237	0.084201
27	Augmented_27			0.850788	0.618644	0.085984
28	Augmented_28			0.884741	0.694915	0.096019
29	Augmented_29			0.898975	0.788136	0.094130
30	Augmented_30			0.900289	0.847458	0.074794
31	Augmented_31			0.864310	0.677966	0.070922
32	Augmented_32			0.876103	0.728814	0.075905
33	Combined_with_mean		0.907864	0.771186	0.097118
34	Combined_with_median	0.908158	0.813559	0.093933


g1_ls0_a0.25

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.738475	0.350577	0.033267	0.714592	0.799794	0.291966	0.040590		0.745763
1	0.834957	0.285201	0.054173	0.766094	0.861710	0.237246	0.061281		0.745763
2	0.856192	0.270455	0.052360	0.804721	0.861319	0.237678	0.057196		0.788136
3	0.879338	0.246841	0.059763	0.834764	0.862107	0.186006	0.078488		0.686441
4	0.887957	0.232772	0.063215	0.854077	0.856578	0.179181	0.085903		0.661017
5	0.883253	0.252961	0.062965	0.854077	0.858164	0.231196	0.072030		0.652542
6	0.902344	0.223169	0.066958	0.888412	0.856486	0.128906	0.109155		0.525424
7	0.930817	0.186915	0.080152	0.905579	0.882676	0.155989	0.099422		0.728814
8	0.964556	0.130573	0.124159	0.950644	0.864806	0.097367	0.134703		0.500000
9	0.977022	0.104597	0.155028	0.952790	0.864761	0.103583	0.133479		0.516949

	Id						AUC			Recall		Precision
0	Non-augmented			0.865658	0.516949	0.133479
1	Augmented_1				0.870673	0.533898	0.134328
2	Augmented_2				0.883699	0.593220	0.116279
3	Augmented_3				0.813525	0.389831	0.088632
4	Augmented_4				0.817021	0.389831	0.077311
5	Augmented_5				0.852930	0.483051	0.114688
6	Augmented_6				0.885642	0.576271	0.111658
7	Augmented_7				0.819744	0.372881	0.082397
8	Augmented_8				0.812424	0.457627	0.092943
9	Augmented_9				0.854078	0.457627	0.123570
10	Augmented_10			0.884132	0.584746	0.120000
11	Augmented_11			0.840013	0.423729	0.097276
12	Augmented_12			0.820299	0.406780	0.082616
13	Augmented_13			0.866752	0.542373	0.133333
14	Augmented_14			0.879314	0.618644	0.116057
15	Augmented_15			0.826769	0.406780	0.086799
16	Augmented_16			0.800100	0.415254	0.075851
17	Augmented_17			0.851719	0.474576	0.112224
18	Augmented_18			0.884322	0.567797	0.108239
19	Augmented_19			0.820310	0.466102	0.095652
20	Augmented_20			0.829214	0.415254	0.080858
21	Augmented_21			0.860792	0.516949	0.118447
22	Augmented_22			0.887115	0.652542	0.117021
23	Augmented_23			0.817181	0.423729	0.085763
24	Augmented_24			0.833024	0.533898	0.098746
25	Augmented_25			0.863796	0.440678	0.130326
26	Augmented_26			0.874926	0.508475	0.123457
27	Augmented_27			0.818165	0.330508	0.090069
28	Augmented_28			0.809921	0.330508	0.083512
29	Augmented_29			0.859787	0.508475	0.121704
30	Augmented_30			0.883106	0.584746	0.115385
31	Augmented_31			0.826493	0.432203	0.091892
32	Augmented_32			0.812077	0.415254	0.081126
33	Combined_with_mean		0.893120	0.440678	0.143251
34	Combined_with_median	0.891706	0.474576	0.142857


g3_ls0_a0.25

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.720556	0.120135	0.032145	0.751073	0.806426	0.100060	0.058871		0.618644
1	0.795002	0.108462	0.042384	0.776824	0.718333	0.124511	0.034247		0.211864
2	0.749916	0.115036	0.034604	0.738197	0.841650	0.101274	0.045730		0.830508
3	0.825237	0.095071	0.047548	0.817597	0.880214	0.083625	0.056856		0.864407
4	0.853438	0.084488	0.053299	0.839056	0.824594	0.110188	0.052535		0.728814
5	0.848610	0.080365	0.049782	0.832618	0.842524	0.082822	0.040987		0.872881
6	0.872532	0.068330	0.056547	0.843348	0.878625	0.061130	0.076156		0.711864
7	0.883125	0.068005	0.060213	0.847640	0.876905	0.058129	0.070411		0.754237
8	0.922659	0.056390	0.077672	0.890558	0.893602	0.050218	0.078818		0.813559
9	0.939408	0.047934	0.087250	0.916309	0.897426	0.055163	0.077435		0.788136

	Id						AUC			Recall		Precision
0	Non-augmented			0.897279	0.788136	0.077435
1	Augmented_1				0.901639	0.788136	0.077051
2	Augmented_2				0.893513	0.898305	0.062648
3	Augmented_3				0.865572	0.788136	0.064138
4	Augmented_4				0.860817	0.711864	0.061493
5	Augmented_5				0.905082	0.805085	0.081545
6	Augmented_6				0.887771	0.847458	0.061275
7	Augmented_7				0.852438	0.779661	0.061828
8	Augmented_8				0.854620	0.737288	0.063273
9	Augmented_9				0.890532	0.779661	0.072555
10	Augmented_10			0.893276	0.872881	0.059572
11	Augmented_11			0.851476	0.754237	0.059373
12	Augmented_12			0.860561	0.771186	0.065000
13	Augmented_13			0.892697	0.779661	0.075163
14	Augmented_14			0.886032	0.830508	0.057444
15	Augmented_15			0.857688	0.754237	0.060013
16	Augmented_16			0.851430	0.728814	0.060777
17	Augmented_17			0.898675	0.788136	0.073634
18	Augmented_18			0.894698	0.889831	0.061332
19	Augmented_19			0.864837	0.788136	0.063094
20	Augmented_20			0.863261	0.737288	0.063411
21	Augmented_21			0.888452	0.813559	0.073789
22	Augmented_22			0.884592	0.864407	0.057368
23	Augmented_23			0.845245	0.745763	0.054692
24	Augmented_24			0.858345	0.771186	0.057125
25	Augmented_25			0.906504	0.788136	0.089080
26	Augmented_26			0.888296	0.838983	0.068465
27	Augmented_27			0.855723	0.754237	0.073554
28	Augmented_28			0.869155	0.728814	0.076039
29	Augmented_29			0.887777	0.762712	0.074751
30	Augmented_30			0.887538	0.855932	0.060263
31	Augmented_31			0.859654	0.771186	0.061528
32	Augmented_32			0.865316	0.771186	0.064220
33	Combined_with_mean		0.903866	0.855932	0.082991
34	Combined_with_median	0.901528	0.838983	0.075979


g1_ls0_a0.75

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.736140	0.346340	0.032885	0.787554	0.840390	0.268750	0.048187		0.822034
1	0.810728	0.294113	0.040268	0.813305	0.853395	0.257448	0.048843		0.805085
2	0.841214	0.280646	0.051229	0.800429	0.873439	0.295438	0.051066		0.872881
3	0.863553	0.263188	0.056430	0.830472	0.866654	0.182322	0.074236		0.720339
4	0.871533	0.252905	0.056873	0.854077	0.840351	0.303526	0.053896		0.779661
5	0.874757	0.247105	0.058166	0.826180	0.857563	0.288356	0.059186		0.813559
6	0.895385	0.223778	0.064066	0.856223	0.875336	0.173204	0.089701		0.686441
7	0.911911	0.200972	0.069799	0.886266	0.870879	0.201696	0.068006		0.745763
8	0.925961	0.180482	0.076071	0.899142	0.876681	0.134709	0.103597		0.610169
9	0.946860	0.154531	0.101231	0.935622	0.870864	0.160047	0.095533		0.652542

	Id						AUC			Recall		Precision
0	Non-augmented			0.871269	0.652542	0.095533
1	Augmented_1				0.874001	0.618644	0.094682
2	Augmented_2				0.878893	0.737288	0.069992
3	Augmented_3				0.896423	0.711864	0.097674
4	Augmented_4				0.879174	0.711864	0.089552
5	Augmented_5				0.864091	0.601695	0.093544
6	Augmented_6				0.874238	0.745763	0.074262
7	Augmented_7				0.886846	0.694915	0.095794
8	Augmented_8				0.884707	0.686441	0.090909
9	Augmented_9				0.863295	0.652542	0.096977
10	Augmented_10			0.875780	0.788136	0.073576
11	Augmented_11			0.886588	0.694915	0.094361
12	Augmented_12			0.877028	0.728814	0.091489
13	Augmented_13			0.864483	0.635593	0.097529
14	Augmented_14			0.874441	0.745763	0.069291
15	Augmented_15			0.879681	0.661017	0.088435
16	Augmented_16			0.863056	0.677966	0.086393
17	Augmented_17			0.872609	0.644068	0.094059
18	Augmented_18			0.883429	0.771186	0.070488
19	Augmented_19			0.906040	0.788136	0.099572
20	Augmented_20			0.884555	0.711864	0.086154
21	Augmented_21			0.873817	0.669492	0.094048
22	Augmented_22			0.872563	0.745763	0.063218
23	Augmented_23			0.891646	0.813559	0.090566
24	Augmented_24			0.887918	0.788136	0.087406
25	Augmented_25			0.867558	0.610169	0.110260
26	Augmented_26			0.874601	0.686441	0.078794
27	Augmented_27			0.878302	0.618644	0.107988
28	Augmented_28			0.867437	0.610169	0.097959
29	Augmented_29			0.867023	0.601695	0.089308
30	Augmented_30			0.882186	0.754237	0.071030
31	Augmented_31			0.892422	0.720339	0.096154
32	Augmented_32			0.882357	0.711864	0.089648
33	Combined_with_mean		0.900173	0.737288	0.102473
34	Combined_with_median	0.899293	0.728814	0.101176


g3_ls0_a0.75

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.719060	0.115218	0.032726	0.755365	0.549179	0.161447	0.022402		0.915254
1	0.812526	0.094323	0.046286	0.776824	0.833331	0.083448	0.061415		0.779661
2	0.823941	0.088224	0.047884	0.757511	0.865175	0.128307	0.040561		0.906780
3	0.838642	0.083444	0.047417	0.789700	0.879922	0.062359	0.065085		0.813559
4	0.837365	0.078148	0.051412	0.785408	0.874101	0.070179	0.057937		0.847458
5	0.854183	0.085469	0.053726	0.836910	0.867004	0.060299	0.069489		0.737288
6	0.861943	0.088178	0.054175	0.832618	0.881581	0.070017	0.071047		0.788136
7	0.885907	0.069268	0.059044	0.845494	0.882617	0.052336	0.084661		0.720339
8	0.917530	0.056690	0.072058	0.877682	0.890574	0.037379	0.106684		0.703390
9	0.927404	0.049917	0.077152	0.888412	0.872196	0.045368	0.074394		0.728814

	Id						AUC			Recall		Precision
0	Non-augmented			0.872349	0.728814	0.074394
1	Augmented_1				0.870069	0.745763	0.075278
2	Augmented_2				0.869428	0.805085	0.064846
3	Augmented_3				0.885998	0.762712	0.068234
4	Augmented_4				0.885277	0.796610	0.073841
5	Augmented_5				0.877263	0.677966	0.077369
6	Augmented_6				0.868921	0.788136	0.070508
7	Augmented_7				0.882750	0.762712	0.074751
8	Augmented_8				0.887019	0.805085	0.083627
9	Augmented_9				0.866973	0.737288	0.069823
10	Augmented_10			0.865445	0.822034	0.062783
11	Augmented_11			0.883844	0.813559	0.066852
12	Augmented_12			0.884313	0.822034	0.070803
13	Augmented_13			0.879668	0.762712	0.076078
14	Augmented_14			0.866928	0.796610	0.061720
15	Augmented_15			0.886669	0.771186	0.067109
16	Augmented_16			0.883146	0.796610	0.070836
17	Augmented_17			0.875931	0.703390	0.071552
18	Augmented_18			0.869495	0.805085	0.066387
19	Augmented_19			0.882376	0.754237	0.067322
20	Augmented_20			0.883209	0.745763	0.070175
21	Augmented_21			0.862810	0.745763	0.069565
22	Augmented_22			0.864986	0.796610	0.059720
23	Augmented_23			0.873281	0.813559	0.064690
24	Augmented_24			0.888439	0.830508	0.069454
25	Augmented_25			0.876165	0.677966	0.081054
26	Augmented_26			0.865153	0.728814	0.071369
27	Augmented_27			0.885159	0.754237	0.082945
28	Augmented_28			0.889606	0.762712	0.085066
29	Augmented_29			0.874339	0.745763	0.076190
30	Augmented_30			0.866679	0.788136	0.063655
31	Augmented_31			0.881892	0.762712	0.067014
32	Augmented_32			0.883629	0.779661	0.071152
33	Combined_with_mean		0.895508	0.779661	0.085422
34	Combined_with_median	0.894002	0.788136	0.079487


g2_ls0.05_a0.25

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.700722	0.213157	0.030989	0.684549	0.657250	0.195899	0.028882		0.652542
1	0.812576	0.165622	0.046527	0.744635	0.838298	0.163946	0.047349		0.779661
2	0.817467	0.159627	0.049839	0.763949	0.793006	0.151301	0.126984		0.542373
3	0.824259	0.156956	0.045910	0.776824	0.874568	0.115539	0.086907		0.652542
4	0.853669	0.140117	0.052781	0.798283	0.877356	0.124398	0.059430		0.830508
5	0.882733	0.122652	0.059485	0.828326	0.871550	0.091115	0.082353		0.711864
6	0.892616	0.116900	0.062147	0.849785	0.839511	0.236781	0.085631		0.500000
7	0.911010	0.109265	0.068399	0.875537	0.884076	0.080598	0.096220		0.711864
8	0.940172	0.087929	0.088594	0.918455	0.900755	0.077219	0.095802		0.754237
9	0.950853	0.078525	0.104127	0.931330	0.902532	0.077345	0.102941		0.771186

	Id						AUC			Recall		Precision
0	Non-augmented			0.902237	0.771186	0.102941
1	Augmented_1				0.909102	0.788136	0.101197
2	Augmented_2				0.902183	0.830508	0.075038
3	Augmented_3				0.874765	0.677966	0.078973
4	Augmented_4				0.875583	0.737288	0.085714
5	Augmented_5				0.916039	0.796610	0.106455
6	Augmented_6				0.901784	0.830508	0.079288
7	Augmented_7				0.869295	0.652542	0.078893
8	Augmented_8				0.887913	0.762712	0.088322
9	Augmented_9				0.903145	0.771186	0.096706
10	Augmented_10			0.900234	0.838983	0.076212
11	Augmented_11			0.870643	0.677966	0.080564
12	Augmented_12			0.886176	0.728814	0.083902
13	Augmented_13			0.901065	0.728814	0.094714
14	Augmented_14			0.897234	0.855932	0.076226
15	Augmented_15			0.885545	0.737288	0.084302
16	Augmented_16			0.876376	0.737288	0.083734
17	Augmented_17			0.906608	0.788136	0.097179
18	Augmented_18			0.902013	0.838983	0.075000
19	Augmented_19			0.878707	0.711864	0.081871
20	Augmented_20			0.880535	0.711864	0.081395
21	Augmented_21			0.896587	0.796610	0.094188
22	Augmented_22			0.892249	0.864407	0.068966
23	Augmented_23			0.877428	0.754237	0.075042
24	Augmented_24			0.896371	0.771186	0.077513
25	Augmented_25			0.905069	0.703390	0.110814
26	Augmented_26			0.896340	0.779661	0.087039
27	Augmented_27			0.875465	0.652542	0.098465
28	Augmented_28			0.884711	0.618644	0.096306
29	Augmented_29			0.907448	0.805085	0.104741
30	Augmented_30			0.897272	0.830508	0.073408
31	Augmented_31			0.879098	0.728814	0.083414
32	Augmented_32			0.883082	0.728814	0.085828
33	Combined_with_mean		0.917383	0.771186	0.108333
34	Combined_with_median	0.917552	0.813559	0.105843

g2_ls0.05_a0.75

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.707734	0.213903	0.032510	0.714592	0.823161	0.133932	0.064167		0.652542
1	0.820685	0.162957	0.048650	0.781116	0.860393	0.112227	0.087558		0.644068
2	0.857858	0.145065	0.052020	0.806867	0.849831	0.111443	0.066610		0.669492
3	0.872157	0.135023	0.055326	0.832618	0.799016	0.150275	0.076633		0.516949
4	0.867370	0.136160	0.055261	0.849785	0.851228	0.132657	0.055357		0.788136
5	0.856189	0.159973	0.055815	0.819743	0.821432	0.102042	0.082734		0.584746
6	0.903044	0.122386	0.070157	0.845494	0.799665	0.124737	0.047901		0.686441
7	0.931855	0.096506	0.081023	0.896996	0.864238	0.066752	0.109091		0.559322
8	0.953389	0.080349	0.106186	0.935622	0.870003	0.055499	0.108168		0.415254
9	0.975940	0.058152	0.154721	0.963519	0.877168	0.045756	0.137427		0.398305

	Id						AUC			Recall		Precision
0	Non-augmented			0.876780	0.398305	0.137427
1	Augmented_1				0.874942	0.432203	0.148688
2	Augmented_2				0.882916	0.567797	0.125704
3	Augmented_3				0.849574	0.381356	0.153061
4	Augmented_4				0.835839	0.364407	0.137380
5	Augmented_5				0.875809	0.406780	0.137931
6	Augmented_6				0.880212	0.533898	0.122093
7	Augmented_7				0.822320	0.313559	0.135036
8	Augmented_8				0.840967	0.347458	0.135314
9	Augmented_9				0.860345	0.389831	0.137725
10	Augmented_10			0.876336	0.550847	0.121269
11	Augmented_11			0.841705	0.338983	0.132013
12	Augmented_12			0.852844	0.372881	0.139241
13	Augmented_13			0.868158	0.415254	0.143695
14	Augmented_14			0.882279	0.576271	0.125926
15	Augmented_15			0.840100	0.338983	0.133779
16	Augmented_16			0.843315	0.406780	0.158940
17	Augmented_17			0.885520	0.474576	0.147757
18	Augmented_18			0.881929	0.550847	0.114638
19	Augmented_19			0.851430	0.389831	0.130312
20	Augmented_20			0.854816	0.381356	0.134731
21	Augmented_21			0.852211	0.449153	0.136598
22	Augmented_22			0.879756	0.618644	0.117742
23	Augmented_23			0.828868	0.406780	0.134078
24	Augmented_24			0.842676	0.432203	0.144886
25	Augmented_25			0.868222	0.372881	0.153310
26	Augmented_26			0.885960	0.525424	0.140590
27	Augmented_27			0.855181	0.279661	0.141026
28	Augmented_28			0.838853	0.313559	0.142857
29	Augmented_29			0.865527	0.415254	0.137640
30	Augmented_30			0.883693	0.559322	0.124294
31	Augmented_31			0.819875	0.322034	0.125413
32	Augmented_32			0.824948	0.364407	0.143813
33	Combined_with_mean		0.894747	0.398305	0.217593
34	Combined_with_median	0.893215	0.432203	0.206478


g2_ls0.1_a0.25

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.693508	0.215911	0.029035	0.753219	0.786022	0.162690	0.045455		0.737288
1	0.745842	0.190385	0.034701	0.740343	0.836661	0.111546	0.087675		0.584746
2	0.801619	0.172487	0.043945	0.772532	0.871535	0.157493	0.053340		0.872881
3	0.838552	0.153797	0.049370	0.806867	0.861169	0.190403	0.040478		0.889831
4	0.824650	0.158017	0.043226	0.783262	0.866581	0.122468	0.072289		0.762712
5	0.842813	0.147057	0.047414	0.828326	0.872801	0.131524	0.060904		0.788136
6	0.859112	0.134896	0.050062	0.866953	0.876305	0.116929	0.064935		0.847458
7	0.875694	0.126601	0.053987	0.845494	0.875931	0.107516	0.091104		0.720339
8	0.899312	0.112953	0.060858	0.879828	0.887073	0.119681	0.051220		0.889831
9	0.920515	0.100505	0.072347	0.905579	0.887350	0.090353	0.078399		0.796610

	Id						AUC			Recall		Precision
0	Non-augmented			0.887452	0.796610	0.078399
1	Augmented_1				0.882525	0.771186	0.075707
2	Augmented_2				0.883331	0.779661	0.070988
3	Augmented_3				0.888357	0.771186	0.075083
4	Augmented_4				0.894204	0.796610	0.079459
5	Augmented_5				0.891499	0.805085	0.086600
6	Augmented_6				0.884407	0.788136	0.075918
7	Augmented_7				0.904672	0.813559	0.085333
8	Augmented_8				0.892748	0.762712	0.081522
9	Augmented_9				0.883859	0.822034	0.078862
10	Augmented_10			0.881902	0.822034	0.069088
11	Augmented_11			0.900376	0.822034	0.073652
12	Augmented_12			0.885427	0.796610	0.072868
13	Augmented_13			0.883792	0.788136	0.080519
14	Augmented_14			0.883984	0.805085	0.070896
15	Augmented_15			0.894323	0.788136	0.073868
16	Augmented_16			0.884971	0.771186	0.071429
17	Augmented_17			0.882550	0.805085	0.077299
18	Augmented_18			0.884006	0.796610	0.070624
19	Augmented_19			0.898169	0.762712	0.071259
20	Augmented_20			0.891237	0.813559	0.077419
21	Augmented_21			0.884296	0.813559	0.073620
22	Augmented_22			0.881698	0.822034	0.066484
23	Augmented_23			0.897044	0.847458	0.068634
24	Augmented_24			0.891275	0.805085	0.066248
25	Augmented_25			0.874927	0.694915	0.085328
26	Augmented_26			0.877320	0.745763	0.078712
27	Augmented_27			0.890251	0.677966	0.084388
28	Augmented_28			0.883984	0.737288	0.090625
29	Augmented_29			0.894381	0.813559	0.079077
30	Augmented_30			0.883964	0.805085	0.072409
31	Augmented_31			0.895757	0.805085	0.077678
32	Augmented_32			0.879479	0.788136	0.076105
33	Combined_with_mean		0.904215	0.805085	0.087719
34	Combined_with_median	0.904504	0.813559	0.084731

g2_ls0.1_a0.75

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.709740	0.205663	0.032623	0.699571	0.797305	0.232912	0.040031		0.864407
1	0.792327	0.173972	0.045191	0.736051	0.842663	0.223040	0.073741		0.694915
2	0.826836	0.159220	0.046772	0.789700	0.870069	0.127596	0.075980		0.788136
3	0.837267	0.150105	0.048993	0.809013	0.830372	0.127207	0.064082		0.686441
4	0.862497	0.136192	0.053359	0.836910	0.850957	0.106936	0.052836		0.805085
5	0.856914	0.139699	0.052560	0.832618	0.861121	0.127858	0.063336		0.762712
6	0.864309	0.130247	0.055035	0.806867	0.874410	0.121414	0.067883		0.788136
7	0.885810	0.122239	0.059163	0.858369	0.868652	0.085458	0.091011		0.686441
8	0.904992	0.112672	0.070537	0.860515	0.887741	0.086171	0.101631		0.686441
9	0.931622	0.094214	0.080518	0.920601	0.887315	0.071997	0.100484		0.703390

	Id						AUC			Recall		Precision
0	Non-augmented			0.887542	0.703390	0.100484
1	Augmented_1				0.893968	0.711864	0.100962
2	Augmented_2				0.882349	0.728814	0.072758
3	Augmented_3				0.886914	0.754237	0.079323
4	Augmented_4				0.898229	0.779661	0.085502
5	Augmented_5				0.884513	0.669492	0.097893
6	Augmented_6				0.885019	0.711864	0.079470
7	Augmented_7				0.889106	0.745763	0.082552
8	Augmented_8				0.901055	0.728814	0.084896
9	Augmented_9				0.882292	0.686441	0.091319
10	Augmented_10			0.876243	0.754237	0.072712
11	Augmented_11			0.893933	0.788136	0.077051
12	Augmented_12			0.906348	0.838983	0.086237
13	Augmented_13			0.887601	0.661017	0.096178
14	Augmented_14			0.878132	0.754237	0.075874
15	Augmented_15			0.886867	0.754237	0.076329
16	Augmented_16			0.885305	0.754237	0.081279
17	Augmented_17			0.882621	0.677966	0.091324
18	Augmented_18			0.878884	0.745763	0.072250
19	Augmented_19			0.889926	0.788136	0.079692
20	Augmented_20			0.892237	0.754237	0.081877
21	Augmented_21			0.881999	0.703390	0.087185
22	Augmented_22			0.877230	0.754237	0.068726
23	Augmented_23			0.891200	0.847458	0.072464
24	Augmented_24			0.887176	0.796610	0.075684
25	Augmented_25			0.880120	0.593220	0.098592
26	Augmented_26			0.876704	0.694915	0.081836
27	Augmented_27			0.889401	0.677966	0.087527
28	Augmented_28			0.900191	0.720339	0.097701
29	Augmented_29			0.879601	0.644068	0.089941
30	Augmented_30			0.881674	0.779661	0.076096
31	Augmented_31			0.886481	0.771186	0.079825
32	Augmented_32			0.890369	0.737288	0.080930
33	Combined_with_mean		0.902026	0.737288	0.097534
34	Combined_with_median	0.900554	0.754237	0.093096



Default (alpha=0.25)
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
9	0.942560	0.084286	0.088485	0.918455	0.905520	0.095813	0.081531		0.830508
	Id						AUC			Recall		Precision
33	Combined_with_mean		0.907995	0.864407	0.079501
34	Combined_with_median	0.908255	0.881356	0.073811

alpha=0.75
	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
9	0.945077	0.082841	0.093442	0.914163	0.900830	0.083322	0.088933		0.762712
	Id						AUC			Recall		Precision
33	Combined_with_mean		0.907864	0.771186	0.097118
34	Combined_with_median	0.908158	0.813559	0.093933


default BFCE,without brig_down & cont_down

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.711786	0.223911	0.032775	0.701717	0.833691	0.137275	0.058589		0.745763
1	0.799764	0.173150	0.042793	0.757511	0.847029	0.255217	0.072835		0.627119
2	0.830177	0.159409	0.052045	0.781116	0.869174	0.139557	0.059387		0.788136
3	0.844758	0.146557	0.050829	0.815451	0.827369	0.143695	0.054688		0.771186
4	0.840909	0.142774	0.048579	0.806867	0.878242	0.094234	0.092655		0.694915
5	0.869948	0.130929	0.057384	0.830472	0.872846	0.123153	0.067265		0.762712
6	0.887563	0.124078	0.062530	0.843348	0.888290	0.092620	0.102767		0.661017
7	0.903168	0.110870	0.067729	0.875537	0.880729	0.120786	0.065562		0.805085
8	0.928968	0.094210	0.079541	0.892704	0.883872	0.104153	0.075331		0.771186
9	0.944039	0.084710	0.093522	0.907725	0.888801	0.067619	0.117851		0.576271

	Id						AUC			Recall		Precision
0	Non-augmented			0.888474	0.576271	0.117851
1	Augmented_1				0.886089	0.533898	0.104478
2	Augmented_2				0.879133	0.669492	0.094272
3	Augmented_3				0.886866	0.694915	0.107471
4	Augmented_4				0.893102	0.677966	0.104167
5	Augmented_5				0.879459	0.559322	0.112054
6	Augmented_6				0.874703	0.669492	0.097291
7	Augmented_7				0.882525	0.669492	0.103268
8	Augmented_8				0.880533	0.661017	0.104698
9	Augmented_9				0.882996	0.567797	0.109299
10	Augmented_10			0.873185	0.627119	0.086449
11	Augmented_11			0.876976	0.694915	0.106910
12	Augmented_12			0.875213	0.661017	0.099111
13	Augmented_13			0.876719	0.567797	0.104851
14	Augmented_14			0.879689	0.677966	0.091638
15	Augmented_15			0.879032	0.635593	0.093750
16	Augmented_16			0.892563	0.703390	0.101467
17	Augmented_17			0.885955	0.618644	0.108793
18	Augmented_18			0.877956	0.669492	0.086528
19	Augmented_19			0.880891	0.711864	0.092613
20	Augmented_20			0.888821	0.728814	0.099307
21	Augmented_21			0.876999	0.559322	0.108374
22	Augmented_22			0.881910	0.694915	0.094688
23	Augmented_23			0.888494	0.711864	0.105793
24	Augmented_24			0.886692	0.686441	0.105195
25	Combined_with_mean		0.902406	0.686441	0.121622
26	Combined_with_median	0.902472	0.686441	0.120357


default BFCE,without brig_down & cont_down for TTA

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.734585	0.207278	0.033168	0.789700	0.837759	0.157586	0.052213		0.779661
1	0.807752	0.165387	0.043812	0.772532	0.877790	0.172602	0.046957		0.915254
2	0.843286	0.145596	0.044657	0.819743	0.880659	0.136584	0.048780		0.847458
3	0.781420	0.178262	0.041068	0.742489	0.850703	0.184775	0.037383		0.915254
4	0.839485	0.156428	0.051313	0.817597	0.852298	0.215773	0.048118		0.855932
5	0.855172	0.153670	0.055720	0.806867	0.873917	0.137614	0.079334		0.686441
6	0.873976	0.133832	0.057515	0.843348	0.874493	0.126531	0.070722		0.788136
7	0.904047	0.113529	0.066480	0.866953	0.885161	0.091987	0.085945		0.720339
8	0.920718	0.102660	0.078962	0.901288	0.894927	0.099733	0.077168		0.822034
9	0.943305	0.085919	0.090755	0.918455	0.901360	0.092829	0.086207		0.805085

	Id						AUC			Recall		Precision
0	Non-augmented			0.901434	0.805085	0.086207
1	Augmented_1				0.897026	0.771186	0.079268
2	Augmented_2				0.897624	0.830508	0.063554
3	Augmented_3				0.875904	0.788136	0.068483
4	Augmented_4				0.867050	0.762712	0.067669
5	Augmented_5				0.901153	0.779661	0.084249
6	Augmented_6				0.896673	0.838983	0.066354
7	Augmented_7				0.867092	0.762712	0.067568
8	Augmented_8				0.872605	0.771186	0.070379
9	Augmented_9				0.899110	0.788136	0.082301
10	Augmented_10			0.896300	0.822034	0.064366
11	Augmented_11			0.864185	0.737288	0.064112
12	Augmented_12			0.867677	0.805085	0.069597
13	Augmented_13			0.897212	0.796610	0.078595
14	Augmented_14			0.898014	0.838983	0.061836
15	Augmented_15			0.868353	0.813559	0.066344
16	Augmented_16			0.876640	0.779661	0.065434
17	Augmented_17			0.893319	0.838983	0.080750
18	Augmented_18			0.894390	0.872881	0.061677
19	Augmented_19			0.868429	0.805085	0.060433
20	Augmented_20			0.869754	0.822034	0.062379
21	Augmented_21			0.897093	0.788136	0.081152
22	Augmented_22			0.896852	0.830508	0.064772
23	Augmented_23			0.887031	0.796610	0.067723
24	Augmented_24			0.874580	0.788136	0.066145
25	Combined_with_mean		0.910723	0.822034	0.084275
26	Combined_with_median	0.906533	0.822034	0.078990


without brig_down & cont_down for TTA, ls=0.05

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.705315	0.213900	0.030293	0.772532	0.800103	0.115992	0.065622		0.567797
1	0.795120	0.173418	0.041527	0.796137	0.828360	0.146084	0.050510		0.796610
2	0.842874	0.158692	0.049752	0.817597	0.838188	0.153691	0.060882		0.737288
3	0.878172	0.136475	0.057511	0.824034	0.862800	0.094322	0.082699		0.644068
4	0.858068	0.140637	0.052089	0.845494	0.851656	0.131844	0.052277		0.855932
5	0.871950	0.146625	0.059526	0.834764	0.886102	0.096411	0.084954		0.703390
6	0.889953	0.131767	0.063283	0.877682	0.870172	0.090650	0.086340		0.567797
7	0.921511	0.112659	0.076273	0.890558	0.851502	0.079094	0.082786		0.533898
8	0.954115	0.084438	0.100905	0.933476	0.856593	0.051215	0.140299		0.398305
9	0.971608	0.064438	0.139825	0.961373	0.855196	0.053674	0.124700		0.440678

	Id						AUC			Recall		Precision
0	Non-augmented			0.856414	0.440678	0.124700
1	Augmented_1				0.853325	0.491525	0.137767
2	Augmented_2				0.898954	0.694915	0.124242
3	Augmented_3				0.825021	0.347458	0.109333
4	Augmented_4				0.812697	0.406780	0.123393
5	Augmented_5				0.868868	0.525424	0.144860
6	Augmented_6				0.895171	0.661017	0.122066
7	Augmented_7				0.828465	0.372881	0.126801
8	Augmented_8				0.796014	0.381356	0.121622
9	Augmented_9				0.845079	0.542373	0.145125
10	Augmented_10			0.898140	0.669492	0.119516
11	Augmented_11			0.836004	0.398305	0.125333
12	Augmented_12			0.787869	0.338983	0.111421
13	Augmented_13			0.860231	0.533898	0.132911
14	Augmented_14			0.894655	0.677966	0.112835
15	Augmented_15			0.817342	0.355932	0.107417
16	Augmented_16			0.811677	0.440678	0.120092
17	Augmented_17			0.850633	0.474576	0.112903
18	Augmented_18			0.893778	0.711864	0.113055
19	Augmented_19			0.801798	0.372881	0.108108
20	Augmented_20			0.834231	0.432203	0.124390
21	Augmented_21			0.844212	0.525424	0.137472
22	Augmented_22			0.895858	0.669492	0.117560
23	Augmented_23			0.801888	0.322034	0.106145
24	Augmented_24			0.817988	0.355932	0.109091
25	Combined_with_mean		0.897880	0.474576	0.208955
26	Combined_with_median	0.886146	0.508475	0.178042


without brig_down & cont_down for TTA, ls0.05_g2.5

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.716970	0.161064	0.030795	0.736051	0.821692	0.113627	0.075795		0.525424
1	0.820973	0.129945	0.046168	0.776824	0.844796	0.112930	0.047217		0.805085
2	0.844656	0.126631	0.051361	0.821888	0.859048	0.093793	0.061737		0.728814
3	0.860177	0.103421	0.052805	0.824034	0.840047	0.102265	0.053571		0.737288
4	0.879436	0.094878	0.060489	0.839056	0.865849	0.074898	0.074040		0.686441
5	0.879053	0.098957	0.057131	0.851931	0.872446	0.085487	0.074402		0.711864
6	0.874224	0.138912	0.056836	0.841202	0.868693	0.089510	0.096264		0.567797
7	0.914174	0.095631	0.074913	0.873391	0.833847	0.051756	0.130699		0.364407
8	0.953158	0.065754	0.104845	0.933476	0.875162	0.043546	0.124722		0.474576
9	0.973595	0.048204	0.143088	0.950644	0.851821	0.034367	0.137681		0.322034

	Id						AUC			Recall		Precision
0	Non-augmented			0.853247	0.322034	0.137681
1	Augmented_1				0.865516	0.381356	0.156250
2	Augmented_2				0.879803	0.491525	0.137116
3	Augmented_3				0.814114	0.220339	0.105691
4	Augmented_4				0.807245	0.271186	0.128000
5	Augmented_5				0.853338	0.338983	0.144404
6	Augmented_6				0.874140	0.483051	0.136038
7	Augmented_7				0.811867	0.279661	0.132000
8	Augmented_8				0.797007	0.220339	0.099617
9	Augmented_9				0.855162	0.372881	0.154930
10	Augmented_10			0.882435	0.508475	0.133333
11	Augmented_11			0.815853	0.245763	0.120332
12	Augmented_12			0.783297	0.254237	0.108303
13	Augmented_13			0.843319	0.338983	0.148148
14	Augmented_14			0.869520	0.432203	0.118056
15	Augmented_15			0.816945	0.288136	0.130268
16	Augmented_16			0.802704	0.245763	0.110687
17	Augmented_17			0.846524	0.364407	0.133127
18	Augmented_18			0.869664	0.550847	0.129741
19	Augmented_19			0.793022	0.296610	0.128205
20	Augmented_20			0.799703	0.254237	0.093168
21	Augmented_21			0.841868	0.330508	0.139785
22	Augmented_22			0.882233	0.508475	0.139211
23	Augmented_23			0.824305	0.271186	0.121673
24	Augmented_24			0.803293	0.245763	0.113725
25	Combined_with_mean		0.889332	0.296610	0.220126
26	Combined_with_median	0.879964	0.338983	0.218579




without brig_down & cont_down for TTA, ls=0.02

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.718557	0.225139	0.033525	0.712446	0.838572	0.188806	0.054430		0.796610
1	0.806367	0.169692	0.044672	0.757511	0.848950	0.697812	0.054324		0.830508
2	0.838404	0.157165	0.049852	0.796137	0.852825	0.140231	0.064974		0.737288
3	0.824304	0.166378	0.050918	0.791846	0.850773	0.167469	0.069726		0.669492
4	0.862374	0.143992	0.057220	0.815451	0.840926	0.111518	0.074002		0.644068
5	0.885339	0.133879	0.063603	0.843348	0.853203	0.092461	0.080260		0.627119
6	0.910738	0.113152	0.071228	0.869099	0.836477	0.094849	0.084296		0.618644
7	0.937872	0.093277	0.089935	0.918455	0.828717	0.069298	0.071141		0.449153
8	0.961398	0.072890	0.114578	0.937768	0.862902	0.056228	0.122622		0.491525
9	0.980095	0.052070	0.163059	0.969957	0.851497	0.046465	0.131868		0.406780

	Id						AUC			Recall		Precision
0	Non-augmented			0.850788	0.406780	0.131868
1	Augmented_1				0.857281	0.474576	0.153425
2	Augmented_2				0.890312	0.601695	0.121575
3	Augmented_3				0.804926	0.279661	0.118280
4	Augmented_4				0.796379	0.254237	0.098684
5	Augmented_5				0.855468	0.406780	0.124352
6	Augmented_6				0.882336	0.593220	0.126812
7	Augmented_7				0.812626	0.305085	0.117647
8	Augmented_8				0.800685	0.245763	0.098976
9	Augmented_9				0.836737	0.389831	0.121053
10	Augmented_10			0.883639	0.601695	0.121784
11	Augmented_11			0.796060	0.220339	0.095588
12	Augmented_12			0.801102	0.347458	0.125767
13	Augmented_13			0.863593	0.474576	0.137592
14	Augmented_14			0.889492	0.601695	0.125000
15	Augmented_15			0.803956	0.305085	0.111455
16	Augmented_16			0.795779	0.237288	0.090909
17	Augmented_17			0.849057	0.440678	0.132316
18	Augmented_18			0.882804	0.627119	0.113323
19	Augmented_19			0.824900	0.305085	0.114286
20	Augmented_20			0.813934	0.355932	0.121037
21	Augmented_21			0.858812	0.415254	0.128272
22	Augmented_22			0.885217	0.618644	0.127178
23	Augmented_23			0.811799	0.237288	0.094915
24	Augmented_24			0.823742	0.279661	0.108197
25	Combined_with_mean		0.887405	0.355932	0.199052
26	Combined_with_median	0.878365	0.381356	0.176471


default BFCE,without brig_down & cont_down for TTA

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.716229	0.210822	0.031805	0.748927	0.766958	0.205109	0.035960		0.796610
1	0.823112	0.159124	0.046407	0.774678	0.844640	0.155587	0.049403		0.771186
2	0.867383	0.139677	0.056751	0.826180	0.828140	0.145048	0.056057		0.737288
3	0.845415	0.162132	0.055855	0.824034	0.650834	0.219732	0.026451		0.652542
4	0.859277	0.145793	0.054832	0.813305	0.824988	0.116693	0.064261		0.618644
5	0.883802	0.124564	0.059846	0.849785	0.857326	0.097421	0.079646		0.610169
6	0.915354	0.104963	0.072466	0.877682	0.855061	0.073242	0.088050		0.593220
7	0.929534	0.094009	0.082781	0.912017	0.874175	0.069093	0.095176		0.618644
8	0.963286	0.069339	0.119653	0.948498	0.865181	0.036120	0.166667		0.364407
9	0.980625	0.049107	0.183138	0.974249	0.856740	0.041293	0.151703		0.415254

	Id						AUC			Recall		Precision
0	Non-augmented			0.857377	0.415254	0.151703
1	Augmented_1				0.864785	0.483051	0.170149
2	Augmented_2				0.881684	0.584746	0.138833
3	Augmented_3				0.825791	0.432203	0.099029
4	Augmented_4				0.836515	0.449153	0.095495
5	Augmented_5				0.864569	0.432203	0.160883
6	Augmented_6				0.890159	0.559322	0.138655
7	Augmented_7				0.834049	0.415254	0.094412
8	Augmented_8				0.861228	0.516949	0.114019
9	Augmented_9				0.848074	0.466102	0.154494
10	Augmented_10			0.879347	0.559322	0.132265
11	Augmented_11			0.830865	0.457627	0.110656
12	Augmented_12			0.831431	0.466102	0.092749
13	Augmented_13			0.860876	0.457627	0.156522
14	Augmented_14			0.886532	0.593220	0.133843
15	Augmented_15			0.833311	0.440678	0.103586
16	Augmented_16			0.824248	0.423729	0.083333
17	Augmented_17			0.865918	0.500000	0.161202
18	Augmented_18			0.887275	0.635593	0.127119
19	Augmented_19			0.824731	0.466102	0.086207
20	Augmented_20			0.838548	0.542373	0.092619
21	Augmented_21			0.860723	0.449153	0.154971
22	Augmented_22			0.884661	0.593220	0.139442
23	Augmented_23			0.844333	0.457627	0.110429
24	Augmented_24			0.843185	0.483051	0.099825
25	Combined_with_mean		0.896816	0.466102	0.194346
26	Combined_with_median	0.893064	0.508475	0.186916



without brig_down & cont_down for TTA, ls=0.03

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.692709	0.218859	0.032401	0.660944	0.797748	0.590678	0.038174		0.779661
1	0.778082	0.177603	0.040132	0.731760	0.836414	0.134533	0.061848		0.703390
2	0.788515	0.172744	0.041045	0.761803	0.659178	0.143615	0.039860		0.483051
3	0.819053	0.159834	0.047079	0.783262	0.879127	0.141458	0.059281		0.838983
4	0.817273	0.153258	0.046681	0.757511	0.786699	0.247163	0.033398		0.881356
5	0.854227	0.138339	0.053062	0.806867	0.887174	0.104210	0.073482		0.779661
6	0.873040	0.128392	0.057541	0.832618	0.815490	0.598419	0.065521		0.516949
7	0.890902	0.121887	0.061190	0.864807	0.865529	0.101683	0.076923		0.754237
8	0.920215	0.101672	0.072142	0.899142	0.888466	0.079342	0.096810		0.745763
9	0.941922	0.087410	0.090616	0.905579	0.893932	0.057801	0.129597		0.627119

	Id						AUC			Recall		Precision
0	Non-augmented			0.893914	0.627119	0.129597
1	Augmented_1				0.896011	0.618644	0.125862
2	Augmented_2				0.886005	0.644068	0.086659
3	Augmented_3				0.900946	0.822034	0.081308
4	Augmented_4				0.894899	0.771186	0.082954
5	Augmented_5				0.889154	0.618644	0.124573
6	Augmented_6				0.888723	0.677966	0.093897
7	Augmented_7				0.896709	0.788136	0.078547
8	Augmented_8				0.896672	0.788136	0.084469
9	Augmented_9				0.893703	0.601695	0.118729
10	Augmented_10			0.885145	0.669492	0.087973
11	Augmented_11			0.890140	0.796610	0.075200
12	Augmented_12			0.883960	0.754237	0.075939
13	Augmented_13			0.884391	0.627119	0.117647
14	Augmented_14			0.887138	0.669492	0.083686
15	Augmented_15			0.890123	0.771186	0.073684
16	Augmented_16			0.889702	0.754237	0.075939
17	Augmented_17			0.891136	0.661017	0.118721
18	Augmented_18			0.882992	0.686441	0.082401
19	Augmented_19			0.882487	0.805085	0.068940
20	Augmented_20			0.888854	0.788136	0.070884
21	Augmented_21			0.894428	0.661017	0.128289
22	Augmented_22			0.888192	0.677966	0.088496
23	Augmented_23			0.891508	0.822034	0.081172
24	Augmented_24			0.886155	0.754237	0.077594
25	Combined_with_mean		0.907574	0.737288	0.109023
26	Combined_with_median	0.906504	0.737288	0.107143


without brig_down & cont_down for TTA, ls=0.02

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.701701	0.214610	0.031131	0.682403	0.801787	0.168121	0.045745		0.728814
1	0.767748	0.179953	0.038432	0.740343	0.842836	0.163840	0.091521		0.576271
2	0.809612	0.165079	0.042737	0.763949	0.817648	0.139278	0.059639		0.728814
3	0.824243	0.153672	0.047202	0.763949	0.866824	0.247284	0.048485		0.881356
4	0.829236	0.146398	0.047351	0.776824	0.870920	0.134642	0.070911		0.745763
5	0.845246	0.140107	0.049354	0.811159	0.884104	0.128000	0.068133		0.813559
6	0.866924	0.129086	0.056412	0.847640	0.882025	0.099789	0.086086		0.728814
7	0.889415	0.116398	0.061777	0.881974	0.863078	0.093553	0.085567		0.703390
8	0.904362	0.107618	0.066507	0.892704	0.895452	0.104658	0.080989		0.805085
9	0.924376	0.094006	0.073046	0.920601	0.894873	0.082931	0.079086		0.762712

	Id						AUC			Recall		Precision
0	Non-augmented			0.894904	0.762712	0.079086
1	Augmented_1				0.892440	0.788136	0.080311
2	Augmented_2				0.887412	0.830508	0.063595
3	Augmented_3				0.888495	0.855932	0.062001
4	Augmented_4				0.885727	0.822034	0.063151
5	Augmented_5				0.893646	0.745763	0.079279
6	Augmented_6				0.888673	0.847458	0.066138
7	Augmented_7				0.900382	0.864407	0.065051
8	Augmented_8				0.894946	0.855932	0.068661
9	Augmented_9				0.889229	0.728814	0.077269
10	Augmented_10			0.896078	0.838983	0.065781
11	Augmented_11			0.895610	0.872881	0.062614
12	Augmented_12			0.890737	0.855932	0.065330
13	Augmented_13			0.893314	0.771186	0.072626
14	Augmented_14			0.891553	0.847458	0.060901
15	Augmented_15			0.897808	0.889831	0.060624
16	Augmented_16			0.900175	0.872881	0.064861
17	Augmented_17			0.897006	0.779661	0.075845
18	Augmented_18			0.889268	0.830508	0.058859
19	Augmented_19			0.884376	0.847458	0.053735
20	Augmented_20			0.888401	0.855932	0.060191
21	Augmented_21			0.893019	0.754237	0.077324
22	Augmented_22			0.891066	0.838983	0.063138
23	Augmented_23			0.897070	0.898305	0.061628
24	Augmented_24			0.893979	0.830508	0.063513
25	Combined_with_mean		0.910126	0.822034	0.080833
26	Combined_with_median	0.907798	0.830508	0.072862


without brig_down & cont_down for TTA, ls=0.05

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.672977	0.231907	0.029357	0.675966	0.512875	0.168910	0.020286		0.288136
1	0.760513	0.186361	0.036992	0.710300	0.816717	0.129721	0.070881		0.627119
2	0.786205	0.172101	0.041358	0.731760	0.855992	0.141889	0.065106		0.754237
3	0.820606	0.159458	0.045100	0.793991	0.846806	0.119483	0.069444		0.720339
4	0.847934	0.143913	0.049609	0.802575	0.859471	0.135753	0.058931		0.822034
5	0.857535	0.138640	0.052104	0.826180	0.866238	0.109157	0.074758		0.720339
6	0.863772	0.133432	0.053937	0.826180	0.860519	0.098592	0.078307		0.627119
7	0.900444	0.114244	0.067352	0.871245	0.879688	0.097024	0.082397		0.745763
8	0.912097	0.105068	0.070344	0.894850	0.886487	0.097557	0.081818		0.762712
9	0.939032	0.087745	0.088126	0.914163	0.881225	0.088645	0.078695		0.694915

	Id						AUC			Recall		Precision
0	Non-augmented			0.881119	0.694915	0.078695
1	Augmented_1				0.885116	0.694915	0.080948
2	Augmented_2				0.890827	0.838983	0.069669
3	Augmented_3				0.888846	0.762712	0.070866
4	Augmented_4				0.879677	0.813559	0.069565
5	Augmented_5				0.880741	0.703390	0.084954
6	Augmented_6				0.893563	0.813559	0.072072
7	Augmented_7				0.887870	0.796610	0.076361
8	Augmented_8				0.881908	0.796610	0.071212
9	Augmented_9				0.879601	0.661017	0.079108
10	Augmented_10			0.885141	0.762712	0.063247
11	Augmented_11			0.890515	0.779661	0.071595
12	Augmented_12			0.872538	0.796610	0.066856
13	Augmented_13			0.879724	0.686441	0.073905
14	Augmented_14			0.887445	0.822034	0.066257
15	Augmented_15			0.885174	0.779661	0.071484
16	Augmented_16			0.884191	0.830508	0.067215
17	Augmented_17			0.873491	0.703390	0.077281
18	Augmented_18			0.880074	0.805085	0.063758
19	Augmented_19			0.883636	0.796610	0.065323
20	Augmented_20			0.873475	0.830508	0.064304
21	Augmented_21			0.881019	0.686441	0.079882
22	Augmented_22			0.886549	0.813559	0.067180
23	Augmented_23			0.885373	0.771186	0.070928
24	Augmented_24			0.869531	0.788136	0.066762
25	Combined_with_mean		0.897780	0.813559	0.082403
26	Combined_with_median	0.896541	0.805085	0.078773


g2_ls0.05_a0.25 set np seed (exec 1)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.703705	0.209222	0.030374	0.718884	0.810517	0.114431	0.065900		0.533898
1	0.799700	0.174787	0.043224	0.746781	0.801235	0.547434	0.050837		0.720339
2	0.818037	0.169849	0.045802	0.785408	0.872838	0.113073	0.080897		0.703390
3	0.849321	0.149043	0.051678	0.826180	0.869805	0.113404	0.078544		0.694915
4	0.859473	0.137162	0.051265	0.821888	0.885081	0.088428	0.093240		0.677966
5	0.873759	0.125997	0.055289	0.841202	0.870115	0.094620	0.072616		0.703390
6	0.881489	0.123722	0.056273	0.841202	0.889064	0.095883	0.070878		0.779661
7	0.907237	0.108988	0.066625	0.909871	0.883958	0.104807	0.063014		0.779661
8	0.930504	0.095675	0.078033	0.905579	0.885184	0.069000	0.106984		0.610169
9	0.947234	0.081841	0.095281	0.914163	0.887985	0.069914	0.096154		0.635593

	Id						AUC			Recall		Precision
0	Non-augmented			0.887787	0.635593	0.096154
1	Augmented_1				0.883636	0.652542	0.100391
2	Augmented_2				0.883031	0.711864	0.078285
3	Augmented_3				0.893432	0.805085	0.077173
4	Augmented_4				0.895871	0.754237	0.081651
5	Augmented_5				0.879100	0.601695	0.098611
6	Augmented_6				0.879488	0.720339	0.083992
7	Augmented_7				0.898431	0.788136	0.078947
8	Augmented_8				0.893866	0.745763	0.083969
9	Augmented_9				0.881400	0.652542	0.098089
10	Augmented_10			0.882724	0.720339	0.079365
11	Augmented_11			0.886971	0.779661	0.075225
12	Augmented_12			0.899373	0.771186	0.081468
13	Augmented_13			0.883588	0.661017	0.099872
14	Augmented_14			0.877938	0.703390	0.078008
15	Augmented_15			0.881352	0.737288	0.070445
16	Augmented_16			0.901232	0.779661	0.082363
17	Augmented_17			0.880881	0.661017	0.092090
18	Augmented_18			0.878370	0.745763	0.076455
19	Augmented_19			0.890099	0.805085	0.070007
20	Augmented_20			0.892900	0.779661	0.078165
21	Augmented_21			0.887027	0.694915	0.105263
22	Augmented_22			0.877577	0.703390	0.077861
23	Augmented_23			0.888165	0.805085	0.074921
24	Augmented_24			0.891905	0.728814	0.078324
25	Combined_with_mean		0.906518	0.754237	0.103609
26	Combined_with_median	0.906177	0.754237	0.097374


g2_ls0.05_a0.25 set np seed (exec 2)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.706810	0.205057	0.029851	0.763949	0.847918	0.182375	0.045412		0.830508
1	0.797410	0.168850	0.040491	0.785408	0.831415	0.161921	0.057895		0.745763
2	0.827458	0.159611	0.046839	0.821888	0.804716	0.162830	0.049115		0.728814
3	0.849935	0.144857	0.053414	0.817597	0.842026	0.135862	0.083632		0.593220
4	0.840533	0.150553	0.050389	0.806867	0.858539	0.103179	0.074586		0.686441
5	0.855494	0.135566	0.051122	0.826180	0.866077	0.096218	0.090909		0.635593
6	0.874391	0.124581	0.054061	0.845494	0.873481	0.078522	0.105991		0.584746
7	0.896054	0.113431	0.064839	0.847640	0.881137	0.107629	0.078112		0.771186
8	0.922376	0.098068	0.075787	0.914163	0.877815	0.082714	0.081340		0.720339
9	0.938868	0.088263	0.087071	0.924893	0.885895	0.069008	0.098237		0.661017

	Id						AUC			Recall		Precision
0	Non-augmented			0.886123	0.661017	0.098237
1	Augmented_1				0.873027	0.618644	0.089571
2	Augmented_2				0.876227	0.720339	0.078777
3	Augmented_3				0.887117	0.771186	0.081761
4	Augmented_4				0.884682	0.737288	0.083977
5	Augmented_5				0.886174	0.652542	0.098718
6	Augmented_6				0.879543	0.720339	0.083089
7	Augmented_7				0.886920	0.754237	0.082331
8	Augmented_8				0.879929	0.711864	0.085020
9	Augmented_9				0.870129	0.627119	0.086651
10	Augmented_10			0.878476	0.728814	0.075175
11	Augmented_11			0.893718	0.822034	0.079770
12	Augmented_12			0.883079	0.745763	0.079279
13	Augmented_13			0.891939	0.694915	0.101611
14	Augmented_14			0.876706	0.711864	0.079621
15	Augmented_15			0.886668	0.762712	0.080071
16	Augmented_16			0.887258	0.745763	0.086529
17	Augmented_17			0.877936	0.737288	0.098305
18	Augmented_18			0.877944	0.745763	0.072190
19	Augmented_19			0.887994	0.813559	0.071269
20	Augmented_20			0.889111	0.805085	0.077299
21	Augmented_21			0.876754	0.627119	0.089806
22	Augmented_22			0.880964	0.728814	0.077617
23	Augmented_23			0.888057	0.779661	0.077703
24	Augmented_24			0.883968	0.720339	0.080492
25	Combined_with_mean		0.902234	0.745763	0.104019
26	Combined_with_median	0.902954	0.771186	0.100664


g2_ls0.05_a0.25 set all seed (exec 1)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.727864	0.191877	0.033123	0.721030	0.816593	0.165121	0.047015		0.754237
1	0.772993	0.176127	0.040169	0.733906	0.830736	0.175957	0.069028		0.644068
2	0.837178	0.150762	0.049462	0.809013	0.848181	0.105711	0.075000		0.711864
3	0.818196	0.157137	0.050368	0.748927	0.830122	0.091836	0.065120		0.618644
4	0.849896	0.140327	0.052956	0.811159	0.827482	0.153598	0.050696		0.771186
5	0.853114	0.140117	0.054664	0.811159	0.866001	0.101911	0.102071		0.584746
6	0.877846	0.127115	0.059472	0.845494	0.876885	0.063809	0.191304		0.372881
7	0.902699	0.113348	0.067065	0.866953	0.872045	0.066374	0.100000		0.559322
8	0.915015	0.102810	0.069624	0.899142	0.895219	0.075584	0.091638		0.677966
9	0.937556	0.089698	0.088963	0.918455	0.883231	0.075624	0.096270		0.677966

	Id						AUC			Recall		Precision
0	Non-augmented			0.883171	0.677966	0.096270
1	Augmented_1				0.886200	0.686441	0.100496
2	Augmented_2				0.881731	0.796610	0.072308
3	Augmented_3				0.884748	0.720339	0.086646
4	Augmented_4				0.880543	0.711864	0.086777
5	Augmented_5				0.874421	0.627119	0.088942
6	Augmented_6				0.875618	0.788136	0.072486
7	Augmented_7				0.884094	0.703390	0.082178
8	Augmented_8				0.889612	0.703390	0.082341
9	Augmented_9				0.888208	0.652542	0.101050
10	Augmented_10			0.884001	0.796610	0.077622
11	Augmented_11			0.895392	0.720339	0.095721
12	Augmented_12			0.895049	0.762712	0.096051
13	Augmented_13			0.884840	0.703390	0.085041
14	Augmented_14			0.878615	0.796610	0.065688
15	Augmented_15			0.883402	0.754237	0.082179
16	Augmented_16			0.895418	0.745763	0.080073
17	Augmented_17			0.885619	0.720339	0.094131
18	Augmented_18			0.886372	0.822034	0.068600
19	Augmented_19			0.887353	0.771186	0.084181
20	Augmented_20			0.876839	0.762712	0.079086
21	Augmented_21			0.883964	0.686441	0.096314
22	Augmented_22			0.878623	0.788136	0.071538
23	Augmented_23			0.882661	0.745763	0.089980
24	Augmented_24			0.879799	0.737288	0.084795
25	Combined_with_mean		0.902460	0.754237	0.099888
26	Combined_with_median	0.899591	0.754237	0.095494


g2_ls0.05_a0.25 set all seed (exec 2)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.720141	0.190813	0.033098	0.714592	0.796600	0.220871	0.043336		0.788136
1	0.775642	0.174806	0.041585	0.725322	0.819164	0.167111	0.051534		0.754237
2	0.832705	0.150517	0.048937	0.785408	0.854897	0.110526	0.070467		0.754237
3	0.847182	0.140313	0.053127	0.811159	0.872478	0.129946	0.054651		0.881356
4	0.865028	0.130816	0.056342	0.819743	0.813537	0.095426	0.062095		0.567797
5	0.863212	0.142671	0.054249	0.821888	0.890665	0.082516	0.110599		0.610169
6	0.875221	0.126553	0.057723	0.841202	0.889050	0.087786	0.110041		0.677966
7	0.903403	0.109929	0.064367	0.869099	0.857656	0.103712	0.066225		0.762712
8	0.931555	0.092125	0.083691	0.920601	0.880689	0.068240	0.100000		0.635593
9	0.947733	0.080174	0.099494	0.929185	0.878764	0.069775	0.091969		0.601695

	Id						AUC			Recall		Precision
0	Non-augmented			0.878530	0.601695	0.091969
1	Augmented_1				0.884473	0.669492	0.100765
2	Augmented_2				0.888091	0.745763	0.083650
3	Augmented_3				0.896021	0.720339	0.095398
4	Augmented_4				0.885378	0.720339	0.090042
5	Augmented_5				0.874255	0.669492	0.099747
6	Augmented_6				0.885271	0.771186	0.083640
7	Augmented_7				0.902659	0.779661	0.097046
8	Augmented_8				0.883288	0.737288	0.089139
9	Augmented_9				0.873092	0.627119	0.101093
10	Augmented_10			0.889709	0.737288	0.084548
11	Augmented_11			0.900495	0.686441	0.092255
12	Augmented_12			0.877898	0.669492	0.084222
13	Augmented_13			0.884266	0.728814	0.101296
14	Augmented_14			0.886283	0.788136	0.080172
15	Augmented_15			0.900327	0.720339	0.088819
16	Augmented_16			0.876708	0.720339	0.083333
17	Augmented_17			0.880500	0.669492	0.092614
18	Augmented_18			0.882134	0.754237	0.075296
19	Augmented_19			0.897515	0.711864	0.079320
20	Augmented_20			0.879674	0.762712	0.080357
21	Augmented_21			0.876547	0.652542	0.097222
22	Augmented_22			0.879115	0.762712	0.082117
23	Augmented_23			0.898224	0.720339	0.092191
24	Augmented_24			0.895336	0.762712	0.090909
25	Combined_with_mean		0.906107	0.754237	0.110973
26	Combined_with_median	0.904021	0.728814	0.100350


g2_ls0.05_a0.25 set all seed updated (exec 1)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.731946	0.199331	0.033570	0.781116	0.793341	0.172611	0.044902		0.720339
1	0.828028	0.159973	0.046699	0.809013	0.871327	0.136531	0.060567		0.796610
2	0.825979	0.162427	0.046465	0.798283	0.856844	0.183893	0.051901		0.855932
3	0.844235	0.150185	0.049615	0.802575	0.851736	0.084354	0.130982		0.440678
4	0.854078	0.141327	0.049017	0.824034	0.865251	0.111512	0.071001		0.703390
5	0.862418	0.146057	0.057372	0.843348	0.830535	0.177232	0.052934		0.703390
6	0.825577	0.164430	0.049051	0.787554	0.876623	0.106005	0.084168		0.711864
7	0.875016	0.131337	0.053952	0.862661	0.872898	0.092695	0.074906		0.677966
8	0.894050	0.117761	0.061583	0.873391	0.887042	0.082247	0.083083		0.703390
9	0.912220	0.104561	0.066773	0.901288	0.888378	0.081273	0.081496		0.720339

	Id						AUC			Recall		Precision
0	Non-augmented			0.888420	0.720339	0.081496
1	Augmented_1				0.890144	0.711864	0.086154
2	Augmented_2				0.889572	0.838983	0.065693
3	Augmented_3				0.894521	0.796610	0.078925
4	Augmented_4				0.879615	0.788136	0.074819
5	Augmented_5				0.884574	0.677966	0.089586
6	Augmented_6				0.891982	0.830508	0.071585
7	Augmented_7				0.881357	0.728814	0.081827
8	Augmented_8				0.872280	0.745763	0.078712
9	Augmented_9				0.881804	0.694915	0.084362
10	Augmented_10			0.889060	0.838983	0.065390
11	Augmented_11			0.892133	0.779661	0.077441
12	Augmented_12			0.867324	0.728814	0.069467
13	Augmented_13			0.883419	0.728814	0.079117
14	Augmented_14			0.887635	0.864407	0.061743
15	Augmented_15			0.888323	0.822034	0.071693
16	Augmented_16			0.867078	0.788136	0.066145
17	Augmented_17			0.890201	0.737288	0.082936
18	Augmented_18			0.891112	0.838983	0.063748
19	Augmented_19			0.899837	0.813559	0.078303
20	Augmented_20			0.878900	0.771186	0.074104
21	Augmented_21			0.895042	0.728814	0.087935
22	Augmented_22			0.888429	0.830508	0.064304
23	Augmented_23			0.892410	0.754237	0.074539
24	Augmented_24			0.868686	0.737288	0.068504
25	Combined_with_mean		0.901226	0.779661	0.086629
26	Combined_with_median	0.902515	0.796610	0.083407


g2_ls0.05_a0.25 set all seed updated (exec 2)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.702587	0.214144	0.030493	0.710300	0.791230	0.157684	0.055404		0.686441
1	0.788390	0.176647	0.041550	0.731760	0.737987	0.205934	0.042434		0.703390
2	0.818080	0.164305	0.043539	0.806867	0.808661	0.130939	0.055065		0.686441
3	0.830526	0.161334	0.050955	0.796137	0.863917	0.124055	0.074707		0.703390
4	0.838555	0.143334	0.046554	0.817597	0.861481	0.074654	0.093708		0.593220
5	0.825492	0.156245	0.048492	0.783262	0.869472	0.098796	0.075996		0.694915
6	0.864897	0.137947	0.055937	0.821888	0.877324	0.087781	0.096386		0.677966
7	0.891157	0.118516	0.061168	0.849785	0.842086	0.089910	0.077754		0.610169
8	0.914931	0.105157	0.074088	0.875537	0.873520	0.074843	0.096277		0.635593
9	0.933524	0.091864	0.086665	0.916309	0.876057	0.071603	0.094667		0.601695

	Id						AUC			Recall		Precision
0	Non-augmented			0.876282	0.601695	0.094667
1	Augmented_1				0.887191	0.610169	0.094987
2	Augmented_2				0.893626	0.728814	0.083333
3	Augmented_3				0.902057	0.711864	0.096663
4	Augmented_4				0.889177	0.711864	0.091205
5	Augmented_5				0.879037	0.644068	0.106443
6	Augmented_6				0.891724	0.703390	0.083333
7	Augmented_7				0.894173	0.720339	0.102533
8	Augmented_8				0.887214	0.737288	0.097973
9	Augmented_9				0.885318	0.627119	0.103497
10	Augmented_10			0.890099	0.737288	0.085714
11	Augmented_11			0.901014	0.703390	0.097532
12	Augmented_12			0.884515	0.694915	0.089520
13	Augmented_13			0.877222	0.644068	0.095119
14	Augmented_14			0.890782	0.720339	0.077839
15	Augmented_15			0.891575	0.720339	0.087449
16	Augmented_16			0.887508	0.771186	0.086749
17	Augmented_17			0.884808	0.677966	0.100756
18	Augmented_18			0.888722	0.686441	0.075349
19	Augmented_19			0.894056	0.711864	0.091304
20	Augmented_20			0.884783	0.686441	0.083505
21	Augmented_21			0.876867	0.601695	0.093421
22	Augmented_22			0.891547	0.728814	0.083012
23	Augmented_23			0.898194	0.711864	0.096552
24	Augmented_24			0.895271	0.728814	0.092573
25	Combined_with_mean		0.905250	0.677966	0.105820
26	Combined_with_median	0.901322	0.669492	0.099622


g2_ls0.05_a0.25 set all seed updated2 (set env var TF_DETERMINISTIC_OPS) (exec 1)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.705047	0.206364	0.030653	0.723176	0.800561	0.205375	0.035964		0.872881
1	0.781482	0.177835	0.038951	0.755365	0.827368	0.164505	0.043178		0.847458
2	0.830362	0.157812	0.044823	0.830472	0.833421	0.241401	0.042918		0.847458
3	0.848804	0.148793	0.051367	0.793991	0.839001	0.129824	0.047996		0.771186
4	0.840695	0.147894	0.045876	0.806867	0.876425	0.104914	0.075623		0.720339
5	0.772049	0.173864	0.037033	0.759657	0.845737	0.161621	0.045332		0.855932
6	0.855572	0.138301	0.050249	0.824034	0.868255	0.097801	0.081081		0.711864
7	0.871302	0.130393	0.054199	0.843348	0.876474	0.098745	0.084142		0.661017
8	0.895560	0.117224	0.063540	0.860515	0.883368	0.113605	0.069971		0.813559
9	0.912322	0.105494	0.068234	0.890558	0.893183	0.093106	0.074819		0.788136

	Id						AUC			Recall		Precision
0	Non-augmented			0.893080	0.788136	0.074819
1	Augmented_1				0.890479	0.771186	0.074774
2	Augmented_2				0.887017	0.872881	0.060695
3	Augmented_3				0.893373	0.838983	0.067485
4	Augmented_4				0.883452	0.813559	0.062459
5	Augmented_5				0.893152	0.779661	0.075163
6	Augmented_6				0.882226	0.855932	0.058146
7	Augmented_7				0.890874	0.830508	0.066351
8	Augmented_8				0.889436	0.855932	0.066404
9	Augmented_9				0.887197	0.771186	0.077844
10	Augmented_10			0.881787	0.855932	0.061698
11	Augmented_11			0.898486	0.847458	0.070028
12	Augmented_12			0.891074	0.855932	0.068942
13	Augmented_13			0.887559	0.813559	0.080335
14	Augmented_14			0.885511	0.872881	0.059676
15	Augmented_15			0.895415	0.847458	0.067295
16	Augmented_16			0.890103	0.830508	0.062740
17	Augmented_17			0.882241	0.796610	0.066244
18	Augmented_18			0.882048	0.872881	0.054411
19	Augmented_19			0.880473	0.830508	0.056977
20	Augmented_20			0.883272	0.847458	0.056085
21	Augmented_21			0.892638	0.830508	0.076682
22	Augmented_22			0.885871	0.889831	0.059659
23	Augmented_23			0.899188	0.855932	0.067021
24	Augmented_24			0.893476	0.864407	0.065849
25	Combined_with_mean		0.901921	0.822034	0.072442
26	Combined_with_median	0.904251	0.847458	0.069061


g2_ls0.05_a0.25 set all seed updated2 (set env var TF_DETERMINISTIC_OPS) (exec 2)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.705047	0.206364	0.030653	0.723176	0.800561	0.205375	0.035964		0.872881
1	0.781482	0.177835	0.038951	0.755365	0.827368	0.164505	0.043178		0.847458
2	0.830362	0.157812	0.044823	0.830472	0.833421	0.241401	0.042918		0.847458
3	0.848804	0.148793	0.051367	0.793991	0.839001	0.129824	0.047996		0.771186
4	0.840695	0.147894	0.045876	0.806867	0.876425	0.104914	0.075623		0.720339
5	0.772049	0.173864	0.037033	0.759657	0.845737	0.161621	0.045332		0.855932
6	0.855572	0.138301	0.050249	0.824034	0.868255	0.097801	0.081081		0.711864
7	0.871302	0.130393	0.054199	0.843348	0.876474	0.098745	0.084142		0.661017
8	0.895560	0.117224	0.063540	0.860515	0.883368	0.113605	0.069971		0.813559
9	0.912322	0.105494	0.068234	0.890558	0.893183	0.093106	0.074819		0.788136

	Id						AUC			Recall		Precision
0	Non-augmented			0.893080	0.788136	0.074819
1	Augmented_1				0.890479	0.771186	0.074774
2	Augmented_2				0.887017	0.872881	0.060695
3	Augmented_3				0.893373	0.838983	0.067485
4	Augmented_4				0.883452	0.813559	0.062459
5	Augmented_5				0.893152	0.779661	0.075163
6	Augmented_6				0.882226	0.855932	0.058146
7	Augmented_7				0.890874	0.830508	0.066351
8	Augmented_8				0.889436	0.855932	0.066404
9	Augmented_9				0.887197	0.771186	0.077844
10	Augmented_10			0.881787	0.855932	0.061698
11	Augmented_11			0.898486	0.847458	0.070028
12	Augmented_12			0.891074	0.855932	0.068942
13	Augmented_13			0.887559	0.813559	0.080335
14	Augmented_14			0.885511	0.872881	0.059676
15	Augmented_15			0.895415	0.847458	0.067295
16	Augmented_16			0.890103	0.830508	0.062740
17	Augmented_17			0.882241	0.796610	0.066244
18	Augmented_18			0.882048	0.872881	0.054411
19	Augmented_19			0.880473	0.830508	0.056977
20	Augmented_20			0.883272	0.847458	0.056085
21	Augmented_21			0.892638	0.830508	0.076682
22	Augmented_22			0.885871	0.889831	0.059659
23	Augmented_23			0.899188	0.855932	0.067021
24	Augmented_24			0.893476	0.864407	0.065849
25	Combined_with_mean		0.901921	0.822034	0.072442
26	Combined_with_median	0.904251	0.847458	0.069061


g2_ls0.05_a0.25 set all seed updated3 (exec1)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.747954	0.197418	0.033384	0.759657	0.855918	0.176078	0.048300		0.830508
1	0.810175	0.166576	0.045552	0.772532	0.829119	0.168656	0.055880		0.728814
2	0.839429	0.160426	0.050747	0.809013	0.877710	0.159707	0.053180		0.864407
3	0.862079	0.143409	0.054655	0.817597	0.863367	0.111356	0.075926		0.694915
4	0.862702	0.138098	0.056447	0.830472	0.881100	0.088119	0.103208		0.627119
5	0.884631	0.123792	0.060003	0.834764	0.888254	0.102991	0.080790		0.762712
6	0.900356	0.113036	0.065085	0.871245	0.883684	0.102771	0.080729		0.788136
7	0.924095	0.098934	0.075509	0.899142	0.899278	0.074706	0.102625		0.728814
8	0.934945	0.091054	0.084069	0.899142	0.896951	0.078165	0.096921		0.720339
9	0.958763	0.073557	0.112411	0.948498	0.897755	0.067633	0.108538		0.635593

	Id						AUC			Recall		Precision
0	Non-augmented			0.897773	0.635593	0.108538
1	Augmented_1				0.861502	0.661017	0.104139
2	Augmented_2				0.884079	0.661017	0.098609
3	Augmented_3				0.889031	0.635593	0.112108
4	Augmented_4				0.891057	0.737288	0.081384
5	Augmented_5				0.858977	0.627119	0.097884
6	Augmented_6				0.852532	0.618644	0.090458
7	Augmented_7				0.890089	0.669492	0.114162
8	Augmented_8				0.884419	0.737288	0.079817
9	Augmented_9				0.847118	0.601695	0.088750
10	Augmented_10			0.859954	0.644068	0.091566
11	Augmented_11			0.896089	0.669492	0.113506
12	Augmented_12			0.882069	0.745763	0.078782
13	Augmented_13			0.846400	0.567797	0.085787
14	Augmented_14			0.852455	0.601695	0.087117
15	Augmented_15			0.899385	0.652542	0.115097
16	Augmented_16			0.886542	0.728814	0.079777
17	Augmented_17			0.866881	0.669492	0.084402
18	Augmented_18			0.849687	0.661017	0.076923
19	Augmented_19			0.887077	0.686441	0.104247
20	Augmented_20			0.880022	0.754237	0.073071
21	Augmented_21			0.867269	0.661017	0.095238
22	Augmented_22			0.868402	0.635593	0.088863
23	Augmented_23			0.896979	0.677966	0.113314
24	Augmented_24			0.885878	0.728814	0.077130
25	Combined_with_mean		0.903136	0.677966	0.118694
26	Combined_with_median	0.904257	0.694915	0.116976
	

g2_ls0.05_a0.25 set all seed updated3 (exec2)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.735008	0.203367	0.033044	0.744635	0.829866	0.179260	0.044783		0.796610
1	0.815183	0.162027	0.042940	0.811159	0.870692	0.121478	0.069193		0.711864
2	0.843886	0.153135	0.051311	0.776824	0.887896	0.190387	0.047387		0.906780
3	0.857177	0.141957	0.053731	0.800429	0.859504	0.105095	0.068847		0.677966
4	0.872592	0.138652	0.059310	0.815451	0.879779	0.109443	0.102302		0.677966
5	0.890124	0.124605	0.061199	0.832618	0.871728	0.092870	0.084492		0.669492
6	0.900339	0.115235	0.064029	0.864807	0.892268	0.079500	0.103832		0.711864
7	0.921318	0.100225	0.074845	0.879828	0.901008	0.079303	0.101058		0.728814
8	0.936448	0.090177	0.086350	0.916309	0.890345	0.078799	0.091416		0.694915
9	0.957423	0.073099	0.106216	0.931330	0.891747	0.069154	0.104558		0.661017

	Id						AUC			Recall		Precision
0	Non-augmented			0.891562	0.661017	0.104558
1	Augmented_1				0.858494	0.652542	0.087600
2	Augmented_2				0.870814	0.652542	0.085938
3	Augmented_3				0.889226	0.703390	0.107932
4	Augmented_4				0.887289	0.728814	0.080675
5	Augmented_5				0.861130	0.610169	0.083141
6	Augmented_6				0.864417	0.652542	0.092105
7	Augmented_7				0.889816	0.686441	0.103846
8	Augmented_8				0.883289	0.703390	0.077498
9	Augmented_9				0.859239	0.610169	0.078176
10	Augmented_10			0.866668	0.610169	0.084507
11	Augmented_11			0.888745	0.703390	0.110226
12	Augmented_12			0.890399	0.720339	0.081107
13	Augmented_13			0.848771	0.627119	0.080610
14	Augmented_14			0.875706	0.661017	0.087444
15	Augmented_15			0.884947	0.694915	0.107330
16	Augmented_16			0.880632	0.677966	0.075829
17	Augmented_17			0.854105	0.686441	0.076777
18	Augmented_18			0.868032	0.677966	0.081301
19	Augmented_19			0.881182	0.686441	0.097590
20	Augmented_20			0.878778	0.720339	0.071851
21	Augmented_21			0.866539	0.644068	0.081545
22	Augmented_22			0.873682	0.661017	0.086379
23	Augmented_23			0.888205	0.669492	0.098874
24	Augmented_24			0.889199	0.745763	0.079351
25	Combined_with_mean		0.902420	0.661017	0.110795
26	Combined_with_median	0.898941	0.661017	0.102228


g2_ls0.05_a0.25 set all seed  with seed = 1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.736000	0.190891	0.031900	0.735426	0.848906	0.168393	0.055869		0.789855
1	0.799398	0.166856	0.039886	0.782511	0.729264	0.284997	0.038018		0.811594
2	0.823045	0.159901	0.044648	0.766816	0.845848	0.124608	0.071380		0.760870
3	0.854084	0.148258	0.054891	0.822870	0.854552	0.108178	0.090476		0.688406
4	0.866692	0.143073	0.057692	0.827354	0.865952	0.104446	0.089089		0.644928
5	0.890090	0.130730	0.065754	0.818386	0.851274	0.119900	0.092879		0.652174
6	0.911113	0.125028	0.077935	0.876682	0.878806	0.067424	0.170051		0.485507
7	0.940334	0.099187	0.089870	0.881166	0.870350	0.040647	0.257812		0.239130
8	0.966527	0.071866	0.129310	0.941704	0.876652	0.052185	0.161616		0.463768
9	0.985566	0.047316	0.188500	0.977578	0.864323	0.044431	0.175325		0.391304

	Id						AUC			Recall		Precision
0	Non-augmented			0.864104	0.391304	0.175325
1	Augmented_1				0.848499	0.282609	0.175676
2	Augmented_2				0.819397	0.246377	0.122302
3	Augmented_3				0.867163	0.369565	0.165049
4	Augmented_4				0.879582	0.485507	0.153670
5	Augmented_5				0.830829	0.289855	0.185185
6	Augmented_6				0.830570	0.275362	0.142322
7	Augmented_7				0.853791	0.340580	0.159864
8	Augmented_8				0.884349	0.528986	0.165909
9	Augmented_9				0.852617	0.311594	0.193694
10	Augmented_10			0.838690	0.304348	0.142373
11	Augmented_11			0.848882	0.391304	0.173633
12	Augmented_12			0.882413	0.471014	0.154762
13	Augmented_13			0.824282	0.224638	0.147619
14	Augmented_14			0.818822	0.289855	0.160643
15	Augmented_15			0.851195	0.311594	0.173387
16	Augmented_16			0.872611	0.456522	0.159091
17	Augmented_17			0.828234	0.289855	0.142857
18	Augmented_18			0.824169	0.268116	0.108187
19	Augmented_19			0.858390	0.398551	0.146277
20	Augmented_20			0.878647	0.492754	0.137652
21	Augmented_21			0.836710	0.297101	0.146429
22	Augmented_22			0.816908	0.304348	0.128049
23	Augmented_23			0.859746	0.427536	0.175074
24	Augmented_24			0.882724	0.492754	0.137931
25	Combined_with_mean		0.904273	0.318841	0.278481
26	Combined_with_median	0.900576	0.369565	0.269841



g2_ls0.02_a0.25 set all seed

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.747259	0.204600	0.034265	0.744635	0.861863	0.140042	0.066923		0.737288
1	0.835718	0.154365	0.046937	0.809013	0.860310	0.118311	0.072727		0.677966
2	0.855312	0.145795	0.051359	0.806867	0.859118	0.121901	0.066817		0.754237
3	0.845240	0.148600	0.047460	0.821888	0.875655	0.142475	0.072044		0.779661
4	0.872772	0.133853	0.057644	0.832618	0.890504	0.094474	0.093617		0.745763
5	0.892244	0.119715	0.063775	0.851931	0.889906	0.104217	0.081127		0.805085
6	0.907647	0.107953	0.070041	0.881974	0.831703	0.089375	0.082524		0.576271
7	0.905929	0.114087	0.066266	0.875537	0.885372	0.085595	0.096197		0.728814
8	0.928881	0.098756	0.083957	0.914163	0.890907	0.084110	0.097762		0.703390
9	0.948997	0.080918	0.095124	0.937768	0.896222	0.078258	0.093448		0.737288

	Id						AUC			Recall		Precision
0	Non-augmented			0.896458	0.737288	0.093448
1	Augmented_1				0.854321	0.703390	0.077138
2	Augmented_2				0.879397	0.737288	0.085462
3	Augmented_3				0.897019	0.762712	0.094340
4	Augmented_4				0.890521	0.822034	0.074102
5	Augmented_5				0.848730	0.694915	0.074749
6	Augmented_6				0.874451	0.694915	0.079844
7	Augmented_7				0.893428	0.771186	0.094595
8	Augmented_8				0.887112	0.788136	0.069925
9	Augmented_9				0.852663	0.661017	0.075655
10	Augmented_10			0.851401	0.627119	0.076763
11	Augmented_11			0.897165	0.745763	0.099660
12	Augmented_12			0.890633	0.796610	0.075381
13	Augmented_13			0.857194	0.694915	0.072438
14	Augmented_14			0.856728	0.703390	0.075592
15	Augmented_15			0.893665	0.762712	0.093652
16	Augmented_16			0.883277	0.796610	0.068915
17	Augmented_17			0.849199	0.661017	0.077228
18	Augmented_18			0.871731	0.694915	0.082661
19	Augmented_19			0.890610	0.737288	0.092455
20	Augmented_20			0.889648	0.813559	0.072727
21	Augmented_21			0.839218	0.728814	0.066874
22	Augmented_22			0.857987	0.720339	0.068938
23	Augmented_23			0.883093	0.737288	0.085462
24	Augmented_24			0.880167	0.805085	0.065835
25	Combined_with_mean		0.905014	0.745763	0.103165
26	Combined_with_median	0.906691	0.788136	0.096674


g2_ls0.05_a0.25 set all seed

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.727022	0.206285	0.033377	0.757511	0.848195	0.163390	0.059269		0.728814
1	0.818429	0.161740	0.041840	0.800429	0.857757	0.156966	0.059783		0.838983
2	0.824974	0.170718	0.045503	0.826180	0.865284	0.140630	0.054466		0.847458
3	0.842697	0.152626	0.048916	0.828326	0.860300	0.112405	0.056511		0.779661
4	0.862853	0.136118	0.056016	0.841202	0.883568	0.089711	0.104895		0.635593
5	0.883089	0.127100	0.060082	0.849785	0.871233	0.122497	0.057814		0.771186
6	0.891382	0.115192	0.058377	0.890558	0.878573	0.106296	0.059673		0.805085
7	0.910927	0.104845	0.066784	0.892704	0.902559	0.085187	0.088462		0.779661
8	0.928156	0.096760	0.081951	0.901288	0.908544	0.078386	0.105263		0.728814
9	0.950776	0.078375	0.100833	0.935622	0.905226	0.066714	0.116310		0.737288

	Id						AUC			Recall		Precision
0	Non-augmented			0.905356	0.737288	0.116310
1	Augmented_1				0.874366	0.576271	0.103817
2	Augmented_2				0.883715	0.567797	0.106349
3	Augmented_3				0.895524	0.737288	0.114776
4	Augmented_4				0.896672	0.813559	0.081772
5	Augmented_5				0.860547	0.550847	0.098935
6	Augmented_6				0.876665	0.593220	0.115321
7	Augmented_7				0.907574	0.703390	0.108214
8	Augmented_8				0.899549	0.813559	0.079470
9	Augmented_9				0.853176	0.601695	0.105812
10	Augmented_10			0.864821	0.593220	0.113636
11	Augmented_11			0.904676	0.771186	0.121172
12	Augmented_12			0.899791	0.822034	0.081172
13	Augmented_13			0.859086	0.593220	0.105263
14	Augmented_14			0.870084	0.584746	0.108833
15	Augmented_15			0.897415	0.728814	0.113456
16	Augmented_16			0.894745	0.805085	0.080782
17	Augmented_17			0.867881	0.516949	0.088406
18	Augmented_18			0.873455	0.627119	0.109630
19	Augmented_19			0.907430	0.728814	0.106436
20	Augmented_20			0.900605	0.855932	0.081583
21	Augmented_21			0.867088	0.584746	0.094005
22	Augmented_22			0.872360	0.652542	0.107542
23	Augmented_23			0.887727	0.754237	0.102890
24	Augmented_24			0.892672	0.830508	0.075327
25	Combined_with_mean		0.912037	0.711864	0.133545
26	Combined_with_median	0.908296	0.754237	0.122590


g2_ls0.05_a0.25 set all seed - full train

	auc			loss		precision	recall
0	0.735822	0.202295	0.033539	0.744863
1	0.824608	0.163493	0.045804	0.827055
2	0.847232	0.148973	0.048485	0.830479
3	0.870251	0.134794	0.055084	0.821918
4	0.881305	0.129983	0.060496	0.830479
5	0.870217	0.137533	0.054620	0.821918
6	0.885961	0.130976	0.064807	0.823630
7	0.907669	0.109503	0.069408	0.861301
8	0.929193	0.095929	0.081094	0.893836
9	0.945099	0.083115	0.091807	0.907534

private score : 0.8861, public score : 0.9034


with stage2 - LR 1e-5_1e-6_1e-5 - 10 epoch

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.737249	0.205222	0.033737	0.770386	0.814275	0.225239	0.040242		0.788136
1	0.804882	0.172321	0.043848	0.774678	0.754204	0.181343	0.041863		0.754237
2	0.813475	0.168711	0.050801	0.755365	0.864269	0.175638	0.044843		0.847458
3	0.828371	0.164891	0.048736	0.793991	0.861126	0.129914	0.072049		0.703390
4	0.854900	0.142185	0.051152	0.819743	0.869739	0.087023	0.152284		0.508475
5	0.861986	0.134378	0.056001	0.824034	0.867214	0.139117	0.048651		0.855932
6	0.882901	0.121492	0.057944	0.856223	0.885431	0.116060	0.062708		0.796610
7	0.897147	0.114571	0.061769	0.849785	0.898209	0.091044	0.089768		0.788136
8	0.924150	0.097515	0.077809	0.884120	0.909992	0.084910	0.091556		0.762712
9	0.942678	0.083768	0.091626	0.922747	0.907261	0.071927	0.106007		0.762712

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.893026	0.122174	0.082224	0.742489	0.860595	0.083825	0.079060		0.627119
1	0.901448	0.113962	0.083988	0.757511	0.867804	0.089089	0.076172		0.661017
2	0.913356	0.104050	0.088702	0.839056	0.873891	0.087905	0.078295		0.669492
3	0.921639	0.097878	0.088400	0.845494	0.878357	0.085404	0.077535		0.661017
4	0.925595	0.094341	0.089492	0.862661	0.882665	0.085196	0.081430		0.694915
5	0.931359	0.089862	0.088845	0.873391	0.884569	0.090349	0.076137		0.694915
6	0.941494	0.083399	0.090736	0.918455	0.886951	0.083846	0.081773		0.703390
7	0.949695	0.078720	0.094618	0.950644	0.887026	0.082887	0.083669		0.703390
8	0.950348	0.077697	0.092969	0.933476	0.887969	0.085733	0.081613		0.703390
9	0.952608	0.076930	0.094583	0.944206	0.887098	0.088898	0.078212		0.711864

	Id						AUC			Recall		Precision
0	Non-augmented			0.887028	0.711864	0.078212
1	Augmented_1				0.859221	0.754237	0.065877
2	Augmented_2				0.843986	0.686441	0.062986
3	Augmented_3				0.884581	0.703390	0.078302
4	Augmented_4				0.872318	0.813559	0.056009
5	Augmented_5				0.851119	0.737288	0.063690
6	Augmented_6				0.848257	0.754237	0.066667
7	Augmented_7				0.874702	0.686441	0.073703
8	Augmented_8				0.862341	0.813559	0.055109
9	Augmented_9				0.845773	0.737288	0.063089
10	Augmented_10			0.846524	0.711864	0.062874
11	Augmented_11			0.878454	0.737288	0.075065
12	Augmented_12			0.871433	0.847458	0.055218
13	Augmented_13			0.839602	0.669492	0.055556
14	Augmented_14			0.831664	0.694915	0.060206
15	Augmented_15			0.879175	0.720339	0.076715
16	Augmented_16			0.868535	0.872881	0.059400
17	Augmented_17			0.854817	0.694915	0.061240
18	Augmented_18			0.859492	0.694915	0.063418
19	Augmented_19			0.884389	0.728814	0.076444
20	Augmented_20			0.881891	0.872881	0.059366
21	Augmented_21			0.831432	0.652542	0.053030
22	Augmented_22			0.851426	0.762712	0.064011
23	Augmented_23			0.881296	0.771186	0.080889
24	Augmented_24			0.870898	0.847458	0.055188
25	Combined_with_mean		0.892099	0.771186	0.079685
26	Combined_with_median	0.892895	0.779661	0.075596


with stage2 - LR 1e-5_1e-6_1e-5 - 5 epoch

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.749612	0.197589	0.034984	0.748927	0.830480	0.223308	0.057729		0.762712
1	0.807544	0.168064	0.044409	0.766094	0.823912	0.138036	0.058055		0.677966
2	0.831483	0.158333	0.046703	0.766094	0.850701	0.137874	0.056935		0.796610
3	0.861560	0.143223	0.053901	0.839056	0.868305	0.088368	0.108911		0.559322
4	0.864017	0.134921	0.054925	0.830472	0.871470	0.103207	0.077519		0.677966
5	0.877677	0.126060	0.058611	0.834764	0.886500	0.098783	0.076923		0.737288
6	0.900997	0.112630	0.064073	0.879828	0.881231	0.099264	0.071140		0.745763
7	0.914436	0.106993	0.072688	0.866953	0.888905	0.081025	0.112069		0.661017
8	0.932292	0.096472	0.083547	0.909871	0.896950	0.087214	0.092836		0.779661
9	0.948996	0.082327	0.100996	0.935622	0.894925	0.073772	0.110968		0.728814

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.898866	0.123134	0.091218	0.748927	0.846990	0.087432	0.079348		0.618644
1	0.903460	0.117800	0.091654	0.768240	0.856832	0.092015	0.076846		0.652542
2	0.916998	0.105129	0.091395	0.806867	0.864755	0.093463	0.076263		0.677966
3	0.929478	0.095419	0.093736	0.866953	0.868045	0.088836	0.081028		0.694915
4	0.932876	0.091505	0.092677	0.869099	0.868707	0.092167	0.077140		0.694915

	Id						AUC			Recall		Precision
0	Non-augmented			0.868984	0.694915	0.077140
1	Augmented_1				0.853924	0.762712	0.066667
2	Augmented_2				0.843794	0.669492	0.060957
3	Augmented_3				0.884829	0.737288	0.082543
4	Augmented_4				0.869647	0.796610	0.056627
5	Augmented_5				0.839425	0.677966	0.059128
6	Augmented_6				0.835042	0.677966	0.058997
7	Augmented_7				0.864818	0.686441	0.075560
8	Augmented_8				0.855806	0.737288	0.051540
9	Augmented_9				0.834684	0.694915	0.059291
10	Augmented_10			0.836647	0.728814	0.063004
11	Augmented_11			0.873959	0.737288	0.078097
12	Augmented_12			0.866577	0.813559	0.055491
13	Augmented_13			0.842990	0.694915	0.057869
14	Augmented_14			0.858774	0.728814	0.063282
15	Augmented_15			0.877370	0.737288	0.077748
16	Augmented_16			0.856012	0.796610	0.055886
17	Augmented_17			0.851029	0.711864	0.064368
18	Augmented_18			0.857775	0.737288	0.066059
19	Augmented_19			0.874936	0.703390	0.076498
20	Augmented_20			0.870939	0.779661	0.056132
21	Augmented_21			0.843644	0.737288	0.059712
22	Augmented_22			0.853084	0.728814	0.061693
23	Augmented_23			0.873162	0.703390	0.072300
24	Augmented_24			0.857302	0.779661	0.055189
25	Combined_with_mean		0.886146	0.728814	0.075175
26	Combined_with_median	0.883648	0.737288	0.071605



with stage2 - LR 1e-5_1e-4_1e-5 - 10 epoch

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.725732	0.210416	0.032027	0.748927	0.823428	0.181754	0.053178		0.737288
1	0.831564	0.159907	0.044599	0.824034	0.843514	0.146931	0.058483		0.771186
2	0.787891	0.185235	0.040140	0.763949	0.853987	0.272042	0.036914		0.940678
3	0.808876	0.170941	0.043325	0.791846	0.834630	0.249868	0.057361		0.762712
4	0.837704	0.149727	0.047688	0.781116	0.884689	0.112120	0.092571		0.686441
5	0.843612	0.150593	0.053162	0.791846	0.839735	0.156755	0.077491		0.711864
6	0.825503	0.161205	0.050383	0.789700	0.856715	0.178238	0.057298		0.805085
7	0.875767	0.135921	0.059469	0.836910	0.881187	0.111796	0.076985		0.805085
8	0.899634	0.118374	0.068209	0.841202	0.885827	0.106570	0.074662		0.796610
9	0.922812	0.103521	0.077439	0.892704	0.893295	0.095775	0.080450		0.788136

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.880320	0.125917	0.067949	0.796137	0.854595	0.111093	0.063528		0.720339
1	0.901051	0.111967	0.069619	0.843348	0.872266	0.109791	0.070455		0.788136
2	0.931432	0.096401	0.076555	0.927039	0.871988	0.112053	0.067153		0.779661
3	0.943690	0.089696	0.082871	0.929185	0.875114	0.098649	0.074106		0.720339
4	0.947522	0.086825	0.088229	0.937768	0.890641	0.100346	0.082428		0.771186
5	0.968389	0.071260	0.111056	0.965665	0.895081	0.083295	0.100592		0.720339
6	0.971735	0.067420	0.132199	0.950644	0.886142	0.084003	0.104651		0.686441
7	0.983005	0.056094	0.156239	0.969957	0.880213	0.072115	0.110759		0.593220
8	0.986946	0.050611	0.183583	0.974249	0.881681	0.071496	0.117241		0.576271
9	0.989373	0.047121	0.197664	0.980687	0.880668	0.072662	0.125430		0.618644

	Id						AUC			Recall		Precision
0	Non-augmented			0.880847	0.618644	0.125430
1	Augmented_1				0.861771	0.567797	0.101979
2	Augmented_2				0.865177	0.584746	0.108833
3	Augmented_3				0.884526	0.644068	0.134276
4	Augmented_4				0.858660	0.669492	0.077073
5	Augmented_5				0.838440	0.542373	0.089013
6	Augmented_6				0.849521	0.593220	0.104322
7	Augmented_7				0.882508	0.601695	0.118729
8	Augmented_8				0.854709	0.686441	0.079568
9	Augmented_9				0.843076	0.533898	0.088235
10	Augmented_10			0.843763	0.610169	0.095745
11	Augmented_11			0.893840	0.669492	0.127214
12	Augmented_12			0.852407	0.728814	0.076854
13	Augmented_13			0.846534	0.525424	0.082447
14	Augmented_14			0.854425	0.601695	0.098474
15	Augmented_15			0.883893	0.627119	0.118211
16	Augmented_16			0.863147	0.703390	0.079731
17	Augmented_17			0.856065	0.584746	0.103759
18	Augmented_18			0.867354	0.567797	0.105678
19	Augmented_19			0.898534	0.618644	0.126298
20	Augmented_20			0.868956	0.728814	0.084729
21	Augmented_21			0.850953	0.593220	0.089629
22	Augmented_22			0.847002	0.627119	0.093317
23	Augmented_23			0.871898	0.618644	0.110106
24	Augmented_24			0.855192	0.720339	0.074431
25	Combined_with_mean		0.899226	0.652542	0.135563
26	Combined_with_median	0.898285	0.652542	0.134615


with stage2 - LR 1e-5_1e-4_1e-5 - 5 epoch

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.732892	0.210757	0.034111	0.753219	0.824769	0.165594	0.053662		0.720339
1	0.805075	0.166213	0.042187	0.783262	0.863961	0.147804	0.053821		0.847458
2	0.838354	0.156234	0.047810	0.819743	0.873461	0.147629	0.054596		0.830508
3	0.866645	0.141059	0.053632	0.811159	0.877332	0.117412	0.067997		0.779661
4	0.863607	0.147776	0.055298	0.817597	0.878267	0.113604	0.101430		0.661017
5	0.882515	0.136645	0.062441	0.851931	0.872255	0.132454	0.063655		0.788136
6	0.900224	0.118800	0.066667	0.866953	0.893900	0.094695	0.088983		0.711864
7	0.919801	0.100862	0.073420	0.892704	0.886664	0.090256	0.091410		0.703390
8	0.939098	0.087813	0.087297	0.909871	0.893871	0.066689	0.112069		0.661017
9	0.950306	0.078983	0.099562	0.927039	0.902862	0.077992	0.106455		0.796610

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.903268	0.123117	0.090457	0.781116	0.865733	0.082904	0.080198		0.686441
1	0.932523	0.090500	0.084249	0.888412	0.871752	0.090778	0.076364		0.711864
2	0.948747	0.078802	0.092423	0.929185	0.873367	0.093040	0.081921		0.737288
3	0.961883	0.070480	0.116649	0.944206	0.869291	0.075921	0.091250		0.618644
4	0.973112	0.060689	0.134524	0.969957	0.875404	0.080791	0.095855		0.627119

	Id						AUC			Recall		Precision
0	Non-augmented			0.875545	0.627119	0.095855
1	Augmented_1				0.853534	0.669492	0.072212
2	Augmented_2				0.865065	0.720339	0.078197
3	Augmented_3				0.880861	0.661017	0.098485
4	Augmented_4				0.873124	0.788136	0.066524
5	Augmented_5				0.868982	0.762712	0.080501
6	Augmented_6				0.858114	0.711864	0.072351
7	Augmented_7				0.877644	0.652542	0.095533
8	Augmented_8				0.864496	0.779661	0.063712
9	Augmented_9				0.850666	0.694915	0.072438
10	Augmented_10			0.854218	0.686441	0.070496
11	Augmented_11			0.865669	0.644068	0.088475
12	Augmented_12			0.860678	0.805085	0.063333
13	Augmented_13			0.849933	0.686441	0.067839
14	Augmented_14			0.852945	0.703390	0.068369
15	Augmented_15			0.876079	0.677966	0.095465
16	Augmented_16			0.866261	0.796610	0.063300
17	Augmented_17			0.863084	0.703390	0.080426
18	Augmented_18			0.857972	0.677966	0.078508
19	Augmented_19			0.892211	0.661017	0.099872
20	Augmented_20			0.871538	0.771186	0.068473
21	Augmented_21			0.849466	0.686441	0.067220
22	Augmented_22			0.855236	0.711864	0.066561
23	Augmented_23			0.871257	0.694915	0.094361
24	Augmented_24			0.868982	0.771186	0.061404
25	Combined_with_mean		0.896251	0.745763	0.096916
26	Combined_with_median	0.894461	0.745763	0.097238



with stage2 - LR 1e-4_1e-3_1e-4 - 10 epoch

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.711196	0.221047	0.031954	0.733906	0.724790	0.469760	0.032702		0.161017
1	0.790970	0.178411	0.039844	0.787554	0.831398	0.152138	0.052885		0.745763
2	0.833896	0.158947	0.048137	0.806867	0.865690	0.254970	0.075490		0.652542
3	0.835954	0.151295	0.050322	0.804721	0.846588	0.174792	0.057692		0.737288
4	0.842095	0.143176	0.050007	0.793991	0.859114	0.103047	0.082300		0.618644
5	0.867340	0.135991	0.053789	0.834764	0.877016	0.142498	0.056707		0.881356
6	0.884639	0.122174	0.064039	0.826180	0.875402	0.114283	0.079452		0.737288
7	0.909245	0.105979	0.067739	0.894850	0.866186	0.092411	0.064563		0.745763
8	0.919330	0.101015	0.074340	0.888412	0.884602	0.139667	0.085884		0.737288
9	0.940656	0.085756	0.089669	0.931330	0.891556	0.083927	0.096502		0.677966

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.898655	0.114839	0.073874	0.819743	0.887152	0.098562	0.068314		0.796610
1	0.916194	0.101650	0.076566	0.884120	0.830271	0.088619	0.073359		0.644068
2	0.918142	0.098386	0.074878	0.888412	0.881701	0.096062	0.074858		0.779661
3	0.887126	0.116633	0.063795	0.839056	0.877481	0.104555	0.063004		0.728814
4	0.887867	0.115268	0.062053	0.856223	0.901190	0.119022	0.072085		0.864407
5	0.912253	0.102919	0.070971	0.879828	0.884423	0.102120	0.072486		0.788136
6	0.938950	0.087953	0.087734	0.905579	0.900559	0.092864	0.090622		0.728814
7	0.954679	0.075852	0.107134	0.931330	0.874995	0.081444	0.094923		0.728814
8	0.967929	0.064614	0.121452	0.954936	0.890676	0.132344	0.105263		0.644068
9	0.978767	0.054439	0.156337	0.963519	0.898280	0.102703	0.123794		0.652542

	Id						AUC			Recall		Precision
0	Non-augmented			0.898515	0.652542	0.123794
1	Augmented_1				0.873636	0.720339	0.080038
2	Augmented_2				0.874492	0.720339	0.079217
3	Augmented_3				0.887429	0.627119	0.121113
4	Augmented_4				0.892420	0.805085	0.079101
5	Augmented_5				0.870168	0.694915	0.076779
6	Augmented_6				0.878410	0.728814	0.080374
7	Augmented_7				0.896862	0.627119	0.119741
8	Augmented_8				0.889686	0.796610	0.077430
9	Augmented_9				0.865843	0.703390	0.073975
10	Augmented_10			0.876052	0.754237	0.078276
11	Augmented_11			0.888344	0.694915	0.121662
12	Augmented_12			0.887810	0.822034	0.071429
13	Augmented_13			0.861398	0.711864	0.084507
14	Augmented_14			0.871716	0.686441	0.082317
15	Augmented_15			0.886524	0.601695	0.125220
16	Augmented_16			0.882857	0.754237	0.079535
17	Augmented_17			0.878318	0.745763	0.078501
18	Augmented_18			0.882268	0.796610	0.080617
19	Augmented_19			0.897727	0.703390	0.121523
20	Augmented_20			0.894528	0.805085	0.071375
21	Augmented_21			0.867615	0.754237	0.073071
22	Augmented_22			0.886681	0.805085	0.077614
23	Augmented_23			0.889313	0.627119	0.109792
24	Augmented_24			0.893793	0.813559	0.070022
25	Combined_with_mean		0.905345	0.771186	0.097639
26	Combined_with_median	0.902818	0.754237	0.097374


with stage2 - LR 1e-3_1e-2_1e-3 - 10 epoch

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.728115	0.204570	0.032456	0.759657	0.794168	0.133165	0.075123		0.516949
1	0.809239	0.169900	0.045513	0.759657	0.853079	0.150001	0.063783		0.737288
2	0.840230	0.155311	0.056441	0.759657	0.824778	0.190397	0.053550		0.754237
3	0.847496	0.152933	0.053286	0.800429	0.854779	0.134973	0.056304		0.779661
4	0.827748	0.165255	0.049763	0.789700	0.882523	0.124671	0.073209		0.796610
5	0.854545	0.145358	0.052545	0.821888	0.861998	0.136529	0.058161		0.788136
6	0.859466	0.133974	0.052898	0.830472	0.882169	0.176173	0.070136		0.788136
7	0.902970	0.111624	0.065833	0.892704	0.875761	0.092910	0.083080		0.694915
8	0.913447	0.105880	0.074678	0.894850	0.891940	0.125354	0.082814		0.788136
9	0.939287	0.087659	0.088636	0.935622	0.898068	0.113437	0.100342		0.745763

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.816986	0.149083	0.051354	0.703863	0.848796	0.176062	0.058161		0.788136
1	0.810775	0.147890	0.040550	0.785408	0.817799	0.274183	0.059357		0.610169
2	0.795479	0.159233	0.038769	0.811159	0.863742	0.200316	0.039106		0.949153
3	0.822241	0.159487	0.043339	0.815451	0.855205	0.139066	0.054101		0.788136
4	0.834733	0.155127	0.047547	0.804721	0.755634	0.175294	0.033715		0.847458
5	0.840232	0.159668	0.045327	0.830472	0.885399	0.138198	0.061981		0.822034
6	0.836897	0.167266	0.045101	0.809013	0.881326	0.182342	0.047204		0.872881
7	0.855538	0.146324	0.048609	0.858369	0.881188	0.126219	0.062827		0.813559
8	0.868960	0.129191	0.050501	0.864807	0.879453	0.117113	0.057158		0.889831
9	0.884444	0.117121	0.053759	0.888412	0.891070	0.133455	0.053737		0.932203

	Id						AUC			Recall		Precision
0	Non-augmented			0.891266	0.932203	0.053737
1	Augmented_1				0.874094	0.957627	0.033167
2	Augmented_2				0.881163	0.974576	0.034318
3	Augmented_3				0.893675	0.923729	0.052454
4	Augmented_4				0.878592	0.974576	0.029862
5	Augmented_5				0.870530	0.949153	0.032568
6	Augmented_6				0.876208	0.966102	0.033738
7	Augmented_7				0.887692	0.906780	0.050496
8	Augmented_8				0.877597	0.974576	0.030010
9	Augmented_9				0.878319	0.932203	0.037957
10	Augmented_10			0.885541	0.932203	0.039118
11	Augmented_11			0.891019	0.872881	0.062349
12	Augmented_12			0.880530	0.957627	0.033651
13	Augmented_13			0.862349	0.940678	0.030621
14	Augmented_14			0.881170	0.966102	0.031897
15	Augmented_15			0.893892	0.932203	0.048544
16	Augmented_16			0.875204	0.974576	0.028381
17	Augmented_17			0.876766	0.957627	0.035039
18	Augmented_18			0.884425	0.966102	0.035658
19	Augmented_19			0.894488	0.898305	0.054192
20	Augmented_20			0.881661	0.974576	0.031123
21	Augmented_21			0.866158	0.983051	0.030161
22	Augmented_22			0.871522	0.974576	0.029808
23	Augmented_23			0.888364	0.932203	0.043790
24	Augmented_24			0.875434	0.983051	0.027128
25	Combined_with_mean		0.890646	0.966102	0.038868
26	Combined_with_median	0.886281	0.966102	0.034926


stage2 LR 1e-4_1e-3_1e-4 LS 0.1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.726245	0.210945	0.033732	0.751073	0.814509	0.148429	0.056590		0.669492
1	0.799738	0.175464	0.044354	0.751073	0.822196	0.128934	0.069027		0.661017
2	0.851601	0.150925	0.050282	0.802575	0.874863	0.137693	0.056054		0.847458
3	0.848410	0.151467	0.052095	0.832618	0.869777	0.099414	0.086462		0.644068
4	0.824965	0.159866	0.047638	0.761803	0.874687	0.151272	0.066165		0.745763
5	0.833606	0.154078	0.045039	0.824034	0.852253	0.099941	0.112847		0.550847
6	0.871387	0.134142	0.054954	0.839056	0.881716	0.110019	0.071259		0.762712
7	0.896970	0.119414	0.064839	0.862661	0.887421	0.087392	0.087133		0.728814
8	0.910500	0.110010	0.072711	0.869099	0.889066	0.092134	0.087379		0.762712
9	0.932313	0.094951	0.083266	0.884120	0.890941	0.071632	0.109312		0.686441

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.892075	0.116030	0.069391	0.819743	0.846474	0.113755	0.057457		0.796610
1	0.905994	0.105188	0.066796	0.884120	0.891522	0.102091	0.068719		0.813559
2	0.921265	0.099301	0.074927	0.886266	0.866529	0.107586	0.068114		0.771186
3	0.922640	0.096238	0.073784	0.888412	0.847458	0.105219	0.065952		0.677966
4	0.880388	0.121380	0.059824	0.830472	0.894063	0.140821	0.052149		0.915254
5	0.907019	0.108288	0.067450	0.899142	0.909004	0.089581	0.083187		0.805085
6	0.935514	0.091554	0.083581	0.905579	0.909105	0.081091	0.100559		0.762712
7	0.960898	0.072959	0.113335	0.935622	0.907718	0.066528	0.133226		0.703390
8	0.976148	0.058240	0.148887	0.961373	0.903412	0.076135	0.113833		0.669492
9	0.983742	0.049017	0.180655	0.969957	0.899321	0.070219	0.131448		0.669492

	Id						AUC			Recall		Precision
0	Non-augmented			0.899363	0.669492	0.131448
1	Augmented_1				0.839190	0.550847	0.083655
2	Augmented_2				0.853751	0.601695	0.087331
3	Augmented_3				0.904490	0.635593	0.126050
4	Augmented_4				0.867800	0.694915	0.078170
5	Augmented_5				0.826272	0.491525	0.073418
6	Augmented_6				0.829345	0.542373	0.079306
7	Augmented_7				0.899347	0.627119	0.123333
8	Augmented_8				0.859515	0.661017	0.074286
9	Augmented_9				0.835954	0.610169	0.083624
10	Augmented_10			0.842364	0.584746	0.082143
11	Augmented_11			0.906262	0.669492	0.124409
12	Augmented_12			0.859481	0.694915	0.074141
13	Augmented_13			0.843028	0.559322	0.084833
14	Augmented_14			0.834797	0.601695	0.087331
15	Augmented_15			0.884066	0.601695	0.108563
16	Augmented_16			0.858597	0.686441	0.076128
17	Augmented_17			0.849034	0.567797	0.086675
18	Augmented_18			0.856970	0.567797	0.084703
19	Augmented_19			0.903849	0.661017	0.140288
20	Augmented_20			0.872130	0.728814	0.084396
21	Augmented_21			0.841308	0.610169	0.078261
22	Augmented_22			0.856004	0.635593	0.081610
23	Augmented_23			0.896029	0.694915	0.116976
24	Augmented_24			0.868260	0.745763	0.073950
25	Combined_with_mean		0.897742	0.703390	0.117730
26	Combined_with_median	0.894799	0.677966	0.113636


stage2 LR 1e-5_1e-3(8) _5e-4(2)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.729326	0.208939	0.031944	0.740343	0.849703	0.214205	0.032912		0.949153
1	0.816324	0.170310	0.046053	0.757511	0.840522	0.141296	0.068182		0.762712
2	0.850202	0.151740	0.050717	0.804721	0.878670	0.158466	0.052938		0.847458
3	0.875345	0.136098	0.058950	0.824034	0.863234	0.130721	0.072089		0.661017
4	0.868557	0.138054	0.059455	0.796137	0.895567	0.100880	0.089087		0.677966
5	0.883871	0.128942	0.063277	0.862661	0.878731	0.123215	0.069237		0.745763
6	0.905395	0.114372	0.069867	0.871245	0.884276	0.090962	0.087047		0.711864
7	0.923923	0.101144	0.077570	0.890558	0.874227	0.089821	0.092865		0.694915
8	0.938853	0.089799	0.090206	0.901288	0.898280	0.075964	0.104369		0.728814
9	0.959515	0.072413	0.115144	0.946352	0.895841	0.063449	0.119685		0.644068

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.922253	0.105081	0.098245	0.804721	0.874718	0.081712	0.086259		0.728814
1	0.939493	0.085002	0.088167	0.896996	0.861786	0.089742	0.078345		0.754237
2	0.950215	0.079172	0.099765	0.912017	0.890575	0.091739	0.092715		0.711864
3	0.955913	0.073924	0.109821	0.933476	0.874731	0.064765	0.105655		0.601695
4	0.940838	0.086748	0.095941	0.892704	0.882630	0.084001	0.086606		0.728814
5	0.934487	0.091600	0.088335	0.896996	0.888112	0.078231	0.094059		0.644068
6	0.938105	0.089244	0.091647	0.896996	0.882115	0.106988	0.064926		0.822034
7	0.935256	0.089326	0.085098	0.905579	0.894975	0.068209	0.100765		0.669492
8	0.936477	0.090353	0.088581	0.905579	0.870286	0.098400	0.074588		0.728814
9	0.962234	0.071033	0.108782	0.935622	0.886053	0.105109	0.095954		0.703390

	Id						AUC			Recall		Precision
0	Non-augmented			0.886640	0.703390	0.095954
1	Augmented_1				0.854226	0.788136	0.067343
2	Augmented_2				0.851681	0.745763	0.063953
3	Augmented_3				0.886610	0.686441	0.097239
4	Augmented_4				0.858768	0.788136	0.059048
5	Augmented_5				0.846484	0.762712	0.065123
6	Augmented_6				0.843296	0.703390	0.059456
7	Augmented_7				0.866426	0.644068	0.086168
8	Augmented_8				0.857483	0.822034	0.059840
9	Augmented_9				0.829636	0.754237	0.057531
10	Augmented_10			0.823953	0.720339	0.055267
11	Augmented_11			0.874571	0.711864	0.087047
12	Augmented_12			0.836768	0.779661	0.051225
13	Augmented_13			0.846860	0.737288	0.065120
14	Augmented_14			0.826539	0.661017	0.058559
15	Augmented_15			0.874550	0.711864	0.092920
16	Augmented_16			0.854097	0.805085	0.061889
17	Augmented_17			0.852839	0.762712	0.064748
18	Augmented_18			0.842084	0.728814	0.060436
19	Augmented_19			0.895481	0.694915	0.099878
20	Augmented_20			0.866276	0.822034	0.059582
21	Augmented_21			0.856996	0.788136	0.065771
22	Augmented_22			0.844615	0.745763	0.061324
23	Augmented_23			0.874155	0.728814	0.085487
24	Augmented_24			0.862299	0.838983	0.060588
25	Combined_with_mean		0.887357	0.779661	0.089581
26	Combined_with_median	0.880492	0.788136	0.080729


stage2 LR 1e-6_1e-5_1e-6

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.966781	0.064966	0.136279	0.946352	0.895617	0.076233	0.102662		0.686441
1	0.968325	0.063891	0.131476	0.942060	0.895576	0.079284	0.100985		0.694915
2	0.967722	0.063895	0.136196	0.952790	0.896087	0.084539	0.096413		0.728814
3	0.966560	0.065719	0.129429	0.924893	0.896479	0.075440	0.103316		0.686441
4	0.969052	0.062246	0.134293	0.961373	0.896908	0.072427	0.103403		0.669492
5	0.968793	0.063448	0.133193	0.950644	0.897187	0.075010	0.101990		0.694915
6	0.971018	0.060956	0.135905	0.952790	0.896959	0.073993	0.103581		0.686441
7	0.970201	0.061426	0.131259	0.950644	0.897529	0.081499	0.099768		0.728814
8	0.972198	0.059692	0.136874	0.967811	0.896994	0.072985	0.102696		0.677966
9	0.972831	0.058939	0.138236	0.972103	0.897738	0.069954	0.107629		0.669492


stage2 LR 1e-4_1e-3_1e-4 LS 0.2

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.970658	0.067926	0.138398	0.942060	0.890150	0.070121	0.112927		0.644068
1	0.968157	0.068286	0.134911	0.942060	0.891886	0.067690	0.109843		0.652542
2	0.969732	0.066185	0.138304	0.959227	0.897969	0.097410	0.083110		0.788136
3	0.970505	0.064832	0.134154	0.959227	0.882091	0.071113	0.110193		0.677966
4	0.970090	0.064501	0.132487	0.963519	0.886377	0.058044	0.110465		0.644068
5	0.970996	0.066027	0.131980	0.948498	0.883935	0.077032	0.107050		0.694915
6	0.973317	0.062765	0.142675	0.959227	0.892763	0.079009	0.112694		0.737288
7	0.980150	0.056576	0.155494	0.965665	0.883703	0.065665	0.116279		0.635593
8	0.982379	0.053670	0.180040	0.967811	0.894613	0.063364	0.122951		0.635593
9	0.986394	0.046374	0.190299	0.984979	0.884320	0.059681	0.147679		0.593220


stage2 LR 1e-6_1e-3(8) _5e-4(2)

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.968099	0.063907	0.132132	0.944206	0.896079	0.070886	0.107239		0.677966
1	0.972078	0.059855	0.139900	0.957082	0.894870	0.067456	0.112230		0.661017
2	0.970778	0.059471	0.138166	0.957082	0.889434	0.076247	0.107011		0.737288
3	0.971111	0.059247	0.136476	0.950644	0.892424	0.056277	0.122449		0.610169
4	0.968664	0.061648	0.134011	0.961373	0.888905	0.061327	0.106218		0.694915
5	0.971156	0.059458	0.130396	0.952790	0.875308	0.080197	0.099881		0.711864
6	0.972896	0.057069	0.139491	0.952790	0.883773	0.059791	0.120536		0.686441
7	0.974461	0.058652	0.144763	0.954936	0.837012	0.035424	0.220000		0.279661
8	0.969260	0.067182	0.136251	0.942060	0.871174	0.064088	0.106129		0.601695
9	0.980049	0.053635	0.167585	0.978541	0.882660	0.060375	0.136821		0.576271


stage2 LR 1e-6_1e-4(8) _5e-5(2)

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.966961	0.065241	0.133026	0.929185	0.895827	0.076401	0.102662		0.686441
1	0.967906	0.064385	0.131211	0.946352	0.897440	0.078140	0.099631		0.686441
2	0.970877	0.060347	0.138727	0.967811	0.896646	0.079281	0.098914		0.694915
3	0.971238	0.060049	0.137546	0.952790	0.893567	0.070903	0.106809		0.677966
4	0.971790	0.059447	0.137623	0.959227	0.894108	0.070023	0.107239		0.677966
5	0.971180	0.059929	0.138092	0.950644	0.896591	0.071519	0.104271		0.703390
6	0.974969	0.054143	0.144997	0.976395	0.892833	0.067009	0.107692		0.652542
7	0.975636	0.054252	0.145514	0.957082	0.894111	0.068049	0.107417		0.711864
8	0.977986	0.052086	0.155161	0.974249	0.898205	0.061858	0.113833		0.669492
9	0.977501	0.052082	0.161221	0.963519	0.897363	0.060093	0.118421		0.610169


stage2 LR 1e-6_1e-3(8) _1e-3(2)

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.967611	0.064060	0.132498	0.952790	0.895368	0.074099	0.100244		0.694915
1	0.970889	0.060719	0.133393	0.959227	0.898240	0.068468	0.109643		0.703390
2	0.970787	0.058908	0.135738	0.952790	0.892996	0.071429	0.102689		0.711864
3	0.972404	0.057376	0.137255	0.961373	0.888002	0.059109	0.114467		0.610169
4	0.969165	0.061385	0.137581	0.959227	0.897316	0.059418	0.123377		0.644068
5	0.974616	0.057833	0.149933	0.954936	0.894534	0.084950	0.097395		0.728814
6	0.975003	0.055729	0.151886	0.959227	0.877924	0.058877	0.133208		0.601695
7	0.974610	0.057098	0.151249	0.961373	0.879797	0.097243	0.081353		0.754237
8	0.977619	0.052991	0.159461	0.965665	0.875631	0.061606	0.107735		0.661017
9	0.976072	0.054294	0.156001	0.967811	0.847197	0.052888	0.134199		0.525424
	

stage2 LR 1e-5_1e-3(8) _1e-3(2)

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.969023	0.062318	0.133534	0.952790	0.895667	0.072632	0.101631		0.686441
1	0.971013	0.059939	0.139932	0.967811	0.894488	0.063059	0.108146		0.652542
2	0.969396	0.060267	0.139849	0.954936	0.882595	0.077176	0.089947		0.720339
3	0.971980	0.058589	0.139062	0.954936	0.883328	0.056528	0.116694		0.610169
4	0.971636	0.058620	0.139637	0.957082	0.893056	0.055061	0.128378		0.644068
5	0.975484	0.056048	0.145103	0.950644	0.876501	0.074801	0.107001		0.686441
6	0.974762	0.054658	0.147337	0.967811	0.876208	0.068261	0.108609		0.694915
7	0.973550	0.058520	0.143591	0.959227	0.879852	0.072870	0.106579		0.686441
8	0.969777	0.065342	0.144310	0.944206	0.880291	0.064663	0.109817		0.559322
9	0.979205	0.050941	0.162055	0.967811	0.841398	0.058083	0.128544		0.576271


stage2 LR 1e-4_1e-3_5e-4

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.969739	0.062064	0.134875	0.963519	0.895880	0.067747	0.104252		0.644068
1	0.968768	0.062226	0.136392	0.950644	0.886567	0.068941	0.103321		0.711864
2	0.971774	0.059721	0.143360	0.961373	0.880982	0.076677	0.096400		0.703390
3	0.971048	0.060202	0.142629	0.959227	0.880771	0.063272	0.103817		0.576271
4	0.971512	0.059195	0.136127	0.954936	0.883543	0.055610	0.127148		0.627119
5	0.967603	0.065989	0.129401	0.946352	0.890553	0.078992	0.102959		0.737288
6	0.976120	0.055619	0.147906	0.969957	0.875889	0.062253	0.124101		0.584746
7	0.979558	0.050272	0.164771	0.963519	0.884515	0.057504	0.129088		0.635593
8	0.982900	0.047916	0.189349	0.961373	0.881750	0.070390	0.108187		0.627119
9	0.982506	0.047458	0.179619	0.972103	0.884399	0.060470	0.125899		0.593220


stage2 LR 1e-4_1e-3_1e-4 g1

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.969359	0.122788	0.128072	0.950644	0.897547	0.126447	0.108727		0.644068
1	0.970233	0.118343	0.132283	0.954936	0.889987	0.151495	0.092772		0.728814
2	0.971840	0.113721	0.140969	0.961373	0.886243	0.186119	0.081705		0.779661
3	0.970237	0.115849	0.141206	0.969957	0.880331	0.121683	0.106545		0.593220
4	0.972833	0.110908	0.139829	0.948498	0.870130	0.091202	0.130952		0.559322
5	0.971940	0.118860	0.136995	0.952790	0.897475	0.179721	0.085499		0.864407
6	0.978768	0.099561	0.156747	0.972103	0.844871	0.118470	0.117958		0.567797
7	0.980795	0.095730	0.168284	0.967811	0.891608	0.121653	0.120125		0.652542
8	0.984682	0.084680	0.186650	0.972103	0.883246	0.116177	0.125940		0.567797
9	0.986463	0.078053	0.209078	0.978541	0.881526	0.100448	0.148837		0.542373


stage1 only head, stage2 full model 1e-5_1e-3_1e-4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.704946	0.226885	0.030975	0.701717	0.805052	0.175206	0.039785		0.754237
1	0.798595	0.176429	0.041112	0.736051	0.820768	0.183807	0.042573		0.779661
2	0.828754	0.172629	0.047005	0.789700	0.830642	0.176496	0.046193		0.771186
3	0.832833	0.180442	0.046931	0.787554	0.856646	0.165349	0.058599		0.779661
4	0.834953	0.179936	0.048151	0.796137	0.847967	0.144440	0.073779		0.627119
5	0.830635	0.181307	0.045347	0.781116	0.848201	0.168415	0.055485		0.737288
6	0.839456	0.183779	0.050211	0.789700	0.863404	0.173563	0.059150		0.813559
7	0.851887	0.168194	0.052034	0.776824	0.871500	0.152846	0.066769		0.737288
8	0.865892	0.156290	0.056116	0.811159	0.869797	0.157524	0.056042		0.813559
9	0.883450	0.143785	0.058576	0.836910	0.874376	0.142838	0.063660		0.813559

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.770323	0.194558	0.042141	0.635193	0.858888	0.164231	0.047479		0.822034
1	0.865656	0.147246	0.049680	0.832618	0.846334	0.154665	0.052976		0.754237
2	0.872247	0.144588	0.056015	0.843348	0.837243	0.162977	0.046200		0.788136
3	0.889952	0.131908	0.057934	0.871245	0.824964	0.139131	0.049096		0.805085
4	0.856574	0.148186	0.056264	0.776824	0.848454	0.110905	0.085642		0.576271
5	0.864355	0.135489	0.049919	0.856223	0.868120	0.127315	0.055168		0.805085
6	0.884876	0.125632	0.061498	0.856223	0.874417	0.119528	0.067399		0.779661
7	0.909666	0.111574	0.068646	0.858369	0.882373	0.101025	0.081613		0.703390
8	0.928825	0.100935	0.076895	0.909871	0.895754	0.105773	0.081406		0.745763
9	0.946631	0.089348	0.088085	0.924893	0.903126	0.109110	0.077703		0.779661


stage1_fullmodel_1e-6_1e-2(15) _1e-3(5)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.644272	0.299721	0.025855	0.759657	0.796467	0.189897	0.037896		0.830508
1	0.754841	0.190540	0.034415	0.789700	0.827621	0.165277	0.049474		0.677966
2	0.818368	0.165136	0.045632	0.776824	0.792784	0.166180	0.043062		0.838983
3	0.798427	0.170150	0.040376	0.811159	0.820000	0.161971	0.043599		0.796610
4	0.770401	0.173501	0.038683	0.748927	0.767093	0.189078	0.047844		0.686441
5	0.782368	0.165757	0.037664	0.845494	0.768101	0.153378	0.032979		0.788136
6	0.779942	0.168082	0.036708	0.806867	0.789387	0.163295	0.039433		0.754237
7	0.784520	0.167867	0.035414	0.821888	0.815321	0.191540	0.045500		0.771186
8	0.798734	0.158367	0.038307	0.817597	0.806808	0.135622	0.043108		0.686441
9	0.805292	0.153350	0.041516	0.789700	0.841289	0.139299	0.038808		0.838983
10	0.814891	0.146902	0.040353	0.854077	0.774531	0.133780	0.051666		0.644068
11	0.802153	0.154155	0.038913	0.789700	0.816764	0.164666	0.043327		0.737288
12	0.798049	0.154956	0.040387	0.806867	0.694660	0.191142	0.027455		0.720339
13	0.798231	0.162124	0.041394	0.800429	0.801340	0.131554	0.056884		0.584746
14	0.797899	0.156630	0.038931	0.800429	0.834005	0.159051	0.037010		0.864407
15	0.791237	0.155397	0.038527	0.828326	0.828873	0.140946	0.049284		0.728814
16	0.806844	0.148934	0.040727	0.802575	0.844404	0.138713	0.055523		0.813559
17	0.824947	0.141709	0.044558	0.824034	0.776979	0.300311	0.036911		0.830508
18	0.836868	0.134457	0.044725	0.819743	0.842845	0.130887	0.055388		0.779661
19	0.841274	0.130687	0.046867	0.802575	0.851502	0.138566	0.047526		0.822034


stage1 only 5e-3_1e-2_5e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.742860	0.219252	0.034705	0.729614	0.832635	0.173319	0.056505		0.728814
1	0.809339	0.200361	0.045215	0.774678	0.815044	0.207034	0.044183		0.762712
2	0.822761	0.197013	0.045535	0.776824	0.825809	0.181257	0.055102		0.686441
3	0.824055	0.199549	0.047944	0.768240	0.854682	0.183211	0.057766		0.762712
4	0.827319	0.195189	0.047606	0.759657	0.837279	0.143339	0.078300		0.593220
5	0.810712	0.201138	0.045621	0.746781	0.811380	0.216461	0.040679		0.771186
6	0.818629	0.199750	0.044975	0.768240	0.844682	0.200572	0.047856		0.813559
7	0.836754	0.189310	0.049208	0.800429	0.859169	0.182190	0.053551		0.805085
8	0.825144	0.182040	0.051863	0.761803	0.862414	0.183969	0.050556		0.847458
9	0.831025	0.170066	0.051485	0.755365	0.861095	0.167924	0.055458		0.805085


stage1 only 1e-3_1e-2_1e-3, g1_a0.25_ls0.02

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.734876	0.362221	0.032924	0.725322	0.808308	0.300470	0.045898		0.796610
1	0.809455	0.309514	0.043330	0.763949	0.851012	0.300465	0.050926		0.838983
2	0.839130	0.299605	0.048077	0.783262	0.841105	0.289232	0.052539		0.762712
3	0.834560	0.315605	0.048993	0.783262	0.840783	0.300311	0.058140		0.720339
4	0.844977	0.317631	0.051139	0.785408	0.851852	0.250267	0.080447		0.610169
5	0.840475	0.326619	0.049307	0.763949	0.849019	0.289547	0.064996		0.703390
6	0.848306	0.326924	0.052266	0.806867	0.860097	0.266369	0.080160		0.677966
7	0.862659	0.312431	0.053621	0.783262	0.864202	0.268394	0.072289		0.711864
8	0.864623	0.305846	0.057701	0.791846	0.878615	0.290980	0.063872		0.813559
9	0.881948	0.283215	0.059893	0.819743	0.874352	0.287727	0.063772		0.796610


stage1 only 1e-3_1e-2_1e-3, g1_a0.5_ls0.02

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.738769	0.361733	0.033903	0.740343	0.822954	0.311651	0.044770		0.855932
1	0.809065	0.309361	0.043938	0.757511	0.846399	0.293614	0.053640		0.830508
2	0.830779	0.307352	0.047022	0.781116	0.831715	0.298217	0.050505		0.720339
3	0.840423	0.314930	0.049745	0.796137	0.843319	0.320355	0.051948		0.779661
4	0.845994	0.325211	0.052488	0.783262	0.847635	0.272414	0.072505		0.652542
5	0.842912	0.328864	0.050596	0.783262	0.834110	0.342033	0.050888		0.728814
6	0.846123	0.332021	0.050185	0.785408	0.858963	0.283659	0.071366		0.694915
7	0.859192	0.315573	0.053343	0.787554	0.870245	0.280960	0.068910		0.728814
8	0.870271	0.299649	0.059218	0.796137	0.872512	0.299789	0.059813		0.813559
9	0.888406	0.275242	0.060488	0.834764	0.872966	0.278818	0.066525		0.796610


stage1 only 1e-3_2e-2_1e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.706602	0.220291	0.030570	0.684549	0.826098	0.155678	0.050351		0.728814
1	0.803170	0.181127	0.043659	0.766094	0.834152	0.180690	0.047405		0.805085
2	0.818008	0.185194	0.044428	0.776824	0.832641	0.176081	0.052153		0.728814
3	0.816627	0.190654	0.044915	0.763949	0.827687	0.171336	0.049280		0.754237
4	0.814048	0.196737	0.046178	0.759657	0.846613	0.153190	0.071062		0.703390
5	0.800021	0.198996	0.040506	0.742489	0.856224	0.167109	0.070740		0.745763
6	0.810796	0.195961	0.045896	0.751073	0.848230	0.187149	0.055226		0.788136
7	0.831702	0.177077	0.048705	0.778970	0.872834	0.147851	0.071011		0.720339
8	0.842037	0.162212	0.052546	0.766094	0.870911	0.156539	0.057298		0.805085
9	0.850864	0.150317	0.052609	0.774678	0.876949	0.142249	0.062041		0.788136


stage1 only 1e-3_1e-2_1e-4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.719595	0.220182	0.033048	0.721030	0.794480	0.161601	0.045265		0.737288
1	0.803682	0.174897	0.043780	0.744635	0.835697	0.170733	0.046364		0.805085
2	0.829250	0.172651	0.046388	0.770386	0.840390	0.187842	0.046154		0.838983
3	0.832272	0.179507	0.047977	0.793991	0.854319	0.164886	0.058939		0.762712
4	0.835049	0.181902	0.046054	0.778970	0.848283	0.147596	0.080308		0.618644
5	0.833904	0.182663	0.047607	0.789700	0.842875	0.174753	0.049706		0.788136
6	0.833503	0.182807	0.048596	0.787554	0.862939	0.187820	0.051057		0.838983
7	0.852755	0.167684	0.052639	0.785408	0.875128	0.132738	0.083166		0.703390
8	0.865156	0.156736	0.056382	0.781116	0.883090	0.150758	0.062946		0.822034
9	0.881282	0.145525	0.059855	0.813305	0.883644	0.146627	0.064926		0.822034


stage1 only 3e-3_3e-2_3e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.735708	0.214281	0.034475	0.701717	0.786497	0.177079	0.043039		0.686441
1	0.798160	0.200365	0.041221	0.753219	0.829513	0.179087	0.053301		0.745763
2	0.808704	0.200314	0.045535	0.774678	0.820740	0.269068	0.037544		0.813559
3	0.805189	0.209799	0.045185	0.753219	0.857226	0.178441	0.058634		0.771186
4	0.792541	0.221266	0.041747	0.744635	0.826016	0.175838	0.069936		0.652542
5	0.788319	0.221082	0.042696	0.723176	0.860882	0.244550	0.040551		0.872881
6	0.793718	0.213787	0.043878	0.768240	0.851230	0.227807	0.046588		0.838983
7	0.815429	0.191503	0.044984	0.755365	0.863200	0.159069	0.062500		0.796610
8	0.829174	0.169993	0.050501	0.778970	0.865949	0.157424	0.062708		0.796610
9	0.837200	0.155452	0.050647	0.772532	0.868459	0.153344	0.053180		0.822034


stage1 from L287 1e-3_1e-2_1e-4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.742434	0.202108	0.033787	0.729614	0.860189	0.179589	0.048577		0.838983
1	0.826137	0.163043	0.047890	0.793991	0.840674	0.153210	0.068652		0.703390
2	0.826111	0.166409	0.046364	0.800429	0.863528	0.152030	0.047619		0.872881
3	0.851710	0.148345	0.049729	0.847640	0.875775	0.119055	0.074074		0.694915
4	0.837625	0.169325	0.048197	0.809013	0.864055	0.142736	0.075506		0.694915
5	0.869652	0.136629	0.055001	0.851931	0.889597	0.131233	0.069584		0.822034
6	0.885526	0.123468	0.060939	0.869099	0.866784	0.156213	0.049674		0.838983
7	0.890605	0.120002	0.060668	0.888412	0.884003	0.110434	0.072958		0.779661
8	0.920311	0.100231	0.066843	0.922747	0.893303	0.095006	0.075077		0.822034
9	0.938939	0.087833	0.077842	0.950644	0.894530	0.090541	0.080335		0.813559


stage1 from L287 1e-3_1e-2_1e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.751739	0.198438	0.034433	0.770386	0.813285	0.143349	0.083616		0.627119
1	0.809261	0.173015	0.046436	0.761803	0.829323	0.138693	0.060444		0.669492
2	0.808314	0.176819	0.044626	0.783262	0.847133	0.145915	0.069565		0.610169
3	0.847508	0.154638	0.048944	0.815451	0.875551	0.110719	0.071730		0.720339
4	0.866175	0.139495	0.053476	0.815451	0.886142	0.093774	0.102597		0.669492
5	0.878344	0.123218	0.056837	0.830472	0.889830	0.106107	0.071920		0.796610
6	0.888156	0.130195	0.061463	0.858369	0.875495	0.115115	0.064944		0.788136
7	0.907957	0.111066	0.066371	0.866953	0.882906	0.093283	0.071828		0.762712
8	0.919975	0.103299	0.072931	0.892704	0.886286	0.095837	0.096811		0.720339
9	0.945886	0.085890	0.095915	0.927039	0.888068	0.076798	0.106648		0.652542


stage1 from L287 1e-3_1e-2_1e-4 g1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.740720	0.351785	0.033537	0.729614	0.709358	0.352636	0.035413		0.711864
1	0.812750	0.299116	0.043150	0.796137	0.862344	0.263256	0.067870		0.796610
2	0.853963	0.273639	0.052508	0.815451	0.874432	0.308480	0.056338		0.847458
3	0.873920	0.251179	0.057074	0.815451	0.874672	0.226236	0.078899		0.728814
4	0.880049	0.241670	0.058665	0.841202	0.880617	0.190530	0.078486		0.720339
5	0.892960	0.230095	0.063725	0.858369	0.886246	0.197558	0.080397		0.754237
6	0.911243	0.206302	0.071912	0.866953	0.878503	0.154047	0.097276		0.635593
7	0.926001	0.190061	0.080085	0.894850	0.862946	0.167742	0.085947		0.627119
8	0.943412	0.166191	0.094509	0.901288	0.891753	0.155634	0.104706		0.754237
9	0.961372	0.139116	0.123464	0.948498	0.895806	0.149541	0.108418		0.720339


https://www.kaggle.com/code/abhivij/melanoma-classification?scriptVersionId=226949743
stage1 from L287 1e-3_1e-2_1e-4 g1_ls0.01

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.765620	0.334637	0.035696	0.744635	0.833109	0.253167	0.073693		0.728814
1	0.810675	0.299398	0.045420	0.768240	0.853534	0.237440	0.075229		0.694915
2	0.850397	0.275142	0.054843	0.798283	0.846960	0.198962	0.077164		0.627119
3	0.859502	0.259848	0.053371	0.806867	0.852351	0.179622	0.096606		0.627119
4	0.865006	0.250606	0.053208	0.824034	0.857439	0.146891	0.138587		0.432203
5	0.876510	0.245771	0.058849	0.858369	0.859124	0.248866	0.064834		0.779661
6	0.900835	0.217308	0.066512	0.860515	0.886128	0.185199	0.083712		0.779661
7	0.919504	0.194073	0.073603	0.881974	0.892101	0.166008	0.088264		0.771186
8	0.934332	0.174716	0.085831	0.903434	0.906774	0.168660	0.087189		0.830508
9	0.957214	0.141656	0.104827	0.950644	0.906944	0.152170	0.103248		0.754237


best stage1 with stage2 1e-4_1e-3_1e-5

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.960889	0.132102	0.115224	0.948498	0.909219	0.147171	0.101224		0.771186
1	0.963157	0.127672	0.120624	0.946352	0.904174	0.138804	0.106152		0.745763
2	0.963901	0.126259	0.120098	0.946352	0.904995	0.171674	0.088423		0.822034
3	0.964427	0.122606	0.127365	0.967811	0.895118	0.127294	0.105464		0.703390
4	0.966560	0.118380	0.129368	0.961373	0.890799	0.096682	0.130360		0.644068
5	0.966319	0.125788	0.131563	0.946352	0.896453	0.152541	0.098451		0.754237
6	0.973628	0.106834	0.144745	0.963519	0.882066	0.115145	0.123264		0.601695
7	0.977761	0.095142	0.158443	0.978541	0.892242	0.124385	0.112344		0.686441
8	0.978398	0.095859	0.170463	0.963519	0.892661	0.122616	0.121726		0.669492
9	0.980032	0.090123	0.172178	0.972103	0.885783	0.120804	0.123596		0.652542


best stage1 with stage2 1e-4_1e-3_1e-4

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.959577	0.135240	0.111591	0.942060	0.908276	0.153128	0.097403		0.762712
1	0.962849	0.128357	0.117103	0.950644	0.902481	0.127778	0.109395		0.720339
2	0.961563	0.131748	0.116707	0.939914	0.906947	0.163333	0.090649		0.805085
3	0.965461	0.121115	0.120849	0.952790	0.901012	0.144071	0.094595		0.711864
4	0.965474	0.121311	0.123849	0.952790	0.897449	0.125800	0.104348		0.711864
5	0.971767	0.108625	0.143403	0.965665	0.885491	0.144966	0.104478		0.711864
6	0.970639	0.111437	0.133551	0.963519	0.890756	0.129285	0.105392		0.728814
7	0.976539	0.097793	0.154367	0.978541	0.888218	0.126446	0.112518		0.677966
8	0.980217	0.088751	0.174347	0.974249	0.884261	0.123615	0.119225		0.677966
9	0.981076	0.086823	0.175087	0.974249	0.884613	0.116065	0.122112		0.627119


best stage1 with stage2 1e-5_1e-4_1e-5

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.960371	0.135047	0.110691	0.942060	0.906231	0.170769	0.095581		0.788136
1	0.960812	0.134237	0.111957	0.946352	0.905790	0.166652	0.096706		0.771186
2	0.960392	0.133507	0.114570	0.939914	0.905959	0.173217	0.091529		0.796610
3	0.961004	0.129930	0.113872	0.944206	0.906212	0.151886	0.094737		0.762712
4	0.962638	0.125153	0.117276	0.957082	0.905904	0.155722	0.095890		0.771186
5	0.964635	0.123451	0.117958	0.957082	0.906176	0.154130	0.096154		0.762712
6	0.963797	0.124301	0.118653	0.952790	0.904671	0.152474	0.096386		0.745763
7	0.967443	0.117380	0.123686	0.959227	0.905097	0.162120	0.093586		0.754237
8	0.966170	0.120232	0.122699	0.944206	0.904048	0.138807	0.105651		0.728814
9	0.966986	0.118878	0.125558	0.965665	0.904412	0.147952	0.106796		0.745763


best stage1 with stage2 1e-6_1e-3(8)_1e-4(2)

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.960043	0.135265	0.111812	0.950644	0.906699	0.174097	0.092722		0.788136
1	0.962393	0.128596	0.118242	0.952790	0.905888	0.155016	0.094595		0.771186
2	0.960291	0.133016	0.112936	0.944206	0.905704	0.161674	0.089219		0.813559
3	0.964411	0.121938	0.117258	0.950644	0.898508	0.133720	0.097315		0.737288
4	0.966580	0.117986	0.121991	0.957082	0.904122	0.118536	0.108723		0.728814
5	0.968956	0.115395	0.137183	0.963519	0.897012	0.134832	0.102464		0.669492
6	0.968047	0.116462	0.125352	0.954936	0.895059	0.125215	0.118497		0.694915
7	0.971851	0.109484	0.138026	0.963519	0.893354	0.151491	0.096133		0.737288
8	0.970558	0.111110	0.134750	0.952790	0.887722	0.121510	0.100130		0.652542
9	0.977592	0.097560	0.149983	0.961373	0.888030	0.138347	0.108666		0.669492


best stage1 with stage2 1e-5_1e-4_1e-4

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.960767	0.134081	0.110398	0.959227	0.905867	0.170122	0.096273		0.788136
1	0.961577	0.132179	0.114286	0.944206	0.906274	0.163588	0.098361		0.762712
2	0.960731	0.132497	0.114578	0.937768	0.906265	0.172565	0.091618		0.796610
3	0.961614	0.129302	0.114006	0.948498	0.905918	0.154593	0.098126		0.754237
4	0.959325	0.133592	0.112941	0.942060	0.907869	0.142270	0.098999		0.754237
5	0.966828	0.120233	0.119407	0.967811	0.906584	0.153636	0.099664		0.754237
6	0.965910	0.120830	0.120544	0.950644	0.904799	0.146189	0.104142		0.745763
7	0.965826	0.119194	0.126745	0.954936	0.905551	0.159307	0.094044		0.762712
8	0.966263	0.118833	0.125141	0.950644	0.906578	0.140538	0.103203		0.737288
9	0.969448	0.110919	0.133984	0.972103	0.902342	0.138886	0.110505		0.686441


best stage1 with stage2 1e-6_1e-5_1e-6

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.960290	0.136123	0.110169	0.948498	0.905782	0.177801	0.092445		0.788136
1	0.961295	0.136160	0.110383	0.939914	0.906426	0.162660	0.099783		0.779661
2	0.960784	0.135171	0.110163	0.942060	0.905784	0.181827	0.090379		0.788136
3	0.958674	0.137233	0.109671	0.944206	0.906482	0.156386	0.100897		0.762712
4	0.960427	0.134515	0.110889	0.952790	0.906647	0.163249	0.098925		0.779661
5	0.962382	0.131541	0.112434	0.952790	0.906073	0.171262	0.094553		0.779661
6	0.961580	0.132954	0.111961	0.942060	0.906125	0.166648	0.095288		0.771186
7	0.961409	0.132094	0.113010	0.950644	0.905806	0.179445	0.091266		0.788136
8	0.961020	0.132924	0.114109	0.952790	0.906195	0.157219	0.100223		0.762712
9	0.959777	0.134079	0.115008	0.957082	0.906371	0.154875	0.101714		0.754237


stage2 1e-6_1e-5_1e-6 g2

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.959920	0.079938	0.111698	0.952790	0.905577	0.115099	0.093280		0.788136
1	0.959795	0.078964	0.109637	0.939914	0.906407	0.106285	0.097015		0.771186
2	0.958480	0.080201	0.109155	0.931330	0.906125	0.117297	0.089780		0.796610
3	0.957924	0.077482	0.108652	0.927039	0.907130	0.097564	0.099021		0.771186
4	0.957781	0.076827	0.108921	0.924893	0.906866	0.095506	0.097768		0.779661
5	0.961067	0.073658	0.113391	0.948498	0.907086	0.093055	0.097535		0.771186
6	0.960546	0.074085	0.111536	0.939914	0.906582	0.095700	0.093401		0.779661
7	0.961863	0.071334	0.114993	0.946352	0.906385	0.093773	0.093333		0.771186
8	0.961681	0.071441	0.116419	0.957082	0.906752	0.082143	0.102445		0.745763
9	0.957548	0.075590	0.115093	0.944206	0.906733	0.081518	0.106332		0.754237

stage2 1e-6_1e-5_1e-6 g0.5

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.961326	0.193536	0.110918	0.959227	0.905790	0.231759	0.093467		0.788136
1	0.961220	0.193745	0.110749	0.948498	0.906325	0.214936	0.099119		0.762712
2	0.961634	0.192022	0.111475	0.950644	0.905991	0.237433	0.091716		0.788136
3	0.958950	0.195255	0.107434	0.933476	0.906112	0.221058	0.098166		0.771186
4	0.959550	0.193171	0.109531	0.942060	0.905836	0.217016	0.099889		0.762712
5	0.960950	0.190035	0.109073	0.944206	0.905283	0.237477	0.093117		0.779661
6	0.962133	0.186279	0.108380	0.946352	0.905468	0.231678	0.095535		0.779661
7	0.963098	0.183648	0.111194	0.961373	0.905195	0.240741	0.092629		0.788136
8	0.959791	0.189774	0.107809	0.942060	0.906049	0.207268	0.101163		0.737288
9	0.962377	0.184646	0.111886	0.963519	0.905520	0.219485	0.098779		0.754237


stage2 1e-4_5e-4_1e-4 (5 epochs) g1

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.961176	0.131193	0.116431	0.954936	0.905477	0.162862	0.093973		0.779661
1	0.962494	0.130151	0.118977	0.948498	0.903468	0.137372	0.108696		0.720339
2	0.963163	0.127762	0.123801	0.942060	0.904275	0.161673	0.092885		0.796610
3	0.966493	0.120118	0.122749	0.950644	0.900508	0.131328	0.103713		0.686441
4	0.970870	0.111897	0.142948	0.961373	0.901614	0.134326	0.106632		0.694915


stage2 1e-4_5e-4_1e-4 (5 epochs) g1 ls0.05

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.960654	0.136427	0.112615	0.946352	0.906413	0.167239	0.094320		0.788136
1	0.961888	0.134699	0.113037	0.937768	0.906269	0.163840	0.088015		0.796610
2	0.962610	0.131133	0.115961	0.946352	0.903615	0.167415	0.090997		0.796610
3	0.965044	0.126311	0.119695	0.944206	0.899108	0.145372	0.095921		0.737288
4	0.970362	0.114366	0.132822	0.965665	0.891194	0.142255	0.103581		0.686441


stage2 lr = 1e-4 - 2 epochs

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.961060	0.134115	0.112691	0.935622	0.906296	0.165033	0.097614		0.762712
1	0.962804	0.129991	0.117788	0.946352	0.906362	0.154543	0.099323		0.745763


stage2 lr = 1e-5 - 2 epochs

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.960159	0.135612	0.111641	0.954936	0.905899	0.171198	0.095778		0.788136
1	0.960848	0.134202	0.113544	0.957082	0.906000	0.164324	0.097826		0.762712


stage2 from L66 with 1e-4_5e-4_1e-4 : 5 epoch

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.961189	0.132136	0.114673	0.944206	0.908065	0.156501	0.096134		0.779661
1	0.962738	0.127077	0.115715	0.952790	0.906076	0.151788	0.094553		0.779661
2	0.961880	0.130930	0.120970	0.942060	0.907261	0.169912	0.092412		0.805085
3	0.967215	0.116833	0.131884	0.950644	0.902181	0.139188	0.095732		0.703390
4	0.969357	0.112899	0.134835	0.963519	0.901170	0.137548	0.107612		0.694915



without training=False : stage1 from L287 1e-3_1e-2_1e-4 g1_ls0.01 stage2 full 1e-4_1e-3_1e-4 

https://www.kaggle.com/code/abhivij/melanoma-classification/notebook?scriptVersionId=227041385

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.743954	0.358183	0.035257	0.751073	0.838543	0.273728	0.060496		0.703390
1	0.826998	0.293047	0.048532	0.776824	0.830953	0.231048	0.068811		0.652542
2	0.859137	0.266078	0.053020	0.817597	0.865869	0.274880	0.055556		0.855932
3	0.875288	0.248237	0.057738	0.839056	0.874232	0.229083	0.063845		0.779661
4	0.852726	0.266953	0.049665	0.828326	0.797305	0.273083	0.042771		0.779661
5	0.834394	0.283091	0.045657	0.815451	0.878217	0.278907	0.061529		0.838983
6	0.864779	0.249798	0.052203	0.811159	0.889129	0.227485	0.065596		0.830508
7	0.894750	0.220806	0.065153	0.849785	0.897960	0.180726	0.081301		0.762712
8	0.918242	0.192815	0.073843	0.890558	0.900620	0.198199	0.076258		0.822034
9	0.933032	0.174036	0.078439	0.914163	0.903994	0.178584	0.087466		0.822034

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.867746	0.251257	0.062661	0.783262	0.844957	0.203230	0.059566		0.720339
1	0.874540	0.232016	0.058770	0.824034	0.862553	0.211792	0.061798		0.745763
2	0.871018	0.233516	0.057242	0.817597	0.861735	0.237760	0.067035		0.720339
3	0.878202	0.227259	0.052878	0.849785	0.898744	0.224421	0.073244		0.830508
4	0.918123	0.192713	0.066152	0.881974	0.908208	0.223047	0.078643		0.864407


without training=False stage2_1e-5_1e-4_1e-5

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.937360	0.168602	0.080498	0.929185	0.903600	0.191490	0.083333		0.830508
1	0.937667	0.167405	0.080414	0.916309	0.904417	0.184319	0.084731		0.813559
2	0.940296	0.164888	0.083998	0.927039	0.903544	0.202052	0.079770		0.822034
3	0.940924	0.162836	0.086472	0.916309	0.904635	0.171846	0.089524		0.796610
4	0.947379	0.154222	0.089945	0.946352	0.903837	0.173375	0.089509		0.788136


hadn't previously actually unfreezed full model ! Corrected that now

stage2 1e-5_5e-5_1e-5 (5 epochs)

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.863381	0.294896	0.074649	0.660944	0.876624	0.187814	0.079279		0.745763
1	0.913817	0.203702	0.085909	0.811159	0.887823	0.181580	0.080000		0.779661
2	0.935219	0.171722	0.090693	0.899142	0.880875	0.201180	0.072827		0.788136
3	0.949356	0.150498	0.095801	0.939914	0.886569	0.163376	0.085540		0.711864
4	0.955092	0.143658	0.098732	0.935622	0.888130	0.172507	0.083015		0.737288

stage2 1e-4_5e-4_1e-4 (5 epochs)

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.888622	0.236385	0.074522	0.793991	0.883020	0.170773	0.075021		0.745763
1	0.915006	0.195665	0.074597	0.884120	0.877208	0.154282	0.093023		0.711864
2	0.921525	0.182293	0.075554	0.892704	0.860889	0.223062	0.050721		0.805085
3	0.914003	0.191898	0.066059	0.886266	0.887194	0.195215	0.066059		0.737288
4	0.953874	0.145926	0.098881	0.948498	0.905934	0.169534	0.096983		0.762712


stage2 1e-4_5e-4_1e-4 adam_beta1_0.6

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.884303	0.243183	0.072452	0.781116	0.897942	0.169872	0.077572		0.779661
1	0.921388	0.183028	0.071441	0.894850	0.853156	0.139252	0.078390		0.627119
2	0.904144	0.206720	0.066862	0.832618	0.868479	0.248740	0.055843		0.822034
3	0.930689	0.173681	0.079240	0.903434	0.858248	0.179565	0.067362		0.711864
4	0.948693	0.152162	0.090226	0.927039	0.896313	0.172080	0.087945		0.754237


stage2 1e-4_5e-4_1e-4 rmsprop

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.882383	0.252114	0.075822	0.761803	0.875645	0.134618	0.094470		0.694915
1	0.904657	0.211318	0.072889	0.826180	0.876159	0.135978	0.099075		0.635593
2	0.906634	0.206023	0.073996	0.826180	0.849047	0.181711	0.065760		0.737288
3	0.914455	0.199279	0.076938	0.856223	0.843429	0.139792	0.093385		0.610169
4	0.948211	0.154199	0.102232	0.914163	0.883335	0.105179	0.126000		0.533898


stage2 1e-5_5e-5_1e-5 rmsprop

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.867035	0.291129	0.079899	0.682403	0.873497	0.164943	0.087662		0.686441
1	0.910849	0.212440	0.088519	0.787554	0.881399	0.152003	0.090058		0.652542
2	0.929330	0.182071	0.094541	0.873391	0.873832	0.166692	0.085216		0.703390
3	0.948744	0.154248	0.103210	0.931330	0.871500	0.140455	0.104302		0.677966
4	0.957853	0.141181	0.107214	0.918455	0.868299	0.146335	0.096894		0.661017


stage2 LR1e-5 rmsprop

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.846183	0.325415	0.076692	0.656652	0.858648	0.172528	0.077963		0.635593
1	0.900530	0.230991	0.086722	0.759657	0.873450	0.161917	0.087527		0.677966

stage2 LR1e-6 rmsprop

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.807438	0.394872	0.069215	0.594421	0.808720	0.181358	0.066667		0.533898
1	0.830460	0.354826	0.071041	0.590129	0.824102	0.187344	0.068205		0.576271




stage1 - convnexttiny 1e-3_1e-2_1e-4 g1_ls0.01

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.757301	0.341269	0.035725	0.748927	0.804237	0.311509	0.043886		0.754237
1	0.813605	0.297659	0.043679	0.770386	0.836752	0.305528	0.046205		0.830508
2	0.844221	0.286305	0.050469	0.785408	0.844183	0.272472	0.057692		0.762712
3	0.840220	0.300747	0.051001	0.798283	0.857319	0.300091	0.054657		0.830508
4	0.854528	0.297841	0.051647	0.793991	0.848800	0.210238	0.103659		0.576271
5	0.852245	0.312072	0.056089	0.815451	0.854945	0.289793	0.061495		0.745763
6	0.856406	0.308823	0.055273	0.793991	0.865619	0.243181	0.083151		0.644068
7	0.874807	0.286961	0.059563	0.813305	0.869179	0.249685	0.072128		0.686441
8	0.889601	0.265154	0.066212	0.832618	0.869220	0.270738	0.061497		0.779661
9	0.906919	0.245150	0.067834	0.864807	0.870061	0.252436	0.064710		0.728814


convnexttiny L111 1e-3_1e-2_1e-4 g1_ls0.01

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.776240	0.328398	0.037935	0.755365	0.753833	0.342119	0.036099		0.652542
1	0.842589	0.274960	0.047414	0.802575	0.854662	0.300358	0.046364		0.805085
2	0.857024	0.262669	0.052129	0.809013	0.870510	0.350444	0.049829		0.864407
3	0.871194	0.247677	0.056540	0.821888	0.832771	0.173385	0.071142		0.601695
4	0.868280	0.248811	0.056670	0.836910	0.862432	0.138381	0.118966		0.584746
5	0.888799	0.227147	0.063188	0.849785	0.877357	0.166441	0.089617		0.694915
6	0.893340	0.222835	0.064711	0.851931	0.877248	0.243565	0.066667		0.788136
7	0.913470	0.200769	0.071656	0.869099	0.888409	0.191211	0.071082		0.745763
8	0.922623	0.187428	0.077751	0.884120	0.894356	0.174317	0.080219		0.745763
9	0.944449	0.160327	0.091681	0.927039	0.892444	0.175279	0.082397		0.745763


convnexttiny L111 1e-4_1e-2_1e-3 g1_ls0.01

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.738343	0.368501	0.033914	0.772532	0.781656	0.744555	0.021691		0.991525
1	0.830891	0.285473	0.050027	0.785408	0.668488	1.422193	0.018215		1.000000
2	0.853423	0.266190	0.053539	0.813305	0.871193	0.276553	0.054335		0.796610
3	0.869485	0.250432	0.058170	0.819743	0.851366	0.209698	0.077213		0.694915
4	0.872086	0.247137	0.058657	0.802575	0.848907	0.174012	0.139726		0.432203
5	0.882299	0.237082	0.059755	0.817597	0.862248	0.111112	0.177215		0.355932
6	0.894686	0.222819	0.066263	0.845494	0.878438	0.129133	0.124774		0.584746
7	0.911063	0.204727	0.072689	0.884120	0.882214	0.172927	0.082745		0.694915
8	0.928190	0.184558	0.086203	0.886266	0.882699	0.314802	0.049832		0.881356
9	0.945914	0.160735	0.097760	0.927039	0.887382	0.184353	0.084211		0.745763



convnexttiny L111 1e-3_1e-2_1e-3 g1_ls0.01


-----------------------------------------------prev best--------------------------------------
stage1 from L287 1e-3_1e-2_1e-4 g1_ls0.01

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.765620	0.334637	0.035696	0.744635	0.833109	0.253167	0.073693		0.728814
1	0.810675	0.299398	0.045420	0.768240	0.853534	0.237440	0.075229		0.694915
2	0.850397	0.275142	0.054843	0.798283	0.846960	0.198962	0.077164		0.627119
3	0.859502	0.259848	0.053371	0.806867	0.852351	0.179622	0.096606		0.627119
4	0.865006	0.250606	0.053208	0.824034	0.857439	0.146891	0.138587		0.432203
5	0.876510	0.245771	0.058849	0.858369	0.859124	0.248866	0.064834		0.779661
6	0.900835	0.217308	0.066512	0.860515	0.886128	0.185199	0.083712		0.779661
7	0.919504	0.194073	0.073603	0.881974	0.892101	0.166008	0.088264		0.771186
8	0.934332	0.174716	0.085831	0.903434	0.906774	0.168660	0.087189		0.830508
9	0.957214	0.141656	0.104827	0.950644	0.906944	0.152170	0.103248		0.754237
----------------------------------------------------------------------------------------------

128x128 effnet L287 1e-4_1e-2_1e-3 dropout 0.4/0.2

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.720995	0.388779	0.030817	0.740343	0.783763	0.403574	0.049883		0.720339
1	0.807198	0.297895	0.041252	0.800429	0.843334	0.267628	0.050135		0.788136
2	0.840139	0.279187	0.046541	0.798283	0.871963	0.285901	0.053419		0.847458
3	0.865514	0.259056	0.053207	0.834764	0.877175	0.196042	0.089569		0.669492
4	0.867438	0.253549	0.053938	0.834764	0.889216	0.172908	0.089325		0.694915
5	0.882977	0.235844	0.061034	0.826180	0.883098	0.240154	0.062910		0.813559
6	0.895350	0.219305	0.065902	0.875537	0.890314	0.187225	0.093714		0.694915
7	0.921572	0.192923	0.074527	0.888412	0.885481	0.164073	0.094857		0.703390
8	0.942279	0.165452	0.092720	0.899142	0.892051	0.175332	0.099874		0.669492
9	0.959992	0.134896	0.117916	0.942060	0.894694	0.150400	0.100883		0.677966


256x256 effnet L287 1e-3_1e-2_1e-4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.764247	0.336788	0.036233	0.766094	0.855574	0.248035	0.059107		0.728814
1	0.843144	0.281873	0.049777	0.813305	0.858266	0.294032	0.054586		0.822034
2	0.856332	0.271218	0.053935	0.817597	0.862578	0.267808	0.065414		0.796610
3	0.873204	0.254065	0.059997	0.845494	0.881617	0.230764	0.066578		0.847458
4	0.882802	0.238677	0.059546	0.832618	0.846530	0.187545	0.076681		0.618644
5	0.876649	0.237908	0.054126	0.871245	0.877507	0.237517	0.062056		0.813559
6	0.910070	0.207224	0.068082	0.894850	0.891813	0.190276	0.081059		0.830508
7	0.925192	0.195908	0.080393	0.894850	0.890259	0.191056	0.079899		0.805085
8	0.945216	0.162929	0.094360	0.922747	0.901289	0.155367	0.100220		0.771186
9	0.960863	0.135086	0.107480	0.952790	0.899076	0.162925	0.093487		0.754237


256x256 effnet L287 1e-3_1e-2_1e-3

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.776213	0.328447	0.037730	0.774678	0.802700	0.254200	0.050779		0.635593
1	0.843297	0.276837	0.047041	0.815451	0.865323	0.367912	0.048581		0.855932
2	0.857106	0.271350	0.053321	0.847640	0.826621	0.616291	0.026790		0.957627
3	0.860423	0.269915	0.052778	0.815451	0.874877	0.246300	0.063033		0.813559
4	0.884136	0.239841	0.059393	0.856223	0.880951	0.217357	0.078530		0.796610
5	0.891187	0.232398	0.060853	0.866953	0.895818	0.345983	0.057377		0.889831
6	0.911001	0.206919	0.069012	0.875537	0.874685	0.220491	0.064907		0.771186
7	0.928981	0.185094	0.079541	0.907725	0.903150	0.150219	0.095965		0.745763
8	0.941311	0.168227	0.093791	0.920601	0.901816	0.156611	0.095890		0.711864
9	0.960605	0.133611	0.112596	0.939914	0.903630	0.130184	0.110390		0.720339


256x256 effnet L287 1e-4_1e-2_1e-3 dropout 0.4/0.2

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.745457	0.357051	0.033271	0.806867	0.827854	0.326143	0.063189		0.728814
1	0.831905	0.286157	0.047589	0.813305	0.860750	0.243865	0.065170		0.762712
2	0.848141	0.274213	0.051286	0.817597	0.871500	0.364666	0.048700		0.872881
3	0.880064	0.244210	0.058065	0.841202	0.839253	0.251250	0.061175		0.635593
4	0.888865	0.232179	0.061543	0.871245	0.896177	0.213182	0.074579		0.788136
5	0.899273	0.218596	0.062814	0.886266	0.905603	0.241686	0.073310		0.872881
6	0.897720	0.224261	0.065110	0.873391	0.897826	0.226691	0.073939		0.855932
7	0.928947	0.185784	0.079614	0.903434	0.906333	0.153042	0.099109		0.754237
8	0.940071	0.168878	0.088119	0.912017	0.900265	0.163586	0.096195		0.771186
9	0.956223	0.145459	0.114494	0.944206	0.908847	0.150146	0.107692		0.771186


	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.744798	0.355453	0.032716	0.800429	0.856202	0.279173	0.053297		0.822034
1	0.836205	0.281097	0.048130	0.809013	0.861322	0.304006	0.055454		0.771186
2	0.841375	0.279990	0.050193	0.783262	0.882545	0.387844	0.045907		0.898305
3	0.851900	0.274436	0.052132	0.836910	0.876628	0.288235	0.069675		0.745763
4	0.871460	0.252386	0.057926	0.839056	0.871573	0.210964	0.067663		0.728814
5	0.885826	0.232667	0.057709	0.869099	0.892161	0.232843	0.062738		0.838983
6	0.895385	0.224986	0.062244	0.881974	0.889531	0.204787	0.068676		0.830508
7	0.915994	0.203749	0.070598	0.899142	0.892952	0.212842	0.063984		0.822034
8	0.935091	0.176562	0.080688	0.916309	0.894787	0.182584	0.077876		0.745763
9	0.954694	0.146019	0.104852	0.950644	0.893386	0.162277	0.091826		0.771186


	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.748633	0.360951	0.033168	0.787554	0.841657	0.251203	0.061484		0.737288
1	0.818992	0.297649	0.047619	0.796137	0.841726	0.276899	0.046944		0.813559
2	0.856989	0.270451	0.052966	0.821888	0.881813	0.305647	0.055349		0.881356
3	0.881956	0.244605	0.061541	0.851931	0.870371	0.266407	0.065817		0.788136
4	0.883849	0.234450	0.056828	0.830472	0.895589	0.207369	0.077419		0.813559
5	0.894003	0.222652	0.062519	0.886266	0.894568	0.212021	0.067855		0.838983
6	0.916672	0.197736	0.073403	0.894850	0.880522	0.177744	0.075214		0.745763
7	0.922902	0.190474	0.076741	0.903434	0.901057	0.176266	0.078925		0.796610
8	0.943504	0.162224	0.087737	0.933476	0.902079	0.139187	0.099768		0.728814
9	0.958526	0.136409	0.111054	0.933476	0.904515	0.153517	0.095679		0.788136


256x256 effnet L287 1e-4_1e-2_1e-3 L2Reg 4e-4 dropout0.4/0.2 - 31 min

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.738521	0.449418	0.032255	0.791846	0.807660	0.381418	0.047782		0.711864
1	0.842775	0.348598	0.048451	0.839056	0.848026	0.330043	0.061118		0.796610
2	0.869877	0.302840	0.054054	0.824034	0.861399	0.358834	0.048816		0.855932
3	0.874871	0.273277	0.059388	0.836910	0.862641	0.353909	0.050101		0.838983
4	0.869630	0.266120	0.053426	0.843348	0.883498	0.200511	0.076649		0.728814
5	0.886096	0.262843	0.060364	0.860515	0.878442	0.241074	0.073432		0.754237
6	0.906865	0.225268	0.067702	0.888412	0.882334	0.196531	0.075928		0.745763
7	0.928795	0.194757	0.082261	0.905579	0.884828	0.163769	0.090395		0.677966
8	0.943203	0.167476	0.091421	0.905579	0.896496	0.157925	0.091571		0.745763
9	0.961412	0.132079	0.111637	0.961373	0.894731	0.139388	0.102445		0.745763


300x300 effnet L287 1e-4_1e-2_1e-3 							- 43 min

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.743521	0.365569	0.032937	0.802575	0.834155	0.280104	0.056010		0.754237
1	0.813707	0.295720	0.044648	0.793991	0.852997	0.283921	0.047929		0.872881
2	0.865957	0.257959	0.053953	0.834764	0.891637	0.249497	0.066667		0.847458
3	0.872883	0.247481	0.055143	0.875537	0.861793	0.320590	0.044161		0.881356
4	0.875194	0.250394	0.057094	0.858369	0.891917	0.238216	0.068892		0.822034
5	0.896752	0.223274	0.063099	0.875537	0.885346	0.258411	0.072812		0.796610
6	0.893899	0.229103	0.064485	0.866953	0.892553	0.194231	0.079555		0.788136
7	0.920512	0.200809	0.077139	0.886266	0.895495	0.180840	0.079017		0.762712
8	0.940535	0.168200	0.089031	0.924893	0.901861	0.174110	0.087923		0.771186
9	0.954713	0.143435	0.103497	0.946352	0.904717	0.155317	0.101862		0.788136


384x384 effnet L287 1e-4_1e-2_1e-3 							- 59 min

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.735943	0.363870	0.033267	0.791846	0.752458	0.393782	0.036392		0.779661
1	0.827734	0.290125	0.046800	0.793991	0.867653	0.255166	0.060513		0.838983
2	0.872580	0.254079	0.055920	0.860515	0.889746	0.307458	0.053279		0.881356
3	0.883789	0.239145	0.058841	0.866953	0.834319	0.356856	0.044124		0.805085
4	0.887950	0.229795	0.057583	0.849785	0.898835	0.186725	0.089899		0.754237
5	0.898632	0.219972	0.062348	0.881974	0.866113	0.257818	0.063300		0.754237
6	0.897865	0.224687	0.062329	0.879828	0.891038	0.220736	0.062166		0.788136
7	0.929510	0.185349	0.079695	0.896996	0.908731	0.172104	0.086299		0.822034
8	0.941207	0.164990	0.086210	0.927039	0.903560	0.164037	0.083184		0.788136
9	0.956339	0.141425	0.101850	0.933476	0.902784	0.178521	0.088235		0.788136


384x384 effnet L287 1e-3_1e-2_1e-4 dropout0.3_0.15 			- 59 min

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.785454	0.326609	0.039284	0.800429	0.860232	0.343525	0.048558		0.855932
1	0.843471	0.280553	0.050921	0.806867	0.808159	0.498392	0.052870		0.593220
2	0.857024	0.268507	0.052180	0.847640	0.891494	0.363151	0.048344		0.915254
3	0.874236	0.248705	0.058710	0.845494	0.891408	0.283931	0.052709		0.906780
4	0.886037	0.230966	0.059333	0.881974	0.884627	0.209902	0.071489		0.720339
5	0.890477	0.229340	0.059351	0.879828	0.874176	0.271758	0.054407		0.847458
6	0.899921	0.225324	0.063504	0.890558	0.889619	0.266142	0.057586		0.881356
7	0.918760	0.199893	0.070936	0.905579	0.904744	0.208396	0.075140		0.796610
8	0.937857	0.167582	0.080701	0.948498	0.911453	0.181446	0.075237		0.872881
9	0.950602	0.148603	0.090010	0.957082	0.909707	0.181206	0.083056		0.847458


384x384 effnet L287 1e-4_1e-2_1e-3 dropout0.4_0.2                                                   --- using this configuration result for comparison with other sizes

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.731636	0.365077	0.031976	0.776824	0.855054	0.405733	0.043405		0.872881
1	0.833758	0.281978	0.046924	0.828326	0.882104	0.284962	0.049051		0.898305
2	0.860965	0.262410	0.050414	0.824034	0.877159	0.316471	0.064927		0.864407
3	0.852982	0.268859	0.052516	0.817597	0.892166	0.266432	0.065920		0.847458
4	0.854715	0.269216	0.054111	0.821888	0.875981	0.235212	0.070928		0.771186
5	0.877142	0.240435	0.055556	0.854077	0.865533	0.284285	0.062764		0.754237
6	0.890415	0.225564	0.060344	0.851931	0.892844	0.303596	0.069454		0.872881
7	0.917970	0.192822	0.074302	0.907725	0.890596	0.259389	0.070087		0.822034
8	0.933338	0.172681	0.081389	0.920601	0.900405	0.223395	0.080851		0.805085
9	0.950067	0.149908	0.097148	0.935622	0.908687	0.185396	0.094094		0.796610


384x384 effnet L287 1e-4_1e-2_1e-3 dropout0.3_0.15

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.748598	0.351173	0.033073	0.772532	0.869324	0.287841	0.050195		0.872881
1	0.829794	0.285211	0.046197	0.806867	0.864412	0.428729	0.039483		0.932203
2	0.852841	0.266993	0.051098	0.824034	0.878720	0.242143	0.060453		0.813559
3	0.859839	0.261959	0.053734	0.809013	0.883038	0.258448	0.085832		0.703390
4	0.858232	0.257019	0.053283	0.821888	0.877704	0.263114	0.056560		0.822034
5	0.876001	0.236784	0.057078	0.858369	0.881484	0.246089	0.071975		0.796610
6	0.885729	0.227045	0.061631	0.856223	0.902536	0.254726	0.068091		0.864407
7	0.919613	0.190255	0.072888	0.903434	0.889764	0.239172	0.056754		0.847458
8	0.930536	0.173545	0.077641	0.909871	0.910048	0.209729	0.070375		0.889831
9	0.952649	0.141227	0.094271	0.946352	0.904527	0.183140	0.083333		0.813559





512x512 BS=16, gradaccu=4 L287 1e-4_1e-2_1e-3 				- 1h 34min

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.709669	0.386867	0.029821	0.748927	0.828304	0.294940	0.040833		0.830508
1	0.806749	0.297063	0.041985	0.802575	0.875697	0.300811	0.040263		0.881356
2	0.819756	0.291885	0.046414	0.778970	0.893764	0.325395	0.050728		0.915254
3	0.835547	0.277144	0.045893	0.817597	0.888239	0.330118	0.050239		0.889831
4	0.844414	0.271503	0.050109	0.836910	0.886984	0.328611	0.052144		0.906780
5	0.872158	0.244872	0.054360	0.856223	0.904809	0.260898	0.052453		0.915254
6	0.886809	0.229652	0.060180	0.873391	0.901793	0.262306	0.064220		0.889831
7	0.898771	0.211079	0.062662	0.884120	0.894783	0.275571	0.058279		0.906780
8	0.919112	0.189883	0.073706	0.907725	0.896026	0.262909	0.061442		0.830508
9	0.926058	0.176455	0.077798	0.909871	0.906357	0.251926	0.067401		0.881356



256x256 effnet L287 1e-4_1e-2_1e-3 dropout0.3_0.15 batchsize 64 grad_accu 4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.742682	0.383685	0.030819	0.791846	0.790894	0.314519	0.044753		0.737288
1	0.838067	0.278677	0.048072	0.834764	0.812919	0.258619	0.051443		0.694915
2	0.869383	0.255768	0.055564	0.821888	0.886130	0.297261	0.049482		0.889831
3	0.894821	0.232649	0.064064	0.873391	0.870461	0.215901	0.064000		0.745763
4	0.903105	0.221474	0.068368	0.879828	0.865648	0.229558	0.063568		0.788136
5	0.912580	0.209267	0.071929	0.903434	0.884688	0.236734	0.059552		0.855932
6	0.878703	0.244359	0.061673	0.849785	0.893334	0.200801	0.073729		0.737288
7	0.930345	0.188002	0.079424	0.912017	0.899293	0.179774	0.082317		0.686441
8	0.947547	0.162281	0.092380	0.931330	0.907472	0.177537	0.090909		0.796610
9	0.958231	0.143788	0.107442	0.935622	0.905131	0.175927	0.092170		0.788136


256x256 effnet L287 1e-4_1e-2_1e-3 dropout0.4_0.04 batchsize 64 grad_accu 4

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.742082	0.380394	0.030759	0.828326	0.841569	0.409553	0.037491		0.906780
1	0.857211	0.264346	0.049653	0.860515	0.869947	0.247663	0.073590		0.762712
2	0.858774	0.264086	0.054178	0.834764	0.859407	0.286775	0.050857		0.779661
3	0.884067	0.242201	0.057398	0.884120	0.868634	0.206027	0.070747		0.754237
4	0.903230	0.221084	0.066678	0.879828	0.880973	0.227351	0.068118		0.822034
5	0.884962	0.240045	0.060254	0.834764	0.889725	0.232200	0.067460		0.864407
6	0.913278	0.210442	0.072689	0.884120	0.876680	0.189302	0.074343		0.694915
7	0.939036	0.176463	0.088552	0.916309	0.896501	0.206759	0.076175		0.796610
8	0.950258	0.159689	0.102185	0.933476	0.905900	0.170409	0.094488		0.813559
9	0.964235	0.135587	0.126430	0.948498	0.907795	0.153237	0.105985		0.720339


256x256 effnet L287 1e-4_1e-2_1e-3 dropout0.3_0.15

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.736074	0.365440	0.031223	0.763949	0.822736	0.325985	0.044244		0.830508
1	0.835821	0.280322	0.047075	0.804721	0.862922	0.258922	0.050873		0.864407
2	0.866863	0.255888	0.052547	0.841202	0.884081	0.286831	0.053684		0.864407
3	0.870082	0.252060	0.054649	0.862661	0.870942	0.336588	0.059242		0.847458
4	0.879263	0.243496	0.057710	0.830472	0.877499	0.285618	0.059168		0.855932
5	0.899309	0.221897	0.064612	0.886266	0.902803	0.249206	0.057440		0.889831
6	0.914377	0.199091	0.071174	0.899142	0.894950	0.186411	0.074017		0.813559
7	0.933247	0.177131	0.084476	0.912017	0.899131	0.162440	0.084172		0.779661
8	0.946588	0.153982	0.091850	0.933476	0.906741	0.120959	0.118621		0.728814
9	0.962571	0.128553	0.114711	0.942060	0.906999	0.134107	0.109337		0.754237


256x256 effnet L287 1e-4_1e-2_1e-3 dropout0.4_0.04

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.759338	0.348620	0.033840	0.821888	0.832861	0.319466	0.038391		0.881356
1	0.830675	0.287711	0.045543	0.806867	0.868088	0.309815	0.062914		0.805085
2	0.855139	0.267904	0.052316	0.824034	0.810462	0.309011	0.041832		0.805085
3	0.863487	0.258844	0.054187	0.849785	0.882589	0.233628	0.072391		0.728814
4	0.889673	0.227770	0.059613	0.858369	0.879872	0.216992	0.071726		0.728814
5	0.884566	0.232148	0.058006	0.869099	0.879289	0.279549	0.049342		0.889831
6	0.904789	0.213133	0.066688	0.892704	0.884899	0.212015	0.070054		0.771186
7	0.926096	0.186671	0.082956	0.903434	0.891711	0.150876	0.092613		0.711864
8	0.935446	0.172956	0.086630	0.916309	0.901011	0.162879	0.090625		0.737288
9	0.955877	0.141786	0.107414	0.942060	0.901923	0.150119	0.101734		0.745763


256x256 effnet L287 1e-4_1e-2_1e-3 dropout0.2_0.02

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.768929	0.339049	0.035791	0.817597	0.811166	0.259062	0.051199		0.669492
1	0.821079	0.293175	0.046379	0.802575	0.852521	0.280560	0.050717		0.838983
2	0.854859	0.266974	0.052373	0.824034	0.866837	0.305761	0.047575		0.864407
3	0.871708	0.250510	0.055214	0.828326	0.884429	0.245361	0.055176		0.822034
4	0.883936	0.235978	0.060748	0.864807	0.884309	0.158436	0.119685		0.644068
5	0.889260	0.230047	0.062040	0.869099	0.881235	0.195536	0.072727		0.779661
6	0.904591	0.209675	0.068718	0.881974	0.879327	0.227203	0.068182		0.788136
7	0.935331	0.171983	0.085847	0.909871	0.897751	0.171961	0.081722		0.788136
8	0.947876	0.150985	0.094059	0.937768	0.893413	0.140707	0.104218		0.711864
9	0.959736	0.133114	0.114190	0.946352	0.897527	0.139309	0.109317		0.745763


256x256 effnet L287 1e-4_1e-2_1e-3 dropout0.2_0.1

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.755983	0.351082	0.032832	0.800429	0.841475	0.291251	0.060976		0.677966
1	0.811800	0.298443	0.044600	0.793991	0.854776	0.309946	0.040673		0.881356
2	0.842656	0.275180	0.047254	0.832618	0.885133	0.327035	0.051749		0.915254
3	0.855745	0.263769	0.051431	0.817597	0.855938	0.329428	0.058859		0.822034
4	0.872298	0.241649	0.053433	0.858369	0.886433	0.208535	0.070122		0.779661
5	0.874053	0.243310	0.054235	0.871245	0.892390	0.175053	0.090999		0.779661
6	0.899283	0.215106	0.064666	0.896996	0.886712	0.154780	0.090810		0.703390
7	0.922686	0.188302	0.074274	0.884120	0.890162	0.163520	0.080292		0.745763
8	0.930337	0.174259	0.079127	0.918455	0.899996	0.149665	0.090020		0.779661
9	0.954888	0.138102	0.100842	0.950644	0.897000	0.157489	0.087669		0.771186



stage2 128x128 LR_1e-5

{'AUC': 0.8947768510120737, 'Recall': 0.6779661016949152, 'Precision': 0.1008827238335435}

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.866374	0.317669	0.083694	0.663090	0.868795	0.169729	0.080292		0.652542
1	0.910075	0.223095	0.093597	0.781116	0.880507	0.169275	0.080039		0.703390


stage2 256x256 LR_1e-5

{'AUC': 0.9048753060336466, 'Recall': 0.788135593220339, 'Precision': 0.09567901234567901}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.862006	0.326959	0.079919	0.673820	0.874742	0.135127	0.094763		0.644068
1	0.908648	0.226035	0.088423	0.804721	0.892043	0.145450	0.093448		0.737288


stage2 300x300 LR_1e-5

{'AUC': 0.9049762250520321, 'Recall': 0.788135593220339, 'Precision': 0.10186199342825848}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.872057	0.298344	0.082127	0.706009	0.868213	0.159605	0.087432		0.677966
1	0.907541	0.221560	0.085761	0.789700	0.882640	0.165190	0.090196		0.779661


stage2 384x384 LR_1e-5

{'AUC': 0.908560816159625, 'Recall': 0.7966101694915254, 'Precision': 0.0940940940940941}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.870047	0.302422	0.081955	0.701717	0.87273		0.158985	0.090503		0.686441
1	0.910361	0.215978	0.087186	0.781116	0.88518		0.155824	0.090517		0.711864


stage2 512x512 LR_1e-5

{'AUC': 0.9063143850230935, 'Recall': 0.8813559322033898, 'Precision': 0.06740116655865197}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.851710	0.297399	0.064678	0.748927	0.881099	0.225009	0.068503		0.822034
1	0.885719	0.232649	0.069245	0.826180	0.888474	0.216212	0.068540		0.847458



stage2 128x128 LR_1e-4

{'AUC': 0.8947768510120737, 'Recall': 0.6779661016949152, 'Precision': 0.1008827238335435}

	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.892731	0.236008	0.075081	0.800429	0.892125	0.165797	0.078261		0.762712
1	0.938579	0.169031	0.085491	0.892704	0.895221	0.174296	0.081592		0.694915


stage2 256x256 LR_1e-4

{'AUC': 0.9048753060336466, 'Recall': 0.788135593220339, 'Precision': 0.09567901234567901}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.889942	0.248142	0.075234	0.809013	0.902922	0.175796	0.075826		0.855932
1	0.939736	0.163449	0.082784	0.916309	0.903497	0.171671	0.080740		0.813559


stage2 300x300 LR_1e-4

{'AUC': 0.9049762250520321, 'Recall': 0.788135593220339, 'Precision': 0.10186199342825848}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.894292	0.237263	0.075727	0.793991	0.877029	0.203162	0.068135		0.838983
1	0.942561	0.160598	0.084314	0.922747	0.890142	0.191544	0.078399		0.796610


stage2 384x384 LR_1e-4

{'AUC': 0.908560816159625, 'Recall': 0.7966101694915254, 'Precision': 0.0940940940940941}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.886815	0.255834	0.077843	0.768240	0.882448	0.192433	0.072812		0.79661
1	0.932793	0.174425	0.086546	0.905579	0.896932	0.182818	0.079459		0.79661


stage2 512x512 LR_1e-4

{'AUC': 0.9063143850230935, 'Recall': 0.8813559322033898, 'Precision': 0.06740116655865197}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.867328	0.262295	0.063942	0.809013	0.896137	0.194587	0.069204		0.847458
1	0.895431	0.214195	0.065178	0.858369	0.895602	0.248036	0.058855		0.923729



256x256 L287 1e-3_1e-2_1e-3 weightinit (with HeNormal and GlorotNormal weight initializers)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.785411	0.319348	0.037109	0.817597	0.827382	0.313039	0.055260		0.703390
1	0.824800	0.293887	0.047218	0.806867	0.866749	0.219936	0.076023		0.771186
2	0.853791	0.270068	0.050259	0.813305	0.875857	0.324982	0.050432		0.889831
3	0.868502	0.260127	0.055890	0.856223	0.848960	0.358575	0.045126		0.847458
4	0.882870	0.239456	0.057799	0.875537	0.880940	0.183034	0.075386		0.703390
5	0.896916	0.228452	0.063183	0.892704	0.878094	0.233303	0.068592		0.805085
6	0.910622	0.215036	0.072561	0.884120	0.891406	0.186271	0.081056		0.754237
7	0.930486	0.184919	0.081123	0.899142	0.892891	0.165251	0.086345		0.728814
8	0.941449	0.168791	0.089942	0.924893	0.896259	0.156705	0.089304		0.728814
9	0.959674	0.136371	0.113028	0.942060	0.896935	0.146880	0.100578		0.737288


256x256 L287 1e-4_1e-2_1e-3 weightinit (with HeNormal and GlorotNormal weight initializers)

	auc			loss		precision	recall		val_auc		val_loss	val_precision	val_recall
0	0.764195	0.342519	0.033816	0.817597	0.837542	0.354237	0.049741		0.813559
1	0.830720	0.286738	0.045952	0.802575	0.831235	0.307509	0.042487		0.805085
2	0.853774	0.271235	0.053310	0.834764	0.886078	0.298009	0.050145		0.881356
3	0.874274	0.254768	0.056290	0.858369	0.889023	0.226510	0.080490		0.779661
4	0.875694	0.253911	0.057019	0.845494	0.881600	0.209599	0.082772		0.728814
5	0.901644	0.219838	0.065643	0.899142	0.871958	0.274797	0.060302		0.813559
6	0.907045	0.214393	0.071842	0.879828	0.888592	0.192456	0.083490		0.754237
7	0.931722	0.179098	0.082179	0.916309	0.895488	0.187064	0.072289		0.813559
8	0.941547	0.163805	0.085991	0.896996	0.908415	0.164378	0.094070		0.779661
9	0.960733	0.131073	0.115746	0.952790	0.904980	0.145978	0.099765		0.720339


stage2 1e-6_1e-4_1e-5

{'AUC': 0.90504568879196, 'Recall': 0.7203389830508474, 'Precision': 0.09976525821596244}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.859921	0.336432	0.085449	0.671674	0.875107	0.166645	0.085170		0.720339
1	0.916676	0.206285	0.097240	0.824034	0.891032	0.154494	0.090361		0.762712
2	0.941673	0.162112	0.102259	0.903434	0.890036	0.214494	0.070758		0.830508
3	0.949757	0.147750	0.104262	0.929185	0.897028	0.163375	0.089888		0.745763
4	0.962752	0.127383	0.111667	0.957082	0.902365	0.169107	0.090819		0.779661
5	0.969792	0.116754	0.120811	0.959227	0.887899	0.170637	0.090816		0.754237
6	0.977598	0.100953	0.145151	0.989270	0.895165	0.140690	0.118669		0.694915
7	0.983656	0.087617	0.184809	0.976395	0.901969	0.146351	0.118741		0.703390
8	0.985326	0.082446	0.196351	0.969957	0.902780	0.113940	0.146947		0.652542
9	0.989624	0.071009	0.223140	0.984979	0.897396	0.096643	0.171717		0.576271


stage2 1e-5_1e-4_1e-5

{'AUC': 0.90504568879196, 'Recall': 0.7203389830508474, 'Precision': 0.09976525821596244}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.888701	0.270123	0.088631	0.721030	0.882765	0.170655	0.084280		0.754237
1	0.930219	0.179176	0.099284	0.862661	0.895114	0.157122	0.091365		0.771186
2	0.946718	0.154296	0.101296	0.922747	0.899139	0.190572	0.078025		0.830508
3	0.960073	0.131164	0.110671	0.959227	0.886911	0.190996	0.078838		0.805085
4	0.967515	0.118968	0.118491	0.957082	0.897434	0.155262	0.100346		0.737288
5	0.969348	0.116121	0.127917	0.952790	0.902793	0.138880	0.113043		0.771186
6	0.976182	0.103997	0.143129	0.967811	0.897026	0.130280	0.111869		0.694915
7	0.982143	0.091082	0.164364	0.969957	0.904322	0.133769	0.120588		0.694915
8	0.985212	0.083331	0.185743	0.978541	0.907517	0.112520	0.143872		0.686441
9	0.989645	0.071915	0.205461	0.984979	0.904961	0.097541	0.158458		0.627119


stage2 1e-6_3e-6(3)_1e-6(2) g2,al0.5,ls0.02

{'AUC': 0.90504568879196, 'Recall': 0.7203389830508474, 'Precision': 0.09976525821596244}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.828305	0.319505	0.078977	0.603004	0.800850	0.101434	0.082677		0.533898
1	0.864201	0.244796	0.085986	0.678112	0.838935	0.094552	0.089727		0.584746
2	0.888645	0.190297	0.089659	0.699571	0.856842	0.099122	0.092090		0.661017
3	0.898595	0.173985	0.093700	0.746781	0.868800	0.098398	0.092637		0.661017
4	0.915244	0.135773	0.097113	0.793991	0.871790	0.091987	0.094595		0.652542


stage2 1e-7_5e-7(3)_1e-7(2)

{'AUC': 0.90504568879196, 'Recall': 0.7203389830508474, 'Precision': 0.09976525821596244}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.819807	0.424840	0.078315	0.594421	0.764671	0.159206	0.074074		0.508475
1	0.815840	0.438712	0.079714	0.598712	0.779028	0.152119	0.078306		0.516949
2	0.838336	0.383376	0.081870	0.620172	0.785187	0.152791	0.079949		0.533898
3	0.834397	0.387557	0.080884	0.620172	0.796480	0.157346	0.081658		0.550847
4	0.848211	0.363240	0.083121	0.630901	0.798934	0.152172	0.081841		0.542373


stage2 1EP_1e-3

{'AUC': 0.90504568879196, 'Recall': 0.7203389830508474, 'Precision': 0.09976525821596244}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.829577	0.29748		0.0514		0.740343	0.809369	0.253576	0.042355		0.694915


stage2 1EP_5e-4

{'AUC': 0.90504568879196, 'Recall': 0.7203389830508474, 'Precision': 0.09976525821596244}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.85223		0.278072	0.058627	0.753219	0.845282	0.22741		0.050696		0.771186


stage2 1EP_1e-4

{'AUC': 0.90504568879196, 'Recall': 0.7203389830508474, 'Precision': 0.09976525821596244}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.903823	0.226111	0.083884	0.7897		0.884783	0.176568	0.083263		0.830508


stage2 1EP_5e-5

{'AUC': 0.90504568879196, 'Recall': 0.7203389830508474, 'Precision': 0.09976525821596244}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.905169	0.231635	0.091045	0.783262	0.890483	0.156979	0.088571		0.788136


stage2 5e-5_1e-4_5e-5

{'AUC': 0.90504568879196, 'Recall': 0.7203389830508474, 'Precision': 0.09976525821596244}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.905028	0.229685	0.088394	0.774678	0.885951	0.179240	0.079498		0.805085
1	0.944830	0.151269	0.092731	0.933476	0.887142	0.176869	0.079661		0.796610
2	0.945893	0.154728	0.095164	0.924893	0.894038	0.219025	0.067301		0.838983
3	0.966772	0.121108	0.113739	0.969957	0.896146	0.189784	0.088073		0.813559
4	0.974655	0.107217	0.130184	0.969957	0.902299	0.159779	0.101210		0.779661

	Id						AUC			Recall		Precision
0	Non-augmented			0.902481	0.779661	0.101210
1	Augmented_1				0.884965	0.745763	0.079494
2	Augmented_2				0.893262	0.788136	0.088152
3	Augmented_3				0.904116	0.813559	0.104348
4	Augmented_4				0.880874	0.771186	0.077381
5	Augmented_5				0.890115	0.779661	0.082290
6	Augmented_6				0.881462	0.762712	0.081374
7	Augmented_7				0.893559	0.762712	0.098684
8	Augmented_8				0.874535	0.771186	0.076793
9	Augmented_9				0.880685	0.779661	0.080279
10	Augmented_10			0.875292	0.771186	0.080602
11	Augmented_11			0.879600	0.745763	0.094726
12	Augmented_12			0.865704	0.805085	0.080372
13	Augmented_13			0.878060	0.779661	0.073424
14	Augmented_14			0.866846	0.728814	0.071667
15	Augmented_15			0.889465	0.771186	0.083563
16	Augmented_16			0.865455	0.771186	0.066569
17	Augmented_17			0.896106	0.737288	0.094156
18	Augmented_18			0.895905	0.745763	0.100228
19	Augmented_19			0.903894	0.754237	0.119143
20	Augmented_20			0.889157	0.771186	0.099671
21	Augmented_21			0.888888	0.830508	0.072539
22	Augmented_22			0.880094	0.754237	0.070024
23	Augmented_23			0.891857	0.779661	0.088632
24	Augmented_24			0.882644	0.864407	0.067505
25	Combined_with_mean		0.910701	0.813559	0.105033
26	Combined_with_median	0.909733	0.838983	0.105431


stage2 5EP_5e-5 g2

{'AUC': 0.90504568879196, 'Recall': 0.7203389830508474, 'Precision': 0.09976525821596244}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.899012	0.161232	0.087958	0.761803	0.894670	0.097890	0.084532		0.796610
1	0.936685	0.088679	0.091676	0.909871	0.904921	0.090849	0.085557		0.788136
2	0.955653	0.073484	0.103064	0.952790	0.905543	0.105058	0.075457		0.805085
3	0.955630	0.074861	0.103326	0.939914	0.906184	0.085194	0.094376		0.838983
4	0.968683	0.063779	0.120722	0.976395	0.906615	0.086874	0.095821		0.796610


stage2 5EP_5e-5                                                              ------- used this for next submission

{'AUC': 0.90504568879196, 'Recall': 0.7203389830508474, 'Precision': 0.09976525821596244}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.906942	0.226137	0.089603	0.789700	0.894100	0.184610	0.081633		0.847458
1	0.944347	0.155402	0.098398	0.922747	0.901536	0.169446	0.087422		0.830508
2	0.956291	0.139956	0.105565	0.948498	0.899882	0.204771	0.081665		0.864407
3	0.967863	0.119531	0.114708	0.965665	0.907837	0.170440	0.093113		0.813559
4	0.973184	0.111684	0.129116	0.959227	0.907757	0.156650	0.103563		0.788136


stage2 5addiEP_5e-5

{'AUC': 0.9076001195300583, 'Recall': 0.788135593220339, 'Precision': 0.10356347438752785}
	auc_1		loss		precision_1	recall_1	val_auc_1	val_loss	val_precision_1	val_recall_1
0	0.981768	0.092096	0.160928	0.982833	0.891295	0.144535	0.115774		0.677966
1	0.981928	0.089594	0.173282	0.974249	0.898930	0.141169	0.115118		0.703390
2	0.985027	0.081429	0.195049	0.980687	0.906617	0.170456	0.106933		0.771186
3	0.985263	0.081551	0.190556	0.969957	0.890933	0.105876	0.139048		0.618644
4	0.989303	0.068324	0.223633	0.982833	0.882078	0.106606	0.150106		0.601695



256x256 image, stage2 trained (trained with train-valid split) - submission

private score : 0.8467, public score : 0.8978

