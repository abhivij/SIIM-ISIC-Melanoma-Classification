{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":20270,"databundleVersionId":1222630,"sourceType":"competition"},{"sourceId":254599,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":217688,"modelId":239397}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport copy\nimport pandas as pd\nimport math, re, os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib_venn import venn2\nimport keras\nfrom pathlib import Path\nfrom functools import partial\nfrom sklearn.model_selection import train_test_split, StratifiedGroupKFold, cross_val_score\nimport cv2\n\nimport warnings\n\nprint(\"Tensorflow version \" + tf.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:09:16.125778Z","iopub.execute_input":"2025-02-12T12:09:16.126054Z","iopub.status.idle":"2025-02-12T12:09:29.370941Z","shell.execute_reply.started":"2025-02-12T12:09:16.126033Z","shell.execute_reply":"2025-02-12T12:09:29.369999Z"}},"outputs":[{"name":"stdout","text":"Tensorflow version 2.17.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# As of now, there seems to be issues with Kaggle TPUs. Hence using GPU\nprint(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:09:29.372293Z","iopub.execute_input":"2025-02-12T12:09:29.373028Z","iopub.status.idle":"2025-02-12T12:09:30.031703Z","shell.execute_reply.started":"2025-02-12T12:09:29.373006Z","shell.execute_reply":"2025-02-12T12:09:30.030645Z"}},"outputs":[{"name":"stdout","text":"Num GPUs Available: 1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Global variables","metadata":{}},{"cell_type":"code","source":"DIR_PATH = \"/kaggle/input/siim-isic-melanoma-classification/\"\nTRAIN_PATH = DIR_PATH + \"tfrecords/train*.tfrec\"\nTEST_PATH = DIR_PATH + \"tfrecords/test*.tfrec\"\n\nTRAIN_JPEG_PATH = DIR_PATH + \"jpeg/train/\"\nTEST_JPEG_PATH = DIR_PATH + \"jpeg/test/\"\n\nTRAIN_TABDATA_PATH = DIR_PATH + \"train.csv\"\nTEST_TABDATA_PATH = DIR_PATH + \"test.csv\"\n\nAUTOTUNE = tf.data.AUTOTUNE\nBATCH_SIZE = 128\nSHUFFLE_BUFFER_SIZE = BATCH_SIZE * 16\nIMAGE_SIZE = [1024, 1024] # for TFRecord images\nIMAGE_RESIZE = [128, 128]\n\nEPOCHS = 5\n\nTRAIN_ON_FULL_DATA = False\nTRAIN_VALID_SPLIT = True\nTEST_PREDICT = False\n\nRANDOM_SEED = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:09:30.033362Z","iopub.execute_input":"2025-02-12T12:09:30.033697Z","iopub.status.idle":"2025-02-12T12:09:30.063881Z","shell.execute_reply.started":"2025-02-12T12:09:30.033664Z","shell.execute_reply":"2025-02-12T12:09:30.063209Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Tabular data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(TRAIN_TABDATA_PATH)\ntrain[\"image_path\"] = train[\"image_name\"].apply(lambda x: os.path.join(TRAIN_JPEG_PATH, f\"{x}.jpg\"))\ntest = pd.read_csv(TEST_TABDATA_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T12:09:30.065053Z","iopub.execute_input":"2025-02-12T12:09:30.065410Z","iopub.status.idle":"2025-02-12T12:09:30.214090Z","shell.execute_reply.started":"2025-02-12T12:09:30.065358Z","shell.execute_reply":"2025-02-12T12:09:30.213400Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.130Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"print(train.shape, test.shape)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.130Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Mising values","metadata":{}},{"cell_type":"code","source":"print(\"Train\")\nprint(train.isnull().sum())\n\nprint(\"\\n----------------\\n\")\n\nprint(\"Test\")\nprint(test.isnull().sum())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.131Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can replace null values in the columns sex and anatom_site with \"unknown\".\nLets check what to replace null values in age_approx with.","metadata":{}},{"cell_type":"code","source":"train.age_approx.describe()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sum(train.age_approx == 0)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.loc[train.age_approx == 0, :]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.loc[train.age_approx != 0, :].describe()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.age_approx.unique(), test.age_approx.unique()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"age_approx values are in multiples of 5. \n\nThere are 2 rows with values 0. So missing values can't be replaced with 0. Its strange that there are age_approx with 0. But we'll leave it as it is. Could be any age under 5.\n\nWe'll replace missing values with -10 for visualization purpose and also include age_missing column","metadata":{}},{"cell_type":"code","source":"train[\"sex\"] = train[\"sex\"].fillna(\"unknown\")\n\ntrain[\"anatom_site_general_challenge\"] = train[\"anatom_site_general_challenge\"].fillna(\"unknown\")\ntest[\"anatom_site_general_challenge\"] = test[\"anatom_site_general_challenge\"].fillna(\"unknown\")\n\ntrain[\"age_approx\"] = train[\"age_approx\"].fillna(-10)\n\ntrain[\"age_missing\"] = (train.age_approx == -10)\ntrain[\"age_missing\"] = train[\"age_missing\"].astype(int)\n\ntest[\"age_missing\"] = (test.age_approx == -10)\ntest[\"age_missing\"] = test[\"age_missing\"].astype(int)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.age_approx.unique(), test.age_approx.unique()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train\")\nprint(train.isnull().sum())\n\nprint(\"\\n----------------\\n\")\n\nprint(\"Test\")\nprint(test.isnull().sum())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.139Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Target distribution","metadata":{}},{"cell_type":"code","source":"target_counts = train.target.value_counts()\ntarget_counts","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_counts[0]*100/sum(target_counts), target_counts[1]*100/sum(target_counts)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.140Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"98.237 % of images are of benign cases and only 1.762 % is of malignant case.","metadata":{}},{"cell_type":"markdown","source":"## Unique images","metadata":{}},{"cell_type":"code","source":"print(train.shape, test.shape)\nprint(len(train.image_name.unique()), len(test.image_name.unique()))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"So all images are unique in both train and test","metadata":{}},{"cell_type":"markdown","source":"## Images per patient","metadata":{}},{"cell_type":"code","source":"train_patientids = pd.DataFrame(train.patient_id.value_counts())\ndisplay(train_patientids.describe())\ntest_patientids = pd.DataFrame(test.patient_id.value_counts())\ndisplay(test_patientids.describe())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def hist_and_box(df, col_name, main_title, title, hist_xlabel, bin_range=None):\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n    sns.histplot(data=df[col_name].values, binwidth=5, binrange=bin_range, kde=True, ax=axes[0])\n    axes[0].set_title(\"Histogram of \" + title)\n    axes[0].set_xlabel(hist_xlabel)\n    axes[0].set_ylabel(\"Frequency\")\n    \n    sns.boxplot(data=df[col_name].values, ax=axes[1])\n    axes[1].set_title(\"Boxplot of \" + title)\n    axes[1].set_ylabel(hist_xlabel)\n\n    plt.suptitle(main_title)\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\", message=\"use_inf_as_na option is deprecated\")\n\nhist_and_box(train_patientids, \"count\", \"Train data\", \"images per patient\", \"Number of images for a patient\")\n\nhist_and_box(test_patientids, \"count\", \"Test data\", \"images per patient\", \"Number of images for a patient\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.144Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Patient ID overlap","metadata":{}},{"cell_type":"code","source":"venn2(subsets = (set(train.patient_id.unique()), set(test.patient_id.unique())),\n      set_labels = ('Train Patient IDs', 'Test Patient IDs'))\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.145Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are no common patient ids b/w train and test. For train-validation split, we'll need to ensure the same.","metadata":{}},{"cell_type":"markdown","source":"## Sex","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\nsns.countplot(data=train, x=\"sex\", ax=axes[0])\nfor container in axes[0].containers:\n    axes[0].bar_label(container)\naxes[0].set_title(\"Train data\")\n\naxes[1] = sns.countplot(data=test, x=\"sex\", ax=axes[1])\nfor container in axes[1].containers:\n    axes[1].bar_label(container)\naxes[1].set_title(\"Test data\")    \n\nplt.tight_layout() \nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ax = sns.countplot(data=train, x=\"sex\",\n                   hue=\"target\")\nfor container in ax.containers:\n    ax.bar_label(container)\nax.set_title(\"Sex and target count\")\nax.tick_params(axis='x', labelrotation=90)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.146Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Anatomical site","metadata":{}},{"cell_type":"code","source":"print(np.sort(train.anatom_site_general_challenge.unique()))\nprint(np.sort(test.anatom_site_general_challenge.unique()))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"anatom_site_order = train.anatom_site_general_challenge.value_counts().index\nanatom_site_order","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\nsns.countplot(data=train, x=\"anatom_site_general_challenge\", \n              order=anatom_site_order,\n              ax=axes[0])\nfor container in axes[0].containers:\n    axes[0].bar_label(container)\naxes[0].set_title(\"Train data\")\naxes[0].tick_params(axis='x', labelrotation=90)\n\naxes[1] = sns.countplot(data=test, x=\"anatom_site_general_challenge\", \n                        order=anatom_site_order,\n                        ax=axes[1])\nfor container in axes[1].containers:\n    axes[1].bar_label(container)\naxes[1].set_title(\"Test data\")   \naxes[1].tick_params(axis='x', labelrotation=90)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ax = sns.countplot(data=train, x=\"anatom_site_general_challenge\",\n                   hue=\"target\",\n                   order=anatom_site_order)\nfor container in ax.containers:\n    ax.bar_label(container)\nax.set_title(\"Anatom_site and target count\")\nax.tick_params(axis='x', labelrotation=90)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.148Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Age","metadata":{}},{"cell_type":"code","source":"print(\"Train\")\nprint(train.age_approx.describe())\n\nprint(\"\\n----------------\\n\")\n\nprint(\"Test\")\nprint(test.age_approx.describe())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hist_and_box(train, \"age_approx\", \"Train data\", \"patient age\", \"Patient age\", (-10,100))\nhist_and_box(test, \"age_approx\", \"Test data\", \"patient age\", \"Patient age\", (-10,100))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hist_and_box(train.loc[train.target==1, :], \"age_approx\", \"Malignant\", \"patient age\", \"Patient age\", (-10,100))\nhist_and_box(train.loc[train.target==0, :], \"age_approx\", \"Benign\", \"patient age\", \"Patient age\", (-10,100))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.loc[(train.target==1) & (train.age_approx==-10.0), :]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.151Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The proportion of categories of sex, anatom_site, age_approx seem to be visibly similar in train and test data. So that need not be an additional consideration during train-validation split.","metadata":{}},{"cell_type":"markdown","source":"# Check images","metadata":{}},{"cell_type":"code","source":"print(\"Examples : Malignant (With Melanoma)\")\nimgs = train.loc[train.target==1].sample(10).image_name.values\nplt.figure(figsize=(20,8))\nfor i,k in enumerate(imgs):\n    img = cv2.imread(TRAIN_JPEG_PATH + k + \".jpg\")\n    img = cv2.resize(img, IMAGE_RESIZE)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.subplot(2,5,i+1); plt.axis('off')\n    plt.imshow(img)\nplt.show()\n\nprint(\"Examples : Benign (Without Melanoma)\")\nimgs = train.loc[train.target==0].sample(10).image_name.values\nplt.figure(figsize=(20,8))\nfor i,k in enumerate(imgs):\n    img = cv2.imread(TRAIN_JPEG_PATH + k + \".jpg\")\n    img = cv2.resize(img, IMAGE_RESIZE)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.subplot(2,5,i+1); plt.axis('off')\n    plt.imshow(img)\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.151Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"def decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    # image = tf.cast(image, tf.float32) / 255.0   # convnext doesn't need this\n    # image = tf.reshape(image, [*IMAGE_SIZE, 3])  # not required since resize is done next\n    image = tf.image.resize(image, IMAGE_RESIZE)\n    return image","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.152Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load TFRecord data","metadata":{}},{"cell_type":"code","source":"def read_tfrecord(example, labeled):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    } if labeled else {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    if labeled:\n        label = tf.cast(example['target'], tf.int32)\n        return image, label\n    idnum = example['image_name']\n    return image, idnum\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n    return dataset","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_filenames = tf.io.gfile.glob(TRAIN_PATH) \ntest_filenames = tf.io.gfile.glob(TEST_PATH)\n\nprint('Train TFRecord Files:', len(train_filenames))\nprint('Test TFRecord Files:', len(test_filenames))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nnum_training_images = count_data_items(train_filenames)\nnum_test_images = count_data_items(test_filenames)\nprint(\n    'Dataset: {} training images, {} unlabeled test images'.format(\n        num_training_images, num_test_images\n    )\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.154Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load jpeg data","metadata":{}},{"cell_type":"code","source":"def load_image(image_path, label):\n    image = tf.io.read_file(image_path)\n    image = decode_image(image)\n    return image, label\n\n# image_paths = train[\"image_path\"].values\n# labels = train[\"target\"].values\n\ndef load_jpeg_dataset(image_paths, labels):\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n    dataset = dataset.map(lambda x, y: load_image(x, y))\n    return dataset","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.154Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{}},{"cell_type":"code","source":"def augmentation_pipeline(image, label):\n    image = tf.image.random_flip_left_right(image)\n    return image, label","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.155Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Get datasets","metadata":{}},{"cell_type":"code","source":"def get_training_dataset(tfrecord=True, filenames=None, image_paths=None, labels=None):\n    if tfrecord:\n        dataset = load_dataset(filenames, labeled=True)\n    else: #jpeg\n        dataset = load_jpeg_dataset(image_paths, labels)\n    dataset = dataset.map(augmentation_pipeline, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\ndef get_validation_dataset(ordered=False, tfrecord=True, filenames=None, image_paths=None, labels=None):\n    if tfrecord:\n        dataset = load_dataset(filenames, labeled=True, ordered=ordered) \n    else: #jpeg\n        dataset = load_jpeg_dataset(image_paths, labels)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\ndef get_test_dataset(ordered=False):   # only use TFRecord for predicting test data\n    dataset = load_dataset(filenames, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_batch(image_batch, label_batch):\n    plt.figure(figsize=(10,10))\n    image_batch = tf.cast(image_batch, tf.float32) / 255.0\n    for n in range(25):\n        ax = plt.subplot(5,5,n+1)\n        plt.imshow(image_batch[n])\n        if label_batch[n]:\n            plt.title(\"Malignant\")\n        else:\n            plt.title(\"Benign\")\n        plt.axis(\"off\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = get_training_dataset(filenames=train_filenames)\nimage_batch, label_batch = next(iter(train_dataset))\nshow_batch(image_batch.numpy(), label_batch.numpy())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.157Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train validation split using jpeg data","metadata":{}},{"cell_type":"markdown","source":"Since TFRecord images are grouped together into different files, performing train-validation split using patientid column would be easier with jpeg files.\n\nLets perform 80-20 train-validation split taking into account the columns : target and patientid. Train and validation datasets need \n* equal proportion of target column values.\n* non-overlapping sets of patientids","metadata":{}},{"cell_type":"code","source":"train_index, valid_index = next(StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED).split(train.image_name, train.target, train.patient_id))\n\nprint(len(train_index), len(valid_index))\nprint(len(train_index)/train.shape[0], len(valid_index)/train.shape[0])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.158Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can use the rest of the 4 splits generated above, but we'll only use the 1st split for quick iteration and then train on full data once we identify the best model and hyperparams.","metadata":{}},{"cell_type":"code","source":"train_data = train.loc[train_index, :]\nvalid_data = train.loc[valid_index, :]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"venn2(subsets = (set(train_data.patient_id.unique()), \n                 set(valid_data.patient_id.unique())),\n      set_labels = ('Trainset Patient IDs', 'Validset Patient IDs'))\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.159Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As expected, it non-overlapping","metadata":{}},{"cell_type":"code","source":"t_target_counts = train_data.target.value_counts()\nprint(t_target_counts[0]*100/sum(t_target_counts), t_target_counts[1]*100/sum(t_target_counts))\n\nv_target_counts = valid_data.target.value_counts()\nprint(v_target_counts[0]*100/sum(v_target_counts), v_target_counts[1]*100/sum(v_target_counts))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.160Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Proportion of target categories is same in train and validation","metadata":{}},{"cell_type":"code","source":"train_jpeg_dataset = get_training_dataset(tfrecord=False, \n                                          image_paths=train_data.image_path.values, \n                                          labels=train_data.target.values)\nimage_batch, label_batch = next(iter(train_jpeg_dataset))\nshow_batch(image_batch.numpy(), label_batch.numpy())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"valid_jpeg_dataset = get_validation_dataset(tfrecord=False,\n                                            image_paths=valid_data.image_path.values,\n                                            labels=valid_data.target.values)\nimage_batch, label_batch = next(iter(valid_jpeg_dataset))\nshow_batch(image_batch.numpy(), label_batch.numpy())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load the train and valid dataset again, since we have iterated over them to show_batch","metadata":{}},{"cell_type":"code","source":"train_jpeg_dataset = get_training_dataset(tfrecord=False, \n                                          image_paths=train_data.image_path.values, \n                                          labels=train_data.target.values)\nvalid_jpeg_dataset = get_validation_dataset(tfrecord=False,\n                                            image_paths=valid_data.image_path.values,\n                                            labels=valid_data.target.values)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if TRAIN_ON_FULL_DATA or TRAIN_VALID_SPLIT:\n\n    base_model = keras.applications.EfficientNetV2S(\n        include_top=False,\n        input_shape=(*IMAGE_RESIZE, 3),\n        include_preprocessing=True\n    )\n    base_model.trainable = False\n        \n    inputs = keras.Input(shape=(*IMAGE_RESIZE, 3))\n    x = base_model(inputs, training=False)\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    x = keras.layers.Dropout(0.2)(x)\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n    \n    model = keras.Model(inputs, outputs)\n    \n    model.summary(show_trainable=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if TRAIN_ON_FULL_DATA or TRAIN_VALID_SPLIT:\n\n    lr_scheduler = keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=1e-5, \n        decay_steps=10000, \n        decay_rate=0.9)\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler, epsilon=0.001),\n        loss='binary_crossentropy',  \n        metrics=[keras.metrics.AUC(name='auc')])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if TRAIN_VALID_SPLIT:\n    steps_per_epoch = int(np.ceil(train_data.shape[0] / BATCH_SIZE))\n    validation_steps = int(np.ceil(valid_data.shape[0] / BATCH_SIZE))\n    history = model.fit(train_jpeg_dataset, \n                        steps_per_epoch=steps_per_epoch, \n                        epochs=EPOCHS,\n                        validation_data=valid_jpeg_dataset,\n                        validation_steps=validation_steps)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if TRAIN_VALID_SPLIT:\n    history_frame = pd.DataFrame(history.history)\n    history_frame.loc[:, ['loss', 'val_loss']].plot()\n    history_frame.loc[:, ['auc', 'val_auc']].plot()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if TRAIN_ON_FULL_DATA:\n    steps_per_epoch = int(np.ceil(num_training_images / BATCH_SIZE))\n    history = model.fit(train_dataset, \n                        steps_per_epoch=steps_per_epoch, \n                        epochs=EPOCHS)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if TRAIN_ON_FULL_DATA:\n    history_frame = pd.DataFrame(history.history)\n    history_frame.loc[:, ['loss', 'auc']].plot()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if TRAIN_ON_FULL_DATA:\n    model.save(\"effnetv2_s_1.keras\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if TEST_PREDICT:\n    model = keras.models.load_model(\"/kaggle/input/melanoma-classification/tensorflow2/effnetv2_s_1/1/effnetv2_s_1.keras\")\n    model.summary()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.167Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predict on test data","metadata":{}},{"cell_type":"code","source":"if TEST_PREDICT:\n    test_dataset = get_test_dataset(ordered=True)\n    test_images = test_dataset.map(lambda image, idnum: image)\n    prediction_prob = model.predict(test_images, steps=np.ceil(num_test_images / BATCH_SIZE))\n\n    print(prediction_prob)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.167Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create submission file","metadata":{}},{"cell_type":"code","source":"if TEST_PREDICT:\n    image_names = np.array([img_name.numpy().decode(\"utf-8\") \n                            for img, img_name in iter(test_dataset.unbatch())])\n    \n    submission = pd.DataFrame(dict(image_name=image_names, target=prediction_prob[:, 0]))\n    submission = submission.sort_values('image_name') \n    submission.to_csv('submission.csv', index=False)\n    !head submission.csv","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-12T12:09:52.168Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# References\n* https://www.kaggle.com/code/jessemostipak/getting-started-tpus-cassava-leaf-disease\n* https://www.kaggle.com/code/amyjang/tensorflow-transfer-learning-melanoma\n* https://www.kaggle.com/code/cdeotte/triple-stratified-kfold-with-tfrecords\n* https://www.kaggle.com/code/ibtesama/siim-baseline-keras-vgg16\n","metadata":{}}]}